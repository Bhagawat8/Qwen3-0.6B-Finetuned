{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-07-13T18:23:00.364952Z",
     "iopub.status.busy": "2025-07-13T18:23:00.364655Z",
     "iopub.status.idle": "2025-07-13T18:23:00.618179Z",
     "shell.execute_reply": "2025-07-13T18:23:00.617484Z",
     "shell.execute_reply.started": "2025-07-13T18:23:00.364931Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/qwen3-finetuned/transformers/default/3/__huggingface_repos__.json\n",
      "/kaggle/input/qwen3-finetuned/transformers/default/3/wandb/run-20250712_162007-as3dltlu/run-as3dltlu.wandb\n",
      "/kaggle/input/qwen3-finetuned/transformers/default/3/wandb/run-20250712_162007-as3dltlu/logs/debug.log\n",
      "/kaggle/input/qwen3-finetuned/transformers/default/3/wandb/run-20250712_162007-as3dltlu/logs/debug-internal.log\n",
      "/kaggle/input/qwen3-finetuned/transformers/default/3/wandb/run-20250712_162007-as3dltlu/files/wandb-summary.json\n",
      "/kaggle/input/qwen3-finetuned/transformers/default/3/wandb/run-20250712_162007-as3dltlu/files/config.yaml\n",
      "/kaggle/input/qwen3-finetuned/transformers/default/3/wandb/run-20250712_162007-as3dltlu/files/output.log\n",
      "/kaggle/input/qwen3-finetuned/transformers/default/3/wandb/run-20250712_162007-as3dltlu/files/requirements.txt\n",
      "/kaggle/input/qwen3-finetuned/transformers/default/3/wandb/run-20250712_162007-as3dltlu/files/wandb-metadata.json\n",
      "/kaggle/input/qwen3-finetuned/transformers/default/3/lora-checkpoints-20250712_1620/latest-checkpoint/adapter_model.safetensors\n",
      "/kaggle/input/qwen3-finetuned/transformers/default/3/lora-checkpoints-20250712_1620/latest-checkpoint/merges.txt\n",
      "/kaggle/input/qwen3-finetuned/transformers/default/3/lora-checkpoints-20250712_1620/latest-checkpoint/adapter_config.json\n",
      "/kaggle/input/qwen3-finetuned/transformers/default/3/lora-checkpoints-20250712_1620/latest-checkpoint/README.md\n",
      "/kaggle/input/qwen3-finetuned/transformers/default/3/lora-checkpoints-20250712_1620/latest-checkpoint/tokenizer.json\n",
      "/kaggle/input/qwen3-finetuned/transformers/default/3/lora-checkpoints-20250712_1620/latest-checkpoint/vocab.json\n",
      "/kaggle/input/qwen3-finetuned/transformers/default/3/lora-checkpoints-20250712_1620/latest-checkpoint/training_state.pt\n",
      "/kaggle/input/qwen3-finetuned/transformers/default/3/lora-checkpoints-20250712_1620/latest-checkpoint/tokenizer_config.json\n",
      "/kaggle/input/qwen3-finetuned/transformers/default/3/lora-checkpoints-20250712_1620/latest-checkpoint/metadata.json\n",
      "/kaggle/input/qwen3-finetuned/transformers/default/3/lora-checkpoints-20250712_1620/latest-checkpoint/chat_template.jinja\n",
      "/kaggle/input/qwen3-finetuned/transformers/default/3/lora-checkpoints-20250712_1620/latest-checkpoint/special_tokens_map.json\n",
      "/kaggle/input/qwen3-finetuned/transformers/default/3/lora-checkpoints-20250712_1620/latest-checkpoint/added_tokens.json\n"
     ]
    }
   ],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd \n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-13T18:23:01.430648Z",
     "iopub.status.busy": "2025-07-13T18:23:01.430292Z",
     "iopub.status.idle": "2025-07-13T18:23:06.148823Z",
     "shell.execute_reply": "2025-07-13T18:23:06.148117Z",
     "shell.execute_reply.started": "2025-07-13T18:23:01.430617Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.6/193.6 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "bigframes 2.8.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\n",
      "cesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\n",
      "torch 2.6.0+cu124 requires nvidia-cublas-cu12==12.4.5.8; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cublas-cu12 12.5.3.2 which is incompatible.\n",
      "torch 2.6.0+cu124 requires nvidia-cuda-cupti-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-cupti-cu12 12.5.82 which is incompatible.\n",
      "torch 2.6.0+cu124 requires nvidia-cuda-nvrtc-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-nvrtc-cu12 12.5.82 which is incompatible.\n",
      "torch 2.6.0+cu124 requires nvidia-cuda-runtime-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-runtime-cu12 12.5.82 which is incompatible.\n",
      "torch 2.6.0+cu124 requires nvidia-cudnn-cu12==9.1.0.70; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cudnn-cu12 9.3.0.75 which is incompatible.\n",
      "torch 2.6.0+cu124 requires nvidia-cufft-cu12==11.2.1.3; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cufft-cu12 11.2.3.61 which is incompatible.\n",
      "torch 2.6.0+cu124 requires nvidia-curand-cu12==10.3.5.147; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-curand-cu12 10.3.6.82 which is incompatible.\n",
      "torch 2.6.0+cu124 requires nvidia-cusolver-cu12==11.6.1.9; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusolver-cu12 11.6.3.83 which is incompatible.\n",
      "torch 2.6.0+cu124 requires nvidia-cusparse-cu12==12.3.1.170; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusparse-cu12 12.5.1.3 which is incompatible.\n",
      "torch 2.6.0+cu124 requires nvidia-nvjitlink-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-nvjitlink-cu12 12.5.82 which is incompatible.\n",
      "gcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2025.3.0 which is incompatible.\n",
      "bigframes 2.8.0 requires google-cloud-bigquery[bqstorage,pandas]>=3.31.0, but you have google-cloud-bigquery 3.25.0 which is incompatible.\n",
      "bigframes 2.8.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install -q evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-13T18:23:06.150610Z",
     "iopub.status.busy": "2025-07-13T18:23:06.150374Z",
     "iopub.status.idle": "2025-07-13T18:23:09.548185Z",
     "shell.execute_reply": "2025-07-13T18:23:09.547264Z",
     "shell.execute_reply.started": "2025-07-13T18:23:06.150581Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.8/51.8 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.1/104.1 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "!pip install -q sacrebleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-13T18:23:09.549965Z",
     "iopub.status.busy": "2025-07-13T18:23:09.549338Z",
     "iopub.status.idle": "2025-07-13T18:23:12.689898Z",
     "shell.execute_reply": "2025-07-13T18:23:12.688980Z",
     "shell.execute_reply.started": "2025-07-13T18:23:09.549938Z"
    }
   },
   "outputs": [],
   "source": [
    "!pip install -q wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-13T18:23:12.692408Z",
     "iopub.status.busy": "2025-07-13T18:23:12.692108Z",
     "iopub.status.idle": "2025-07-13T18:23:42.497152Z",
     "shell.execute_reply": "2025-07-13T18:23:42.496545Z",
     "shell.execute_reply.started": "2025-07-13T18:23:12.692376Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-13 18:23:26.824964: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1752431007.005480      36 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1752431007.059761      36 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    }
   ],
   "source": [
    "# imports\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, Trainer, TrainingArguments, DataCollatorWithPadding, get_scheduler\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "import wandb\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from accelerate import Accelerator\n",
    "from transformers.optimization import AdafactorSchedule\n",
    "from transformers.optimization import get_cosine_schedule_with_warmup\n",
    "from accelerate import Accelerator\n",
    "from tqdm.auto import tqdm\n",
    "import math\n",
    "from transformers.optimization import Adafactor, get_scheduler\n",
    "from typing import Dict, List, Tuple\n",
    "import json\n",
    "from datetime import datetime\n",
    "from peft import LoraConfig , get_peft_model\n",
    "from transformers import DataCollatorForSeq2Seq\n",
    "import shutil\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "import gc\n",
    "torch.backends.cudnn.benchmark = True  # Optimize for P100\n",
    "torch.backends.cudnn.deterministic = False  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-13T18:23:42.498037Z",
     "iopub.status.busy": "2025-07-13T18:23:42.497828Z",
     "iopub.status.idle": "2025-07-13T18:23:48.868846Z",
     "shell.execute_reply": "2025-07-13T18:23:48.868244Z",
     "shell.execute_reply.started": "2025-07-13T18:23:42.498020Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33m22cs3033\u001b[0m (\u001b[33m22cs3033-rgipt-jais\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# connect wandb library for graphs\n",
    "from kaggle_secrets import UserSecretsClient\n",
    "\n",
    "secret = UserSecretsClient()\n",
    "os.environ[\"WANDB_API_KEY\"] = secret.get_secret(\"WANDB_KEY\") \n",
    "\n",
    "wandb.login()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-13T18:23:48.870195Z",
     "iopub.status.busy": "2025-07-13T18:23:48.869605Z",
     "iopub.status.idle": "2025-07-13T18:23:58.306773Z",
     "shell.execute_reply": "2025-07-13T18:23:58.305939Z",
     "shell.execute_reply.started": "2025-07-13T18:23:48.870177Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3c030e2a2e04817b5838a272ce8edb6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/726 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91f4d7b8ae7345279aaaf5e3f2ea0bd0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.50G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "282220317a9e4abb96b824c42a4f627c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/239 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e0186e9881242389af3fa5f6a2eebb9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85c0d1d02bba4dd0bca2620905a7031b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "541873d9d066489b895a3113844e686d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9facd80bbc42472faaadfc25920c647a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/11.4M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# loading model and tokenizer into the RAM\n",
    "model_name = \"Qwen/Qwen3-0.6B\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, low_cpu_mem_usage=True )\n",
    "model.config.use_cache = False\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-13T18:23:58.307880Z",
     "iopub.status.busy": "2025-07-13T18:23:58.307599Z",
     "iopub.status.idle": "2025-07-13T18:25:23.800343Z",
     "shell.execute_reply": "2025-07-13T18:25:23.799546Z",
     "shell.execute_reply.started": "2025-07-13T18:23:58.307854Z"
    }
   },
   "outputs": [],
   "source": [
    "# create the response on default pretrained model\n",
    "prompt = \"Dengue cases on the rise in city\"\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a translator. who knows marathai and english language very well. Your task is convert given english sentence in marathi.\"},\n",
    "    {\"role\": \"user\", \"content\": prompt}\n",
    "]\n",
    "text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True\n",
    ")\n",
    "model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "generated_ids = model.generate(\n",
    "    **model_inputs,\n",
    "    max_new_tokens=512\n",
    ")\n",
    "generated_ids = [\n",
    "    output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
    "]\n",
    "\n",
    "response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-13T18:25:23.802956Z",
     "iopub.status.busy": "2025-07-13T18:25:23.802704Z",
     "iopub.status.idle": "2025-07-13T18:25:23.808207Z",
     "shell.execute_reply": "2025-07-13T18:25:23.807599Z",
     "shell.execute_reply.started": "2025-07-13T18:25:23.802938Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<think>\\nOkay, the user wants me to translate the sentence \"Dengue cases on the rise in city\" into Marathi. Let me start by understanding the original sentence. The main components are \"Dengue cases on the rise\" and \"in city\". \\n\\nFirst, \"Dengue cases\" is a medical term, so I need to make sure it\\'s translated accurately. In Marathi, \"Dengue\" is commonly known as \"देंगू\" or sometimes \"देंगू अस्पर्त रेंस\". Since it\\'s a specific condition, maybe \"देंगू\" is better. \\n\\nNext, \"cases on the rise\" – \"cases\" here refers to the number of cases, so \"संख्या\" might be appropriate. The phrase \"on the rise\" could be \"स्थायितीय रूप वार्ता\". So combining these, \"Dengue cases on the rise in city\" becomes \"देंगू अस्पर्त रेंस स्थायितीय रूप वार्ता वर्ता विश्व राज्य\". \\n\\nWait, \"in city\" – \"विश्व राज्य\" is the general term for a country or region. But \"city\" could also be \"महाराज्य\" or \"महाराज्य विश्व\". Since the original is \"on the rise in city\", maybe \"महाराज्य विश्व\" is more specific. \\n\\nI should check if \"Dengue cases\" is best translated as \"देंगू अस्पर्त रेंस\" or \"देंगू अस्पर्त रेंस विश्व\". The latter might be clearer. Also, \"rise\" in this context is a positive trend, so \"स्थायितीय रूप वार्ता\" is correct. \\n\\nPutting it all together, the translation should be accurate and natural in Marathi. Let me double-check'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-13T18:25:23.808954Z",
     "iopub.status.busy": "2025-07-13T18:25:23.808745Z",
     "iopub.status.idle": "2025-07-13T18:25:44.696186Z",
     "shell.execute_reply": "2025-07-13T18:25:44.695487Z",
     "shell.execute_reply.started": "2025-07-13T18:25:23.808936Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd1fbc4749fe4e109a40731fec758b8a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/206 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2cc3cd5d20044471bb2583cdfaea67cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train.csv:   0%|          | 0.00/621M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cbc71decf6864be29162276e115a3b69",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "test.csv:   0%|          | 0.00/243M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a796fc0c9e04910bc0a01bcb6f5acaa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/2637962 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d831c9f72734682a230f7db021d98ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/879321 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# load the english to marathi translation data from huggingface\n",
    "ds = load_dataset(\"anujsahani01/English-Marathi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-13T18:25:44.697061Z",
     "iopub.status.busy": "2025-07-13T18:25:44.696842Z",
     "iopub.status.idle": "2025-07-13T18:25:44.701989Z",
     "shell.execute_reply": "2025-07-13T18:25:44.701220Z",
     "shell.execute_reply.started": "2025-07-13T18:25:44.697043Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['english', 'marathi'],\n",
       "        num_rows: 2637962\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['english', 'marathi'],\n",
       "        num_rows: 879321\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-13T18:25:44.702997Z",
     "iopub.status.busy": "2025-07-13T18:25:44.702702Z",
     "iopub.status.idle": "2025-07-13T18:25:44.719541Z",
     "shell.execute_reply": "2025-07-13T18:25:44.719060Z",
     "shell.execute_reply.started": "2025-07-13T18:25:44.702979Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'english': 'The funds collected are used for social purposes.',\n",
       " 'marathi': 'त्या निधीचा वापर सामाजिक कार्यासाठी करतो.'}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds['train'][10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-13T18:25:44.720530Z",
     "iopub.status.busy": "2025-07-13T18:25:44.720253Z",
     "iopub.status.idle": "2025-07-13T18:25:44.738304Z",
     "shell.execute_reply": "2025-07-13T18:25:44.737760Z",
     "shell.execute_reply.started": "2025-07-13T18:25:44.720514Z"
    }
   },
   "outputs": [],
   "source": [
    "# Dataset is too big so slice it accroding your computation resources\n",
    "df_train = ds[\"train\"].select(range(10000))\n",
    "df_validation = ds[\"train\"].select(range(10000 , 11000))\n",
    "df_test = ds[\"test\"].select(range(200))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-13T18:25:44.739656Z",
     "iopub.status.busy": "2025-07-13T18:25:44.739046Z",
     "iopub.status.idle": "2025-07-13T18:25:44.750517Z",
     "shell.execute_reply": "2025-07-13T18:25:44.749846Z",
     "shell.execute_reply.started": "2025-07-13T18:25:44.739635Z"
    }
   },
   "outputs": [],
   "source": [
    "# # Dataset is too big so slice it for quick experimentation\n",
    "# df_train = ds[\"train\"].select(range(500))\n",
    "# df_validation = ds[\"train\"].select(range(500 , 550))\n",
    "# df_test = ds[\"test\"].select(range(50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-13T18:25:44.751304Z",
     "iopub.status.busy": "2025-07-13T18:25:44.751085Z",
     "iopub.status.idle": "2025-07-13T18:25:45.111296Z",
     "shell.execute_reply": "2025-07-13T18:25:45.110632Z",
     "shell.execute_reply.started": "2025-07-13T18:25:44.751289Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa080a61d7e84993a6a59ab24bff6f93",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a1e943ef920482795dc2dfedb0128e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab7cfca083de449a991df32969a4206a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/200 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Qwen3-0.6B is Causal Language Models so we have finetuned it Instruction-output format. (instruction‑tuning)\n",
    "def add_instruction(example):\n",
    "    example[\"instruction\"] = \"Convert the English text into Marathi language.\"\n",
    "    return example\n",
    "\n",
    "\n",
    "df_train = df_train.map(add_instruction, batched=False)\n",
    "df_validation = df_validation.map(add_instruction, batched=False)\n",
    "df_test = df_test.map(add_instruction, batched=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-13T18:25:45.112327Z",
     "iopub.status.busy": "2025-07-13T18:25:45.112071Z",
     "iopub.status.idle": "2025-07-13T18:25:45.117095Z",
     "shell.execute_reply": "2025-07-13T18:25:45.116423Z",
     "shell.execute_reply.started": "2025-07-13T18:25:45.112308Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['english', 'marathi']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds[\"train\"].column_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-13T18:25:45.118096Z",
     "iopub.status.busy": "2025-07-13T18:25:45.117824Z",
     "iopub.status.idle": "2025-07-13T18:25:45.138554Z",
     "shell.execute_reply": "2025-07-13T18:25:45.138040Z",
     "shell.execute_reply.started": "2025-07-13T18:25:45.118081Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'english': 'Next few months are really crucial for us.',\n",
       " 'marathi': 'पुढील तीन महिने आमच्यासाठी खूप महत्त्वपूर्ण आहेत.',\n",
       " 'instruction': 'Convert the English text into Marathi language.'}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-13T18:25:45.139496Z",
     "iopub.status.busy": "2025-07-13T18:25:45.139267Z",
     "iopub.status.idle": "2025-07-13T18:25:45.155077Z",
     "shell.execute_reply": "2025-07-13T18:25:45.154486Z",
     "shell.execute_reply.started": "2025-07-13T18:25:45.139473Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [5847, 2421, 3951, 525, 2167, 16587, 369, 601, 13, 86162, 72653, 149269, 43647, 91811, 14925, 97, 43647, 60096, 91217, 93948, 42311, 101, 34370, 14925, 228, 87244, 146113, 30484, 107, 31411, 116, 31411, 254, 43647, 14925, 244, 12619, 224, 86162, 91217, 93948, 79238, 30484, 97, 30484, 113, 86162, 12619, 224, 44179, 30484, 96, 14925, 228, 93948, 54784, 97, 13], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': [12012, 279, 6364, 1467, 1119, 2876, 66531, 4128, 13]}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Know about how tokenizer works\n",
    "inputs = tokenizer('Next few months are really crucial for us.','पुढील तीन महिने आमच्यासाठी खूप महत्त्वपूर्ण आहेत.', 'Convert the English text into Marathi language.' )\n",
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-13T18:25:45.155783Z",
     "iopub.status.busy": "2025-07-13T18:25:45.155614Z",
     "iopub.status.idle": "2025-07-13T18:25:45.174006Z",
     "shell.execute_reply": "2025-07-13T18:25:45.173265Z",
     "shell.execute_reply.started": "2025-07-13T18:25:45.155771Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Next',\n",
       " 'Ġfew',\n",
       " 'Ġmonths',\n",
       " 'Ġare',\n",
       " 'Ġreally',\n",
       " 'Ġcrucial',\n",
       " 'Ġfor',\n",
       " 'Ġus',\n",
       " '.',\n",
       " 'à¤ª',\n",
       " 'à¥ģ',\n",
       " 'à¤¢',\n",
       " 'à¥Ģ',\n",
       " 'à¤²',\n",
       " 'Ġà¤',\n",
       " '¤',\n",
       " 'à¥Ģ',\n",
       " 'à¤¨',\n",
       " 'Ġà¤®',\n",
       " 'à¤¹',\n",
       " 'à¤¿à¤',\n",
       " '¨',\n",
       " 'à¥ĩ',\n",
       " 'Ġà¤',\n",
       " 'Ĩ',\n",
       " 'à¤®',\n",
       " 'à¤ļ',\n",
       " 'à¥įà¤',\n",
       " '¯',\n",
       " 'à¤¾à¤',\n",
       " '¸',\n",
       " 'à¤¾à¤',\n",
       " 'ł',\n",
       " 'à¥Ģ',\n",
       " 'Ġà¤',\n",
       " 'ĸ',\n",
       " 'à¥',\n",
       " 'Ĥ',\n",
       " 'à¤ª',\n",
       " 'Ġà¤®',\n",
       " 'à¤¹',\n",
       " 'à¤¤',\n",
       " 'à¥įà¤',\n",
       " '¤',\n",
       " 'à¥įà¤',\n",
       " 'µ',\n",
       " 'à¤ª',\n",
       " 'à¥',\n",
       " 'Ĥ',\n",
       " 'à¤°',\n",
       " 'à¥įà¤',\n",
       " '£',\n",
       " 'Ġà¤',\n",
       " 'Ĩ',\n",
       " 'à¤¹',\n",
       " 'à¥ĩà¤',\n",
       " '¤',\n",
       " '.']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-13T18:25:45.178015Z",
     "iopub.status.busy": "2025-07-13T18:25:45.177411Z",
     "iopub.status.idle": "2025-07-13T18:25:45.190736Z",
     "shell.execute_reply": "2025-07-13T18:25:45.190027Z",
     "shell.execute_reply.started": "2025-07-13T18:25:45.177994Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Convert the English text into Marathi language. English: Next few months are really crucial for us. Marathi:\n",
      "Length of tokenize form of prompt: torch.Size([1, 23])\n"
     ]
    }
   ],
   "source": [
    "instruction = \"Convert the English text into Marathi language.\"\n",
    "src_text     = \"Next few months are really crucial for us.\"\n",
    "tgt_text     = \"पुढील तीन महिने आमच्यासाठी खूप महत्त्वपूर्ण आहेत.\"\n",
    "\n",
    "# Prompt the model should *see*\n",
    "prompt = f\"{instruction} English: {src_text} Marathi:\"\n",
    "print(prompt)\n",
    "\n",
    "inputs  = tokenizer(prompt, return_tensors=\"pt\") \n",
    "print('Length of tokenize form of prompt:', inputs['input_ids'].size())\n",
    "\n",
    "\n",
    "# Every token sequence in a causal‐LM fine‑tuning setup needs a clear “stop” signal so the model knows when it’s done generating. That’s exactly what tokenizer.eos_token_id is for\n",
    "\n",
    "labels  = tokenizer(tgt_text, add_special_tokens=False).input_ids + [tokenizer.eos_token_id] #Here labes are tokenize form our marathi output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-13T18:25:45.191684Z",
     "iopub.status.busy": "2025-07-13T18:25:45.191477Z",
     "iopub.status.idle": "2025-07-13T18:25:45.209283Z",
     "shell.execute_reply": "2025-07-13T18:25:45.208758Z",
     "shell.execute_reply.started": "2025-07-13T18:25:45.191670Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[12012,   279,  6364,  1467,  1119,  2876, 66531,  4128,    13,  6364,\n",
       "            25,  9295,  2421,  3951,   525,  2167, 16587,   369,   601,    13,\n",
       "          2876, 66531,    25]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-13T18:25:45.210451Z",
     "iopub.status.busy": "2025-07-13T18:25:45.209998Z",
     "iopub.status.idle": "2025-07-13T18:25:45.220525Z",
     "shell.execute_reply": "2025-07-13T18:25:45.219813Z",
     "shell.execute_reply.started": "2025-07-13T18:25:45.210428Z"
    }
   },
   "outputs": [],
   "source": [
    "# The length of tokenize input prompt( instrution + context) is 23"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-13T18:25:45.221431Z",
     "iopub.status.busy": "2025-07-13T18:25:45.221195Z",
     "iopub.status.idle": "2025-07-13T18:25:45.237038Z",
     "shell.execute_reply": "2025-07-13T18:25:45.236435Z",
     "shell.execute_reply.started": "2025-07-13T18:25:45.221416Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "151645"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-13T18:25:45.238316Z",
     "iopub.status.busy": "2025-07-13T18:25:45.237697Z",
     "iopub.status.idle": "2025-07-13T18:25:45.251813Z",
     "shell.execute_reply": "2025-07-13T18:25:45.251265Z",
     "shell.execute_reply.started": "2025-07-13T18:25:45.238299Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[86162,\n",
       " 72653,\n",
       " 149269,\n",
       " 43647,\n",
       " 91811,\n",
       " 14925,\n",
       " 97,\n",
       " 43647,\n",
       " 60096,\n",
       " 91217,\n",
       " 93948,\n",
       " 42311,\n",
       " 101,\n",
       " 34370,\n",
       " 14925,\n",
       " 228,\n",
       " 87244,\n",
       " 146113,\n",
       " 30484,\n",
       " 107,\n",
       " 31411,\n",
       " 116,\n",
       " 31411,\n",
       " 254,\n",
       " 43647,\n",
       " 14925,\n",
       " 244,\n",
       " 12619,\n",
       " 224,\n",
       " 86162,\n",
       " 91217,\n",
       " 93948,\n",
       " 79238,\n",
       " 30484,\n",
       " 97,\n",
       " 30484,\n",
       " 113,\n",
       " 86162,\n",
       " 12619,\n",
       " 224,\n",
       " 44179,\n",
       " 30484,\n",
       " 96,\n",
       " 14925,\n",
       " 228,\n",
       " 93948,\n",
       " 54784,\n",
       " 97,\n",
       " 13,\n",
       " 151645]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-13T18:25:45.252582Z",
     "iopub.status.busy": "2025-07-13T18:25:45.252375Z",
     "iopub.status.idle": "2025-07-13T18:25:45.268225Z",
     "shell.execute_reply": "2025-07-13T18:25:45.267689Z",
     "shell.execute_reply.started": "2025-07-13T18:25:45.252567Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(labels) # 49 orignal + 1 tokenizer.eos_token_id = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-13T18:25:45.269258Z",
     "iopub.status.busy": "2025-07-13T18:25:45.269107Z",
     "iopub.status.idle": "2025-07-13T18:25:45.286400Z",
     "shell.execute_reply": "2025-07-13T18:25:45.285762Z",
     "shell.execute_reply.started": "2025-07-13T18:25:45.269246Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of tokenize form of model_inputs: torch.Size([1, 73])\n",
      "Tokenized Prompt length: 23\n"
     ]
    }
   ],
   "source": [
    "# For a causal model, you feed it the entire sequence (prompt and target), because it learns by shifting that sequence one token at a time.\n",
    "model_inputs = tokenizer(prompt + \" \" + tgt_text + tokenizer.eos_token,  return_tensors=\"pt\")\n",
    "print('Length of tokenize form of model_inputs:', model_inputs['input_ids'].size())  \n",
    "\n",
    "# You need to know how many tokens correspond just to the instruction+source (the “prompt”) so that you can later mask them out in the labels.\n",
    "prompt_len = len(tokenizer(prompt, add_special_tokens=False).input_ids)\n",
    "print(\"Tokenized Prompt length:\" , prompt_len)\n",
    "\n",
    "#  Clone so you don’t overwrite the original inputs\n",
    "labels = model_inputs[\"input_ids\"].clone()\n",
    "\n",
    "\n",
    "# For every position in the prompt, set label to -100  \n",
    "# In Hugging Face Trainer, any label token = –100 is ignored by the loss function.\n",
    "labels[:, :prompt_len] = -100        # works for a batch of size 1 or N\n",
    "# print('Length that going to be masked' , len(labels))\n",
    "model_inputs[\"labels\"] = labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-13T18:25:45.287374Z",
     "iopub.status.busy": "2025-07-13T18:25:45.287155Z",
     "iopub.status.idle": "2025-07-13T18:25:45.305501Z",
     "shell.execute_reply": "2025-07-13T18:25:45.304915Z",
     "shell.execute_reply.started": "2025-07-13T18:25:45.287345Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
       "           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
       "           -100,   -100,   -100,   -100,   -100,  83636,  72653, 149269,  43647,\n",
       "          91811,  14925,     97,  43647,  60096,  91217,  93948,  42311,    101,\n",
       "          34370,  14925,    228,  87244, 146113,  30484,    107,  31411,    116,\n",
       "          31411,    254,  43647,  14925,    244,  12619,    224,  86162,  91217,\n",
       "          93948,  79238,  30484,     97,  30484,    113,  86162,  12619,    224,\n",
       "          44179,  30484,     96,  14925,    228,  93948,  54784,     97,     13,\n",
       "         151645]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-13T18:25:45.306459Z",
     "iopub.status.busy": "2025-07-13T18:25:45.306244Z",
     "iopub.status.idle": "2025-07-13T18:25:45.319262Z",
     "shell.execute_reply": "2025-07-13T18:25:45.318762Z",
     "shell.execute_reply.started": "2025-07-13T18:25:45.306445Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 73])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-13T18:25:45.320047Z",
     "iopub.status.busy": "2025-07-13T18:25:45.319865Z",
     "iopub.status.idle": "2025-07-13T18:25:45.335356Z",
     "shell.execute_reply": "2025-07-13T18:25:45.334758Z",
     "shell.execute_reply.started": "2025-07-13T18:25:45.320033Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23\n",
      "50\n"
     ]
    }
   ],
   "source": [
    "x = labels[0] \n",
    "num_ignored = (x == -100).sum().item()\n",
    "num_not_ignored = (x != -100).sum().item()\n",
    "print(num_ignored)\n",
    "print(num_not_ignored)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-13T18:25:45.336455Z",
     "iopub.status.busy": "2025-07-13T18:25:45.335967Z",
     "iopub.status.idle": "2025-07-13T18:25:45.349885Z",
     "shell.execute_reply": "2025-07-13T18:25:45.349152Z",
     "shell.execute_reply.started": "2025-07-13T18:25:45.336432Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[ 12012,    279,   6364,   1467,   1119,   2876,  66531,   4128,     13,\n",
       "           6364,     25,   9295,   2421,   3951,    525,   2167,  16587,    369,\n",
       "            601,     13,   2876,  66531,     25,  83636,  72653, 149269,  43647,\n",
       "          91811,  14925,     97,  43647,  60096,  91217,  93948,  42311,    101,\n",
       "          34370,  14925,    228,  87244, 146113,  30484,    107,  31411,    116,\n",
       "          31411,    254,  43647,  14925,    244,  12619,    224,  86162,  91217,\n",
       "          93948,  79238,  30484,     97,  30484,    113,  86162,  12619,    224,\n",
       "          44179,  30484,     96,  14925,    228,  93948,  54784,     97,     13,\n",
       "         151645]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1]]), 'labels': tensor([[  -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
       "           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
       "           -100,   -100,   -100,   -100,   -100,  83636,  72653, 149269,  43647,\n",
       "          91811,  14925,     97,  43647,  60096,  91217,  93948,  42311,    101,\n",
       "          34370,  14925,    228,  87244, 146113,  30484,    107,  31411,    116,\n",
       "          31411,    254,  43647,  14925,    244,  12619,    224,  86162,  91217,\n",
       "          93948,  79238,  30484,     97,  30484,    113,  86162,  12619,    224,\n",
       "          44179,  30484,     96,  14925,    228,  93948,  54784,     97,     13,\n",
       "         151645]])}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-13T18:25:45.350680Z",
     "iopub.status.busy": "2025-07-13T18:25:45.350527Z",
     "iopub.status.idle": "2025-07-13T18:25:45.362660Z",
     "shell.execute_reply": "2025-07-13T18:25:45.362069Z",
     "shell.execute_reply.started": "2025-07-13T18:25:45.350668Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 73])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_inputs['input_ids'].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-13T18:25:45.363471Z",
     "iopub.status.busy": "2025-07-13T18:25:45.363303Z",
     "iopub.status.idle": "2025-07-13T18:25:45.377072Z",
     "shell.execute_reply": "2025-07-13T18:25:45.376551Z",
     "shell.execute_reply.started": "2025-07-13T18:25:45.363458Z"
    }
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-13T18:25:45.377845Z",
     "iopub.status.busy": "2025-07-13T18:25:45.377659Z",
     "iopub.status.idle": "2025-07-13T18:25:45.391879Z",
     "shell.execute_reply": "2025-07-13T18:25:45.391222Z",
     "shell.execute_reply.started": "2025-07-13T18:25:45.377832Z"
    }
   },
   "outputs": [],
   "source": [
    "class LoRAConfig:\n",
    "    \"\"\"Centralized configuration for LoRA fine-tuning\"\"\"\n",
    "    \n",
    "    # LoRA Parameters\n",
    "    LORA_RANK = 8  \n",
    "    LORA_ALPHA = 16  \n",
    "    LORA_DROPOUT = 0.1  \n",
    "    \n",
    "    # Training Parameters\n",
    "    LEARNING_RATE = 2e-4  \n",
    "    BATCH_SIZE = 2\n",
    "    GRADIENT_ACCUMULATION_STEPS = 3  \n",
    "    NUM_EPOCHS = 1\n",
    "    WARMUP_STEPS = 100\n",
    "    MAX_GRAD_NORM = 1.0  \n",
    "    \n",
    "    # Optimization\n",
    "    WEIGHT_DECAY = 0.01\n",
    "    BETA1 = 0.9\n",
    "    BETA2 = 0.999\n",
    "    EPS = 1e-8\n",
    "    \n",
    "    # Monitoring and Saving\n",
    "    EVAL_EVERY_N_STEPS = 1000\n",
    "    SAVE_EVERY_N_STEPS = 2000\n",
    "    MAX_CHECKPOINTS = 3\n",
    "    \n",
    "    # Data Processing\n",
    "    MAX_LENGTH = 4096 \n",
    "    PAD_TO_MULTIPLE_OF = 8 \n",
    "\n",
    "config = LoRAConfig()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-13T18:25:45.392643Z",
     "iopub.status.busy": "2025-07-13T18:25:45.392452Z",
     "iopub.status.idle": "2025-07-13T18:25:49.515643Z",
     "shell.execute_reply": "2025-07-13T18:25:49.514966Z",
     "shell.execute_reply.started": "2025-07-13T18:25:45.392621Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Tokenizing datasets\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98a75c596993464e8c7b9b3ffa565ccc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing training data (num_proc=4):   0%|          | 0/10000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bfbb8214a72c4c20a6e284c91a1a9473",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing test data (num_proc=4):   0%|          | 0/200 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3897093be7b64872a2c5998ab9c92f7c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing validation data (num_proc=4):   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenization complete!\n",
      "Train: 10000 examples\n",
      "Validation: 1000 examples\n",
      "Test: 200 examples\n"
     ]
    }
   ],
   "source": [
    "# this function convert string into vectors\n",
    "def tokenize_function(examples):\n",
    "    batch_input_ids = []\n",
    "    batch_attention_mask = []\n",
    "    batch_labels = []\n",
    "\n",
    "    for instruction, english, marathi in zip(\n",
    "            examples[\"instruction\"], examples[\"english\"], examples[\"marathi\"]):\n",
    "\n",
    "        # 1. Create the prompt (what model sees but doesn't train on)\n",
    "        prompt_text = f\"{instruction} English: {english} Marathi:\"\n",
    "        \n",
    "        # 2. Create full text (prompt + target response)\n",
    "        full_text = f\"{prompt_text} {marathi}{tokenizer.eos_token}\"\n",
    "        \n",
    "        # 3. Tokenize prompt separately to know where to mask\n",
    "        prompt_tokens = tokenizer(\n",
    "            prompt_text, \n",
    "            add_special_tokens=False, \n",
    "            truncation=True, \n",
    "            max_length=config.MAX_LENGTH//2,\n",
    "        )\n",
    "        \n",
    "        # 4. Tokenize full text\n",
    "        full_tokens = tokenizer(\n",
    "            full_text,\n",
    "            add_special_tokens=False,\n",
    "            truncation=True,\n",
    "            max_length=config.MAX_LENGTH,\n",
    "            padding=False,\n",
    "        )\n",
    "        \n",
    "        # 5. Create labels: -100 for prompt tokens (ignored in loss)\n",
    "        prompt_length = len(prompt_tokens[\"input_ids\"])\n",
    "        labels = [-100] * prompt_length + full_tokens[\"input_ids\"][prompt_length:]\n",
    "        \n",
    "        # 6. Ensure labels and input_ids have same length\n",
    "        if len(labels) != len(full_tokens[\"input_ids\"]):\n",
    "            labels = labels[:len(full_tokens[\"input_ids\"])]\n",
    "        \n",
    "        # 7. Keep as Python lists (no numpy, no tensors)\n",
    "        batch_input_ids.append(full_tokens[\"input_ids\"])\n",
    "        batch_attention_mask.append(full_tokens[\"attention_mask\"])\n",
    "        batch_labels.append(labels)\n",
    "\n",
    "    return {\n",
    "        \"input_ids\": batch_input_ids,\n",
    "        \"attention_mask\": batch_attention_mask,\n",
    "        \"labels\": batch_labels,\n",
    "    }\n",
    "\n",
    "# Apply tokenization\n",
    "print(\" Tokenizing datasets\")\n",
    "train_tokenized = df_train.map(\n",
    "    tokenize_function, \n",
    "    batched=True, \n",
    "    num_proc=4,\n",
    "    remove_columns=df_train.column_names,\n",
    "    desc=\"Tokenizing training data\"\n",
    ")\n",
    "\n",
    "test_tokenized = df_test.map(\n",
    "    tokenize_function, \n",
    "    batched=True, \n",
    "    num_proc=4,\n",
    "    remove_columns=df_test.column_names,\n",
    "    desc=\"Tokenizing test data\"\n",
    ")\n",
    "\n",
    "validation_tokenized = df_validation.map(\n",
    "    tokenize_function, \n",
    "    batched=True, \n",
    "    num_proc=4,\n",
    "    remove_columns=df_validation.column_names,\n",
    "    desc=\"Tokenizing validation data\"\n",
    ")\n",
    "\n",
    "print(f\"Tokenization complete!\")\n",
    "print(f\"Train: {len(train_tokenized)} examples\")\n",
    "print(f\"Validation: {len(validation_tokenized)} examples\") \n",
    "print(f\"Test: {len(test_tokenized)} examples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-13T18:25:49.517351Z",
     "iopub.status.busy": "2025-07-13T18:25:49.516635Z",
     "iopub.status.idle": "2025-07-13T18:25:49.521691Z",
     "shell.execute_reply": "2025-07-13T18:25:49.521179Z",
     "shell.execute_reply.started": "2025-07-13T18:25:49.517319Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'attention_mask', 'labels'],\n",
       "    num_rows: 200\n",
       "})"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-13T18:25:49.522589Z",
     "iopub.status.busy": "2025-07-13T18:25:49.522340Z",
     "iopub.status.idle": "2025-07-13T18:25:49.561230Z",
     "shell.execute_reply": "2025-07-13T18:25:49.560738Z",
     "shell.execute_reply.started": "2025-07-13T18:25:49.522561Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "90"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_tokenized[0]['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-13T18:25:49.562279Z",
     "iopub.status.busy": "2025-07-13T18:25:49.562038Z",
     "iopub.status.idle": "2025-07-13T18:25:49.579968Z",
     "shell.execute_reply": "2025-07-13T18:25:49.579419Z",
     "shell.execute_reply.started": "2025-07-13T18:25:49.562257Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "class OptimizedDataCollator:\n",
    "\n",
    "    def __init__(self, tokenizer, pad_token_id=None, label_pad_token_id=-100, pad_to_multiple_of=8):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.pad_token_id = pad_token_id if pad_token_id is not None else tokenizer.pad_token_id\n",
    "        self.label_pad_token_id = label_pad_token_id\n",
    "        self.pad_to_multiple_of = pad_to_multiple_of\n",
    "    \n",
    "    def __call__(self, features):\n",
    "        \"\"\"\n",
    "        Efficiently pad and convert lists directly to tensors.\n",
    "        \"\"\"\n",
    "        # Extract sequences (should all be lists now)\n",
    "        input_ids = [feature[\"input_ids\"] for feature in features]\n",
    "        attention_masks = [feature[\"attention_mask\"] for feature in features]\n",
    "        labels = [feature[\"labels\"] for feature in features]\n",
    "        \n",
    "        # Find max length for padding\n",
    "        max_length = max(len(seq) for seq in input_ids)\n",
    "        \n",
    "        # Round up to multiple of pad_to_multiple_of for tensor core optimization\n",
    "        if self.pad_to_multiple_of:\n",
    "            max_length = ((max_length + self.pad_to_multiple_of - 1) \n",
    "                         // self.pad_to_multiple_of * self.pad_to_multiple_of)\n",
    "        \n",
    "        # Pad sequences efficiently\n",
    "        padded_input_ids = []\n",
    "        padded_attention_masks = []\n",
    "        padded_labels = []\n",
    "        \n",
    "        for i in range(len(features)):\n",
    "            # Calculate padding needed\n",
    "            seq_len = len(input_ids[i])\n",
    "            padding_length = max_length - seq_len\n",
    "            \n",
    "            # Pad input_ids\n",
    "            padded_input_ids.append(\n",
    "                input_ids[i] + [self.pad_token_id] * padding_length\n",
    "            )\n",
    "            \n",
    "            # Pad attention_mask\n",
    "            padded_attention_masks.append(\n",
    "                attention_masks[i] + [0] * padding_length\n",
    "            )\n",
    "            \n",
    "            # Pad labels\n",
    "            padded_labels.append(\n",
    "                labels[i] + [self.label_pad_token_id] * padding_length\n",
    "            )\n",
    "        \n",
    "        # Convert to tensors efficiently (all at once)\n",
    "        batch = {\n",
    "            \"input_ids\": torch.tensor(padded_input_ids, dtype=torch.long),\n",
    "            \"attention_mask\": torch.tensor(padded_attention_masks, dtype=torch.long),\n",
    "            \"labels\": torch.tensor(padded_labels, dtype=torch.long),\n",
    "        }\n",
    "        \n",
    "        return batch\n",
    "\n",
    "\n",
    "\n",
    "# Setup tokenizer padding\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.add_special_tokens({\"pad_token\": tokenizer.eos_token})\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "model.config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "# Create data collator\n",
    "data_collator = OptimizedDataCollator(\n",
    "    tokenizer=tokenizer,\n",
    "    pad_token_id=tokenizer.pad_token_id,\n",
    "    label_pad_token_id=-100,\n",
    "    pad_to_multiple_of=config.PAD_TO_MULTIPLE_OF,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-13T18:25:49.581001Z",
     "iopub.status.busy": "2025-07-13T18:25:49.580771Z",
     "iopub.status.idle": "2025-07-13T18:25:49.602366Z",
     "shell.execute_reply": "2025-07-13T18:25:49.601765Z",
     "shell.execute_reply.started": "2025-07-13T18:25:49.580980Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'attention_mask', 'labels'],\n",
       "    num_rows: 10000\n",
       "})"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-13T18:25:49.603281Z",
     "iopub.status.busy": "2025-07-13T18:25:49.603055Z",
     "iopub.status.idle": "2025-07-13T18:25:49.617564Z",
     "shell.execute_reply": "2025-07-13T18:25:49.617056Z",
     "shell.execute_reply.started": "2025-07-13T18:25:49.603257Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaders created:\n",
      "Train batches: 5000\n",
      "Eval batches: 500\n",
      "Test batches: 100\n"
     ]
    }
   ],
   "source": [
    "def create_dataloaders(train_ds, val_ds, test_ds, data_collator, config):\n",
    "    \"\"\"Creates optimized data loaders for P100 GPU\"\"\"\n",
    "    \n",
    "    train_loader = DataLoader(\n",
    "        train_ds,\n",
    "        batch_size=config.BATCH_SIZE,\n",
    "        shuffle=True,\n",
    "        collate_fn=data_collator,\n",
    "        num_workers=0,  \n",
    "        pin_memory=True,\n",
    "        drop_last=True, \n",
    "    )\n",
    "    \n",
    "    eval_loader = DataLoader(\n",
    "        val_ds,\n",
    "        batch_size=config.BATCH_SIZE ,  \n",
    "        shuffle=False,\n",
    "        collate_fn=data_collator,\n",
    "        num_workers=0,\n",
    "        pin_memory=True,\n",
    "    )\n",
    "    \n",
    "    test_loader = DataLoader(\n",
    "        test_ds,\n",
    "        batch_size=config.BATCH_SIZE ,\n",
    "        shuffle=False,\n",
    "        collate_fn=data_collator,\n",
    "        num_workers=0,\n",
    "        pin_memory=True,\n",
    "    )\n",
    "    \n",
    "    return train_loader, eval_loader, test_loader\n",
    "\n",
    "\n",
    "train_loader, eval_loader, test_loader = create_dataloaders(\n",
    "    train_tokenized, validation_tokenized, test_tokenized, data_collator, config\n",
    ")\n",
    "\n",
    "print(f\"Data loaders created:\")\n",
    "print(f\"Train batches: {len(train_loader)}\")\n",
    "print(f\"Eval batches: {len(eval_loader)}\")\n",
    "print(f\"Test batches: {len(test_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-13T18:25:49.618965Z",
     "iopub.status.busy": "2025-07-13T18:25:49.618368Z",
     "iopub.status.idle": "2025-07-13T18:25:49.635956Z",
     "shell.execute_reply": "2025-07-13T18:25:49.635262Z",
     "shell.execute_reply.started": "2025-07-13T18:25:49.618948Z"
    }
   },
   "outputs": [],
   "source": [
    "#model has LoRA adapters added, but the original weights are frozen (not trainable). Only the adapters will learn during training, saving memory and time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-13T18:25:49.636990Z",
     "iopub.status.busy": "2025-07-13T18:25:49.636763Z",
     "iopub.status.idle": "2025-07-13T18:25:49.891652Z",
     "shell.execute_reply": "2025-07-13T18:25:49.891069Z",
     "shell.execute_reply.started": "2025-07-13T18:25:49.636969Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Setting up LoRA configuration...\n",
      " LoRA setup complete!\n",
      "   Total parameters: 601,096,192\n",
      "   Trainable parameters: 5,046,272\n",
      "   Trainable %: 0.84%\n"
     ]
    }
   ],
   "source": [
    "# creating the final model = base model + Adapter\n",
    "def setup_lora_model(model, config):\n",
    "    \"\"\"\n",
    "    Configures and applies LoRA to the model.\n",
    "    \n",
    "    LoRA works by decomposing weight updates into two smaller matrices:\n",
    "    Instead of updating W directly, we learn ΔW = A × B where A and B are much smaller.\n",
    "    \"\"\"   \n",
    "    print(\" Setting up LoRA configuration...\")\n",
    "    \n",
    "    peft_config = LoraConfig(\n",
    "        r=config.LORA_RANK, \n",
    "        lora_alpha=config.LORA_ALPHA,  \n",
    "        lora_dropout=config.LORA_DROPOUT, \n",
    "        bias=\"none\", \n",
    "        target_modules=\"all-linear\", \n",
    "        task_type=\"CAUSAL_LM\", \n",
    "    )\n",
    "    \n",
    "    # Apply LoRA to model\n",
    "    model = get_peft_model(model, peft_config)\n",
    "    \n",
    "    # Print trainable parameters info\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    \n",
    "    print(f\" LoRA setup complete!\")\n",
    "    print(f\"   Total parameters: {total_params:,}\")\n",
    "    print(f\"   Trainable parameters: {trainable_params:,}\")\n",
    "    print(f\"   Trainable %: {100 * trainable_params / total_params:.2f}%\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Apply LoRA\n",
    "model = setup_lora_model(model, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-13T18:25:49.892734Z",
     "iopub.status.busy": "2025-07-13T18:25:49.892518Z",
     "iopub.status.idle": "2025-07-13T18:25:49.907504Z",
     "shell.execute_reply": "2025-07-13T18:25:49.906772Z",
     "shell.execute_reply.started": "2025-07-13T18:25:49.892693Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Optimizer and scheduler setup:\n",
      "   Learning rate: 0.0002\n",
      "   Total training steps: 1666\n",
      "   Warmup steps: 100\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def setup_optimizer_and_scheduler(model, train_loader, config):\n",
    "    \"\"\"\n",
    "    Creates optimizer with proper weight decay and learning rate scheduler.\n",
    "    \"\"\"    \n",
    "    # Get trainable parameters only (LoRA adapters)\n",
    "    trainable_params = [p for p in model.parameters() if p.requires_grad]\n",
    "    \n",
    "    # Separate parameters that should/shouldn't have weight decay\n",
    "    decay_params = []\n",
    "    no_decay_params = []\n",
    "    \n",
    "    for name, param in model.named_parameters():\n",
    "        if param.requires_grad:\n",
    "            if any(nd in name for nd in [\"bias\", \"layer_norm\", \"layernorm\"]):\n",
    "                no_decay_params.append(param)\n",
    "            else:\n",
    "                decay_params.append(param)\n",
    "    \n",
    "    # Create parameter groups\n",
    "    optimizer_grouped_parameters = [\n",
    "        {\n",
    "            \"params\": decay_params,\n",
    "            \"weight_decay\": config.WEIGHT_DECAY,\n",
    "        },\n",
    "        {\n",
    "            \"params\": no_decay_params,\n",
    "            \"weight_decay\": 0.0,\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    # Create optimizer\n",
    "    optimizer = AdamW(\n",
    "        optimizer_grouped_parameters,\n",
    "        lr=config.LEARNING_RATE,\n",
    "        betas=(config.BETA1, config.BETA2),\n",
    "        eps=config.EPS,\n",
    "    )\n",
    "    \n",
    "    # Calculate total training steps\n",
    "    total_steps = len(train_loader) * config.NUM_EPOCHS // config.GRADIENT_ACCUMULATION_STEPS\n",
    "    \n",
    "    # Create learning rate scheduler\n",
    "    scheduler = CosineAnnealingLR(\n",
    "        optimizer,\n",
    "        T_max=total_steps,\n",
    "        eta_min=config.LEARNING_RATE * 0.1  # End at 10% of initial LR\n",
    "    )\n",
    "    \n",
    "    print(f\" Optimizer and scheduler setup:\")\n",
    "    print(f\"   Learning rate: {config.LEARNING_RATE}\")\n",
    "    print(f\"   Total training steps: {total_steps}\")\n",
    "    print(f\"   Warmup steps: {config.WARMUP_STEPS}\")\n",
    "    \n",
    "    return optimizer, scheduler, total_steps\n",
    "\n",
    "optimizer, scheduler, total_steps = setup_optimizer_and_scheduler(model, train_loader, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-13T18:25:49.909088Z",
     "iopub.status.busy": "2025-07-13T18:25:49.908326Z",
     "iopub.status.idle": "2025-07-13T18:25:57.197280Z",
     "shell.execute_reply": "2025-07-13T18:25:57.196756Z",
     "shell.execute_reply.started": "2025-07-13T18:25:49.909066Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.20.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/kaggle/working/wandb/run-20250713_182549-2kfoqqwb</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/22cs3033-rgipt-jais/lora-marathi-translation/runs/2kfoqqwb' target=\"_blank\">lora-r8-alpha16-20250713_1825</a></strong> to <a href='https://wandb.ai/22cs3033-rgipt-jais/lora-marathi-translation' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/22cs3033-rgipt-jais/lora-marathi-translation' target=\"_blank\">https://wandb.ai/22cs3033-rgipt-jais/lora-marathi-translation</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/22cs3033-rgipt-jais/lora-marathi-translation/runs/2kfoqqwb' target=\"_blank\">https://wandb.ai/22cs3033-rgipt-jais/lora-marathi-translation/runs/2kfoqqwb</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def init_wandb(config, model):\n",
    "    \"\"\"Initialize Weights & Biases for experiment tracking\"\"\"\n",
    "    \n",
    "    wandb.init(\n",
    "        project=\"lora-marathi-translation\",\n",
    "        name=f\"lora-r{config.LORA_RANK}-alpha{config.LORA_ALPHA}-{datetime.now().strftime('%Y%m%d_%H%M')}\",\n",
    "        config={\n",
    "            # LoRA settings\n",
    "            \"lora_rank\": config.LORA_RANK,\n",
    "            \"lora_alpha\": config.LORA_ALPHA, \n",
    "            \"lora_dropout\": config.LORA_DROPOUT,\n",
    "            \n",
    "            # Training settings\n",
    "            \"learning_rate\": config.LEARNING_RATE,\n",
    "            \"batch_size\": config.BATCH_SIZE,\n",
    "            \"gradient_accumulation_steps\": config.GRADIENT_ACCUMULATION_STEPS,\n",
    "            \"num_epochs\": config.NUM_EPOCHS,\n",
    "            \"max_grad_norm\": config.MAX_GRAD_NORM,\n",
    "            \n",
    "            # Model info\n",
    "            \"model_name\": model.config.name_or_path if hasattr(model.config, 'name_or_path') else \"unknown\",\n",
    "            \"total_params\": sum(p.numel() for p in model.parameters()),\n",
    "            \"trainable_params\": sum(p.numel() for p in model.parameters() if p.requires_grad),\n",
    "        }\n",
    "    )\n",
    "    \n",
    "\n",
    "init_wandb(config, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-13T18:25:57.198332Z",
     "iopub.status.busy": "2025-07-13T18:25:57.198062Z",
     "iopub.status.idle": "2025-07-13T18:25:57.205502Z",
     "shell.execute_reply": "2025-07-13T18:25:57.204909Z",
     "shell.execute_reply.started": "2025-07-13T18:25:57.198308Z"
    }
   },
   "outputs": [],
   "source": [
    "def evaluate_model(model, dataloader, device, desc=\"Evaluating\"):\n",
    "    \"\"\"\n",
    "    Comprehensive model evaluation with multiple metrics.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    total_tokens = 0\n",
    "    losses = []\n",
    "    \n",
    "    eval_bar = tqdm(dataloader, desc=desc, leave=False)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in eval_bar:\n",
    "            # Move to device\n",
    "            batch = {k: v.to(device, non_blocking=True) for k, v in batch.items()}\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(**batch)\n",
    "            loss = outputs.loss\n",
    "            \n",
    "            # Calculate metrics\n",
    "            batch_loss = loss.item()\n",
    "            batch_tokens = (batch[\"labels\"] != -100).sum().item()\n",
    "            \n",
    "            total_loss += batch_loss * batch_tokens\n",
    "            total_tokens += batch_tokens\n",
    "            losses.append(batch_loss)\n",
    "            \n",
    "            eval_bar.set_postfix({\n",
    "                \"loss\": f\"{batch_loss:.4f}\",\n",
    "                \"avg_loss\": f\"{total_loss/total_tokens:.4f}\"\n",
    "            })\n",
    "    \n",
    "    avg_loss = total_loss / total_tokens if total_tokens > 0 else float('inf')\n",
    "    perplexity = torch.exp(torch.tensor(avg_loss)).item()\n",
    "    \n",
    "    return {\n",
    "        \"avg_loss\": avg_loss,\n",
    "        \"perplexity\": perplexity,\n",
    "        \"total_tokens\": total_tokens,\n",
    "        \"num_batches\": len(losses)\n",
    "    }\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-13T18:25:57.207278Z",
     "iopub.status.busy": "2025-07-13T18:25:57.206290Z",
     "iopub.status.idle": "2025-07-13T18:25:57.224184Z",
     "shell.execute_reply": "2025-07-13T18:25:57.223627Z",
     "shell.execute_reply.started": "2025-07-13T18:25:57.207240Z"
    }
   },
   "outputs": [],
   "source": [
    "def save_checkpoint(model, tokenizer, optimizer, scheduler, step, loss, output_dir, is_best=False, epoch=None):  # Add epoch param\n",
    "       \"\"\"Save model checkpoint with improved management - only keep latest + best\"\"\"\n",
    "       scaler = GradScaler()\n",
    "       if is_best:\n",
    "           checkpoint_name = \"best-checkpoint\"\n",
    "       else:\n",
    "           checkpoint_name = \"latest-checkpoint\"  \n",
    "       \n",
    "       checkpoint_dir = os.path.join(output_dir, checkpoint_name)\n",
    "       \n",
    "       # Remove existing latest checkpoint before saving new one\n",
    "       if not is_best and os.path.exists(checkpoint_dir):\n",
    "           try:\n",
    "               shutil.rmtree(checkpoint_dir)\n",
    "               print(f\"🗑️ Removed previous latest checkpoint\")\n",
    "           except OSError as e:\n",
    "               print(f\"Warning: Could not remove old checkpoint: {e}\")\n",
    "       \n",
    "       os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "       \n",
    "       # Save model and tokenizer\n",
    "       model.save_pretrained(checkpoint_dir)\n",
    "       tokenizer.save_pretrained(checkpoint_dir)\n",
    "       \n",
    "       # Save training state\n",
    "       torch.save({\n",
    "           'step': step,\n",
    "           'epoch': epoch,  # NEW: Save current epoch (0-based)\n",
    "           'model_state_dict': model.state_dict(),\n",
    "           'optimizer_state_dict': optimizer.state_dict(),\n",
    "           'scheduler_state_dict': scheduler.state_dict(),\n",
    "           'loss': loss,\n",
    "           'scaler_state_dict': scaler.state_dict(),\n",
    "       }, os.path.join(checkpoint_dir, 'training_state.pt'))\n",
    "       \n",
    "       # Save metadata\n",
    "       metadata = {\n",
    "           \"step\": step,\n",
    "           \"epoch\": epoch,  # NEW\n",
    "           \"loss\": loss,\n",
    "           \"timestamp\": datetime.now().isoformat(),\n",
    "           \"is_best\": is_best\n",
    "       }\n",
    "       \n",
    "       with open(os.path.join(checkpoint_dir, 'metadata.json'), 'w') as f:\n",
    "           json.dump(metadata, f, indent=2)\n",
    "       \n",
    "       if is_best:\n",
    "           print(f\" Best checkpoint saved: {checkpoint_dir}\")\n",
    "       else:\n",
    "           print(f\" Latest checkpoint saved: {checkpoint_dir}\")\n",
    "       \n",
    "       return checkpoint_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-13T18:25:57.225148Z",
     "iopub.status.busy": "2025-07-13T18:25:57.224911Z",
     "iopub.status.idle": "2025-07-13T18:50:06.644458Z",
     "shell.execute_reply": "2025-07-13T18:50:06.643580Z",
     "shell.execute_reply.started": "2025-07-13T18:25:57.225122Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Training on device: cuda\n",
      " Starting training for 1 epochs\n",
      "   Effective batch size: 6\n",
      "   Total steps: 1666\n",
      "   Checkpoints saved to: lora-checkpoints-20250713_1825\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_36/2233922602.py:24: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler()\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14ed217a92e841eaa7cd759c87b734b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1/1:   0%|          | 0/5000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_36/2233922602.py:44: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Running evaluation at step 1000\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Validation loss: 1.2388\n",
      "   Perplexity: 3.45\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_36/3401146592.py:3: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Best checkpoint saved: lora-checkpoints-20250713_1825/best-checkpoint\n",
      "\n",
      " Epoch 1 Summary:\n",
      "   Average loss: 0.4291\n",
      "   Steps completed: 5000\n",
      "   Global step: 1666\n",
      "\n",
      " Final evaluation...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Final Evaluation:   0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Test Evaluation:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Final Results:\n",
      "   Best validation loss: 1.2388\n",
      "   Final validation loss: 1.2088\n",
      "   Test loss: 1.2244\n",
      "   Test perplexity: 3.40\n",
      " Latest checkpoint saved: lora-checkpoints-20250713_1825/latest-checkpoint\n",
      " Training completed! Models saved in: lora-checkpoints-20250713_1825\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def train_model():\n",
    "    \"\"\"\n",
    "    Main training loop with comprehensive monitoring and checkpointing.\n",
    "    \"\"\"\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\" Training on device: {device}\")\n",
    "    \n",
    "    # Move model to device\n",
    "    model.to(device)\n",
    "    \n",
    "    # Training tracking variables\n",
    "    global_step = 0\n",
    "    best_eval_loss = float('inf')\n",
    "    saved_checkpoints = []\n",
    "    \n",
    "    # Create output directory\n",
    "    output_dir = f\"lora-checkpoints-{datetime.now().strftime('%Y%m%d_%H%M')}\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    print(f\" Starting training for {config.NUM_EPOCHS} epochs\")\n",
    "    print(f\"   Effective batch size: {config.BATCH_SIZE * config.GRADIENT_ACCUMULATION_STEPS}\")\n",
    "    print(f\"   Total steps: {total_steps}\")\n",
    "    print(f\"   Checkpoints saved to: {output_dir}\")\n",
    "    scaler = GradScaler()\n",
    "    for epoch in range(config.NUM_EPOCHS):\n",
    "        model.train()\n",
    "        \n",
    "        # Epoch tracking\n",
    "        epoch_loss = 0.0\n",
    "        epoch_steps = 0\n",
    "        accumulated_loss = 0.0\n",
    "        \n",
    "        # Progress bar for epoch\n",
    "        epoch_bar = tqdm(\n",
    "            train_loader, \n",
    "            desc=f\"Epoch {epoch+1}/{config.NUM_EPOCHS}\",\n",
    "            total=len(train_loader)\n",
    "        )\n",
    "        \n",
    "        for step, batch in enumerate(epoch_bar):\n",
    "            # Move batch to device\n",
    "            batch = {k: v.to(device, non_blocking=True) for k, v in batch.items()}\n",
    "\n",
    "            with torch.cuda.amp.autocast():\n",
    "                \n",
    "                # NEW – fp16 forward/backward\n",
    "                outputs = model(**batch)\n",
    "                loss = outputs.loss\n",
    "                loss = loss / config.GRADIENT_ACCUMULATION_STEPS\n",
    "\n",
    "            \n",
    "            # # Forward pass\n",
    "            # outputs = model(**batch)\n",
    "            # loss = outputs.loss\n",
    "            \n",
    "            # # Scale loss for gradient accumulation\n",
    "            # loss = loss / config.GRADIENT_ACCUMULATION_STEPS\n",
    "            scaler.scale(loss).backward()\n",
    "            # Backward pass\n",
    "            \n",
    "            \n",
    "            # Accumulate loss\n",
    "            accumulated_loss += loss.item()\n",
    "            epoch_loss += loss.item()\n",
    "            \n",
    "            # Update weights every GRADIENT_ACCUMULATION_STEPS\n",
    "            if (step + 1) % config.GRADIENT_ACCUMULATION_STEPS == 0:\n",
    "                # Clip gradients\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), config.MAX_GRAD_NORM)\n",
    "\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), config.MAX_GRAD_NORM)\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "                scheduler.step()\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "\n",
    "                global_step += 1\n",
    "                \n",
    "                # Log to wandb\n",
    "                current_lr = scheduler.get_last_lr()[0]\n",
    "                wandb.log({\n",
    "                    \"train/loss\": accumulated_loss,\n",
    "                    \"train/learning_rate\": current_lr,\n",
    "                    \"train/epoch\": epoch + (step + 1) / len(train_loader),\n",
    "                    \"train/global_step\": global_step\n",
    "                }, step=global_step)\n",
    "                \n",
    "                # Update progress bar\n",
    "                epoch_bar.set_postfix({\n",
    "                    \"loss\": f\"{accumulated_loss:.4f}\",\n",
    "                    \"lr\": f\"{current_lr:.2e}\",\n",
    "                    \"step\": global_step\n",
    "                })\n",
    "                \n",
    "                # Reset accumulated loss\n",
    "                accumulated_loss = 0.0\n",
    "                \n",
    "                # Evaluation\n",
    "                if global_step % config.EVAL_EVERY_N_STEPS == 0:\n",
    "                    print(f\"\\n Running evaluation at step {global_step}\")\n",
    "                    \n",
    "                    eval_results = evaluate_model(model, eval_loader, device, \"Validation\")\n",
    "                    \n",
    "                    # Log evaluation results\n",
    "                    wandb.log({\n",
    "                        \"eval/loss\": eval_results[\"avg_loss\"],\n",
    "                        \"eval/perplexity\": eval_results[\"perplexity\"],\n",
    "                        \"eval/tokens\": eval_results[\"total_tokens\"]\n",
    "                    }, step=global_step)\n",
    "                    \n",
    "                    print(f\"   Validation loss: {eval_results['avg_loss']:.4f}\")\n",
    "                    print(f\"   Perplexity: {eval_results['perplexity']:.2f}\")\n",
    "                    \n",
    "                    # Save best model\n",
    "                    if eval_results[\"avg_loss\"] < best_eval_loss:\n",
    "                        best_eval_loss = eval_results[\"avg_loss\"]\n",
    "                        save_checkpoint(\n",
    "                            model, tokenizer, optimizer, scheduler,\n",
    "                            global_step, eval_results[\"avg_loss\"], output_dir, is_best=True\n",
    "                        )\n",
    "                        wandb.log({\"eval/best_loss\": best_eval_loss}, step=global_step)\n",
    "                    \n",
    "                    model.train()  # Back to training mode\n",
    "                \n",
    "                # Save regular checkpoint\n",
    "                if global_step % config.SAVE_EVERY_N_STEPS == 0:\n",
    "                    checkpoint_dir = save_checkpoint(\n",
    "                        model, tokenizer, optimizer, scheduler,\n",
    "                        global_step, accumulated_loss, output_dir, is_best=False\n",
    "                    )\n",
    "            \n",
    "            epoch_steps += 1\n",
    "            \n",
    "            # Memory cleanup\n",
    "            if step % 50 == 0:\n",
    "                torch.cuda.empty_cache()\n",
    "                gc.collect()\n",
    "        \n",
    "        # End of epoch summary\n",
    "        avg_epoch_loss = epoch_loss / epoch_steps\n",
    "        print(f\"\\n Epoch {epoch+1} Summary:\")\n",
    "        print(f\"   Average loss: {avg_epoch_loss:.4f}\")\n",
    "        print(f\"   Steps completed: {epoch_steps}\")\n",
    "        print(f\"   Global step: {global_step}\")\n",
    "        \n",
    "        wandb.log({\n",
    "            \"epoch/avg_loss\": avg_epoch_loss,\n",
    "            \"epoch/number\": epoch + 1\n",
    "        }, step=global_step)\n",
    "    \n",
    "    # Final evaluation\n",
    "    print(\"\\n Final evaluation...\")\n",
    "    final_eval = evaluate_model(model, eval_loader, device, \"Final Evaluation\")\n",
    "    test_eval = evaluate_model(model, test_loader, device, \"Test Evaluation\")\n",
    "    \n",
    "    # Log final results\n",
    "    wandb.log({\n",
    "        \"final/eval_loss\": final_eval[\"avg_loss\"],\n",
    "        \"final/eval_perplexity\": final_eval[\"perplexity\"],\n",
    "        \"final/test_loss\": test_eval[\"avg_loss\"],\n",
    "        \"final/test_perplexity\": test_eval[\"perplexity\"]\n",
    "    })\n",
    "    \n",
    "    print(f\" Final Results:\")\n",
    "    print(f\"   Best validation loss: {best_eval_loss:.4f}\")\n",
    "    print(f\"   Final validation loss: {final_eval['avg_loss']:.4f}\")\n",
    "    print(f\"   Test loss: {test_eval['avg_loss']:.4f}\")\n",
    "    print(f\"   Test perplexity: {test_eval['perplexity']:.2f}\")\n",
    "    \n",
    "    # Save final model\n",
    "    final_checkpoint = save_checkpoint(\n",
    "        model, tokenizer, optimizer, scheduler,\n",
    "        global_step, final_eval[\"avg_loss\"], output_dir\n",
    "    )\n",
    "    \n",
    "    print(f\" Training completed! Models saved in: {output_dir}\")\n",
    "    return output_dir, best_eval_loss\n",
    "\n",
    "# Run training\n",
    "output_dir, best_loss = train_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-13T18:50:06.645893Z",
     "iopub.status.busy": "2025-07-13T18:50:06.645639Z",
     "iopub.status.idle": "2025-07-13T18:50:10.741901Z",
     "shell.execute_reply": "2025-07-13T18:50:10.741094Z",
     "shell.execute_reply.started": "2025-07-13T18:50:06.645875Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Testing model inference...\n",
      "\n",
      " Test Case 1:\n",
      "English: Good morning, how are you today?\n",
      "Generated Marathi: माझ्या दिवसाने काय करत आहोत?\n",
      "\n",
      " Test Case 2:\n",
      "English: I love learning new languages.\n",
      "Generated Marathi: या भाषेचा ज्यासाठी मनाचा सुद्धा बनवावा.\n"
     ]
    }
   ],
   "source": [
    "def test_model_inference(model, tokenizer, device, test_cases=None):\n",
    "    \"\"\"Test the fine-tuned model with sample inputs\"\"\"\n",
    "    \n",
    "    if test_cases is None:\n",
    "        test_cases = [\n",
    "            {\n",
    "                \"instruction\": \"Translate the following English text to Marathi:\",\n",
    "                \"english\": \"Good morning, how are you today?\",\n",
    "            },\n",
    "            {\n",
    "                \"instruction\": \"Convert this English sentence to Marathi:\",\n",
    "                \"english\": \"I love learning new languages.\",\n",
    "            }\n",
    "        ]\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    print(\" Testing model inference...\")\n",
    "    \n",
    "    for i, test_case in enumerate(test_cases):\n",
    "        prompt = f\"{test_case['instruction']} English: {test_case['english']} Marathi:\"\n",
    "        \n",
    "        # Tokenize input\n",
    "        inputs = tokenizer(\n",
    "            prompt,\n",
    "            return_tensors=\"pt\",\n",
    "            truncation=True,\n",
    "            max_length=config.MAX_LENGTH // 2\n",
    "        ).to(device)\n",
    "        \n",
    "        # Generate response\n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=1024,\n",
    "                do_sample=True,\n",
    "                temperature=0.7,\n",
    "                top_p=0.9,\n",
    "                pad_token_id=tokenizer.pad_token_id,\n",
    "                eos_token_id=tokenizer.eos_token_id,\n",
    "            )\n",
    "        \n",
    "        # Decode response\n",
    "        generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        \n",
    "        # Extract just the Marathi translation\n",
    "        marathi_translation = generated_text[len(prompt):].strip()\n",
    "        \n",
    "        print(f\"\\n Test Case {i+1}:\")\n",
    "        print(f\"English: {test_case['english']}\")\n",
    "        print(f\"Generated Marathi: {marathi_translation}\")\n",
    "\n",
    "# Test the model\n",
    "test_model_inference(model, tokenizer, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-13T18:50:10.743005Z",
     "iopub.status.busy": "2025-07-13T18:50:10.742751Z",
     "iopub.status.idle": "2025-07-13T18:56:49.841014Z",
     "shell.execute_reply": "2025-07-13T18:56:49.840052Z",
     "shell.execute_reply.started": "2025-07-13T18:50:10.742982Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compiling model for faster inference...\n",
      "Evaluating model and calculating SacreBLEU score...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48e842fbb2a54c0e987329525baee49e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4ca2de3465244a083072a73baeb4baf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating Predictions:   0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SacreBLEU on test set: 0.29\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import evaluate\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "def evaluate_model_bleu(model, tokenizer, df_test, batch_size=16):\n",
    "    print(\"Evaluating model and calculating SacreBLEU score...\")\n",
    "    metric = evaluate.load(\"sacrebleu\")\n",
    "    model.eval()  # Set model to inference mode\n",
    "\n",
    "    # This is a standard practice for decoder-only models.\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "        model.config.pad_token_id = model.config.eos_token_id\n",
    "\n",
    "    predictions, refs = [], []\n",
    "\n",
    "    for i in tqdm(range(0, len(df_test), batch_size), desc=\"Generating Predictions\"):\n",
    "        batch = df_test[i : i + batch_size]\n",
    "        src_sentences = batch[\"english\"]\n",
    "        ref_sentences = batch[\"marathi\"]\n",
    "\n",
    "        prompts = [\n",
    "            f\"Convert the English text into Marathi language. English: {src} Marathi:\"\n",
    "            for src in src_sentences\n",
    "        ]\n",
    "\n",
    "        # Tokenize inputs\n",
    "        inputs = tokenizer(\n",
    "            prompts,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=512,\n",
    "            padding_side='left', \n",
    "        ).to(model.device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            generated = model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=512,\n",
    "                do_sample=False,  # Use greedy decoding\n",
    "                eos_token_id=tokenizer.eos_token_id,\n",
    "            )\n",
    "\n",
    "        # Efficiently slice and decode the entire batch at once\n",
    "        prompt_token_length = inputs.input_ids.shape[1]\n",
    "        generated_only_ids = generated[:, prompt_token_length:]  # Corrected line\n",
    "\n",
    "        # Use batch_decode for faster processing\n",
    "        decoded_preds = tokenizer.batch_decode(generated_only_ids, skip_special_tokens=True)\n",
    "\n",
    "        # Strip any extra whitespace from each prediction\n",
    "        predictions.extend([pred.strip() for pred in decoded_preds])\n",
    "        refs.extend([[t] for t in ref_sentences])\n",
    "\n",
    "    # Compute and return the final BLEU score\n",
    "    bleu_result = metric.compute(predictions=predictions, references=refs)\n",
    "    return bleu_result\n",
    "\n",
    "# Compile model for faster inference\n",
    "print(\"Compiling model for faster inference...\")\n",
    "model = torch.compile(model)\n",
    "\n",
    "# --- How to use the function ---\n",
    "# Assuming your model, tokenizer, and df_test are loaded\n",
    "bleu_score = evaluate_model_bleu(model, tokenizer, df_test)\n",
    "print(f\"SacreBLEU on test set: {bleu_score['score']:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-13T18:56:49.842008Z",
     "iopub.status.busy": "2025-07-13T18:56:49.841814Z",
     "iopub.status.idle": "2025-07-13T18:56:51.103294Z",
     "shell.execute_reply": "2025-07-13T18:56:51.102671Z",
     "shell.execute_reply.started": "2025-07-13T18:56:49.841992Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "🎉 TRAINING COMPLETE!\n",
      "==================================================\n",
      " Training Configuration:\n",
      "   LoRA Rank: 8\n",
      "   LoRA Alpha: 16\n",
      "   Learning Rate: 0.0002\n",
      "   Batch Size: 2\n",
      "   Epochs: 1\n",
      "\n",
      " Model saved to: lora-checkpoints-20250713_1825\n",
      "Best validation loss: 1.2388\n",
      "\n",
      "View training progress at: https://wandb.ai/22cs3033-rgipt-jais/lora-marathi-translation/runs/2kfoqqwb\n",
      "\n",
      "Next steps:\n",
      "   1. Test model on new examples\n",
      "   2. Fine-tune hyperparameters if needed\n",
      "   3. Deploy model for inference\n",
      "   4. Create LoRA adapters for other tasks\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch/avg_loss</td><td>▁</td></tr><tr><td>epoch/number</td><td>▁</td></tr><tr><td>eval/best_loss</td><td>▁</td></tr><tr><td>eval/loss</td><td>▁</td></tr><tr><td>eval/perplexity</td><td>▁</td></tr><tr><td>eval/tokens</td><td>▁</td></tr><tr><td>final/eval_loss</td><td>▁</td></tr><tr><td>final/eval_perplexity</td><td>▁</td></tr><tr><td>final/test_loss</td><td>▁</td></tr><tr><td>final/test_perplexity</td><td>▁</td></tr><tr><td>train/epoch</td><td>▁▁▁▁▁▂▂▂▂▂▂▂▂▂▂▃▃▃▄▄▅▅▅▅▅▆▆▆▆▆▆▆▆▇▇▇████</td></tr><tr><td>train/global_step</td><td>▁▁▁▂▂▂▂▃▃▃▃▃▃▃▃▄▅▅▅▅▅▆▆▆▆▆▆▆▆▆▇▇▇▇██████</td></tr><tr><td>train/learning_rate</td><td>██████▇▇▇▇▇▇▇▆▆▆▆▆▆▅▅▅▅▄▄▄▄▄▃▃▃▂▂▂▂▂▂▁▁▁</td></tr><tr><td>train/loss</td><td>█▅█▅▅▅▄▃▆▃▇▄▅▄▅▂▂▂▅▂▅▄▄▅▆▄▂▂▂▂▅▁▄▃▃▁▃▃▁▃</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch/avg_loss</td><td>0.42907</td></tr><tr><td>epoch/number</td><td>1</td></tr><tr><td>eval/best_loss</td><td>1.23878</td></tr><tr><td>eval/loss</td><td>1.23878</td></tr><tr><td>eval/perplexity</td><td>3.45141</td></tr><tr><td>eval/tokens</td><td>64291</td></tr><tr><td>final/eval_loss</td><td>1.20879</td></tr><tr><td>final/eval_perplexity</td><td>3.34942</td></tr><tr><td>final/test_loss</td><td>1.22444</td></tr><tr><td>final/test_perplexity</td><td>3.40228</td></tr><tr><td>train/epoch</td><td>0.9996</td></tr><tr><td>train/global_step</td><td>1666</td></tr><tr><td>train/learning_rate</td><td>2e-05</td></tr><tr><td>train/loss</td><td>1.26199</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">lora-r8-alpha16-20250713_1825</strong> at: <a href='https://wandb.ai/22cs3033-rgipt-jais/lora-marathi-translation/runs/2kfoqqwb' target=\"_blank\">https://wandb.ai/22cs3033-rgipt-jais/lora-marathi-translation/runs/2kfoqqwb</a><br> View project at: <a href='https://wandb.ai/22cs3033-rgipt-jais/lora-marathi-translation' target=\"_blank\">https://wandb.ai/22cs3033-rgipt-jais/lora-marathi-translation</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250713_182549-2kfoqqwb/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Memory cleanup completed!\n"
     ]
    }
   ],
   "source": [
    "def training_summary():\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"🎉 TRAINING COMPLETE!\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    print(f\" Training Configuration:\")\n",
    "    print(f\"   LoRA Rank: {config.LORA_RANK}\")\n",
    "    print(f\"   LoRA Alpha: {config.LORA_ALPHA}\")\n",
    "    print(f\"   Learning Rate: {config.LEARNING_RATE}\")\n",
    "    print(f\"   Batch Size: {config.BATCH_SIZE}\")\n",
    "    print(f\"   Epochs: {config.NUM_EPOCHS}\")\n",
    "    \n",
    "    print(f\"\\n Model saved to: {output_dir}\")\n",
    "    print(f\"Best validation loss: {best_loss:.4f}\")\n",
    "    \n",
    "    print(f\"\\nView training progress at: {wandb.run.url}\")\n",
    "    \n",
    "    print(\"\\nNext steps:\")\n",
    "    print(\"   1. Test model on new examples\")\n",
    "    print(\"   2. Fine-tune hyperparameters if needed\")\n",
    "    print(\"   3. Deploy model for inference\")\n",
    "    print(\"   4. Create LoRA adapters for other tasks\")\n",
    "\n",
    "training_summary()\n",
    "\n",
    "# Finish wandb run\n",
    "wandb.finish()\n",
    "\n",
    "# Memory cleanup\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "print(\"\\nMemory cleanup completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-13T18:56:51.104215Z",
     "iopub.status.busy": "2025-07-13T18:56:51.104011Z",
     "iopub.status.idle": "2025-07-13T18:56:51.564209Z",
     "shell.execute_reply": "2025-07-13T18:56:51.563563Z",
     "shell.execute_reply.started": "2025-07-13T18:56:51.104197Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Memory cleanup\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-13T10:26:09.969503Z",
     "iopub.status.busy": "2025-07-13T10:26:09.968772Z",
     "iopub.status.idle": "2025-07-13T10:26:09.989417Z",
     "shell.execute_reply": "2025-07-13T10:26:09.988731Z",
     "shell.execute_reply.started": "2025-07-13T10:26:09.969478Z"
    }
   },
   "outputs": [],
   "source": [
    "def resume_training(\n",
    "    resume_from_checkpoint: str,\n",
    "    base_model_name: str = \"Qwen/Qwen3-0.6B\",\n",
    "    resume_from_epoch: int = 2,\n",
    "    additional_epochs: int = 2,\n",
    "     output_dir: str | None = None,  \n",
    "):\n",
    "\n",
    "\n",
    "    import os, json, gc, evaluate, wandb, torch\n",
    "    from tqdm import tqdm\n",
    "    from datetime import datetime\n",
    "    from peft import PeftModel\n",
    "    from torch.cuda.amp import GradScaler, autocast\n",
    "    from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(\n",
    "        f\" Resuming training from: {resume_from_checkpoint}\\n\"\n",
    "        f\"   Starting at epoch {resume_from_epoch} for {additional_epochs} more.\"\n",
    "    )\n",
    "\n",
    "    output_dir = (\n",
    "        os.path.dirname(resume_from_checkpoint)\n",
    "        if \"latest-checkpoint\" in resume_from_checkpoint\n",
    "        else resume_from_checkpoint\n",
    "    )\n",
    "    training_state_path = os.path.join(resume_from_checkpoint, \"training_state.pt\")\n",
    "    if not os.path.exists(training_state_path):\n",
    "        raise FileNotFoundError(f\"No training_state.pt found in {resume_from_checkpoint}\")\n",
    "\n",
    "    training_state = torch.load(training_state_path, map_location=\"cpu\")\n",
    "    global_step_saved = training_state[\"step\"]\n",
    "\n",
    "    required_vocab_size = training_state[\"model_state_dict\"][\n",
    "        \"base_model.model.model.embed_tokens.weight\"\n",
    "    ].shape[0]\n",
    "    print(f\"Required vocab size in checkpoint: {required_vocab_size}\")\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(resume_from_checkpoint)\n",
    "    if len(tokenizer) < required_vocab_size:\n",
    "        extra = required_vocab_size - len(tokenizer)\n",
    "        tokenizer.add_tokens([f\"<extra_id_{i}>\" for i in range(extra)])\n",
    "        print(f\"➕ Added {extra} dummy tokens (tokenizer now {len(tokenizer)})\")\n",
    "\n",
    "\n",
    "    base_model = AutoModelForCausalLM.from_pretrained(base_model_name, low_cpu_mem_usage=True)\n",
    "    base_model.resize_token_embeddings(len(tokenizer))\n",
    "    base_model.config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "    model = PeftModel.from_pretrained(base_model, resume_from_checkpoint, is_trainable=True)\n",
    "    model.load_state_dict(training_state[\"model_state_dict\"])\n",
    "    model.to(device)\n",
    "    print(\"Model & tokenizer loaded – all shapes match\")\n",
    "\n",
    "\n",
    "    config.NUM_EPOCHS = (resume_from_epoch - 1) + additional_epochs\n",
    "    optimizer, scheduler, total_steps = setup_optimizer_and_scheduler(model, train_loader, config)\n",
    "\n",
    "    saved_opt_state = training_state[\"optimizer_state_dict\"]\n",
    "    if len(optimizer.param_groups) != len(saved_opt_state[\"param_groups\"]):\n",
    "        raise ValueError(\"Parameter-group mismatch with saved optimizer\")\n",
    "\n",
    "    # remap IDs → tensors\n",
    "    new_state = {}\n",
    "    cur_params = [p for g in optimizer.param_groups for p in g[\"params\"]]\n",
    "    saved_param_ids = [pid for g in saved_opt_state[\"param_groups\"] for pid in g[\"params\"]]\n",
    "    if len(cur_params) != len(saved_param_ids):\n",
    "        raise ValueError(\"Trainable-parameter count changed\")\n",
    "\n",
    "    for p_cur, p_id_saved in zip(cur_params, saved_param_ids):\n",
    "        if p_id_saved in saved_opt_state[\"state\"]:\n",
    "            new_state[p_cur] = saved_opt_state[\"state\"][p_id_saved]\n",
    "    optimizer.state = new_state\n",
    "\n",
    "    def _optimizer_to(opt, dev):\n",
    "        \"\"\"Move all tensors in optimizer.state to device `dev`\"\"\"\n",
    "        for st in opt.state.values():\n",
    "            for k, v in st.items():\n",
    "                if isinstance(v, torch.Tensor):\n",
    "                    st[k] = v.to(dev)\n",
    "        return opt\n",
    "\n",
    "    optimizer = _optimizer_to(optimizer, device)\n",
    "    print(\"Optimizer state restored & moved to\", device)\n",
    "\n",
    "    scheduler.load_state_dict(training_state[\"scheduler_state_dict\"])\n",
    "    scheduler.T_max = total_steps\n",
    "\n",
    "    scaler = GradScaler()\n",
    "    scaler.load_state_dict(training_state[\"scaler_state_dict\"])\n",
    "\n",
    "\n",
    "    best_eval_loss = float(\"inf\")\n",
    "    meta_path = os.path.join(output_dir, \"best-checkpoint\", \"metadata.json\")\n",
    "    if os.path.exists(meta_path):\n",
    "        best_eval_loss = json.load(open(meta_path))[\"loss\"]\n",
    "    print(f\"Best eval loss so far: {best_eval_loss:.4f} | global step: {global_step_saved}\")\n",
    "\n",
    "    # WandB \n",
    "    init_wandb(config, model)\n",
    "\n",
    "    # training loop \n",
    "    global_step = global_step_saved\n",
    "    start_epoch = resume_from_epoch - 1\n",
    "\n",
    "    for epoch in range(start_epoch, config.NUM_EPOCHS):\n",
    "        model.train()\n",
    "        epoch_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{config.NUM_EPOCHS}\")\n",
    "\n",
    "        acc_loss = tot_loss = steps = 0.0\n",
    "        for step, batch in enumerate(epoch_bar):\n",
    "            batch = {k: v.to(device, non_blocking=True) for k, v in batch.items()}\n",
    "\n",
    "            with autocast():\n",
    "                loss = model(**batch).loss / config.GRADIENT_ACCUMULATION_STEPS\n",
    "            scaler.scale(loss).backward()\n",
    "\n",
    "            acc_loss += loss.item()\n",
    "            tot_loss += loss.item()\n",
    "\n",
    "            if (step + 1) % config.GRADIENT_ACCUMULATION_STEPS == 0:\n",
    "                scaler.unscale_(optimizer)\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), config.MAX_GRAD_NORM)\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "                scheduler.step()\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                global_step += 1\n",
    "                lr_now = scheduler.get_last_lr()[0]\n",
    "                wandb.log(\n",
    "                    {\n",
    "                        \"train/loss\": acc_loss,\n",
    "                        \"train/lr\": lr_now,\n",
    "                        \"train/epoch\": epoch + (step + 1) / len(train_loader),\n",
    "                        \"train/step\": global_step,\n",
    "                    },\n",
    "                    step=global_step,\n",
    "                )\n",
    "                epoch_bar.set_postfix(loss=f\"{acc_loss:.4f}\", lr=f\"{lr_now:.2e}\")\n",
    "                acc_loss = 0.0\n",
    "\n",
    "                if global_step % config.EVAL_EVERY_N_STEPS == 0:\n",
    "                    eval_stats = evaluate_model(model, eval_loader, device, \"Validation\")\n",
    "                    wandb.log(\n",
    "                        {\n",
    "                            \"eval/loss\": eval_stats[\"avg_loss\"],\n",
    "                            \"eval/perplexity\": eval_stats[\"perplexity\"],\n",
    "                        },\n",
    "                        step=global_step,\n",
    "                    )\n",
    "                    if eval_stats[\"avg_loss\"] < best_eval_loss:\n",
    "                        best_eval_loss = eval_stats[\"avg_loss\"]\n",
    "                        save_checkpoint(\n",
    "                            model, tokenizer, optimizer, scheduler,\n",
    "                            global_step, best_eval_loss, output_dir, is_best=True,\n",
    "                        )\n",
    "\n",
    "                if global_step % config.SAVE_EVERY_N_STEPS == 0:\n",
    "                    save_checkpoint(\n",
    "                        model, tokenizer, optimizer, scheduler,\n",
    "                        global_step, loss.item(), output_dir, is_best=False,\n",
    "                    )\n",
    "\n",
    "            steps += 1\n",
    "            if step % 50 == 0:\n",
    "                torch.cuda.empty_cache()\n",
    "                gc.collect()\n",
    "\n",
    "        print(f\"Epoch {epoch+1} done | avg loss: {tot_loss/steps:.4f} | global step: {global_step}\")\n",
    "        wandb.log({\"epoch/avg_loss\": tot_loss / steps}, step=global_step)\n",
    "\n",
    "    # final evaluation \n",
    "    final_eval = evaluate_model(model, eval_loader, device, \"Final Evaluation\")\n",
    "    test_eval = evaluate_model(model, test_loader, device, \"Test Evaluation\")\n",
    "    wandb.log(\n",
    "        {\n",
    "            \"final/val_loss\": final_eval[\"avg_loss\"],\n",
    "            \"final/test_loss\": test_eval[\"avg_loss\"],\n",
    "            \"final/test_perplexity\": test_eval[\"perplexity\"],\n",
    "        },\n",
    "        step=global_step,\n",
    "    )\n",
    "\n",
    "    # decide where to save\n",
    "    if output_dir is None:\n",
    "        output_dir = (\n",
    "            os.path.dirname(resume_from_checkpoint)\n",
    "            if \"latest-checkpoint\" in resume_from_checkpoint\n",
    "            else resume_from_checkpoint\n",
    "        )\n",
    "    if not os.access(output_dir, os.W_OK):\n",
    "        ts = datetime.now().strftime(\"%Y%m%d_%H%M\")\n",
    "        output_dir = f\"/kaggle/working/lora-resume-{ts}\"\n",
    "        print(f\"Switched output_dir to writable location: {output_dir}\")\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    save_checkpoint(model, tokenizer, optimizer, scheduler, global_step, final_eval[\"avg_loss\"], output_dir)\n",
    "\n",
    "    print(\"🔧 Compiling model for inference …\")\n",
    "    model = torch.compile(model)\n",
    "    bleu = evaluate_model_bleu(model, tokenizer, df_test)\n",
    "    print(f\"SacreBLEU: {bleu['score']:.2f}\")\n",
    "\n",
    "    wandb.finish()\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    print(\"Resumed training complete. Models in:\", output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-07-13T10:49:02.907Z",
     "iopub.execute_input": "2025-07-13T10:26:12.190945Z",
     "iopub.status.busy": "2025-07-13T10:26:12.190457Z"
    }
   },
   "outputs": [],
   "source": [
    "resume_from_checkpoint = \"/kaggle/input/qwen3-finetuned/transformers/default/3/lora-checkpoints-20250712_1620/latest-checkpoint\"\n",
    "resume_training(resume_from_checkpoint, base_model_name=\"Qwen/Qwen3-0.6B\", resume_from_epoch=2, additional_epochs=2 ,  output_dir=\"/kaggle/working/lora-checkpoints-resume\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "isSourceIdPinned": true,
     "modelId": 398206,
     "modelInstanceId": 377671,
     "sourceId": 469375,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 31090,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
