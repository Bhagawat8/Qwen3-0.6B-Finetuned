{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":470281,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":379387,"modelId":399332}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd \n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-07-13T18:52:48.030425Z","iopub.execute_input":"2025-07-13T18:52:48.030920Z","iopub.status.idle":"2025-07-13T18:52:48.312728Z","shell.execute_reply.started":"2025-07-13T18:52:48.030894Z","shell.execute_reply":"2025-07-13T18:52:48.311761Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/trained_50k_46/transformers/default/1/__huggingface_repos__.json\n/kaggle/input/trained_50k_46/transformers/default/1/qwen_marathi/best-checkpoint-step/config.json\n/kaggle/input/trained_50k_46/transformers/default/1/qwen_marathi/best-checkpoint-step/merges.txt\n/kaggle/input/trained_50k_46/transformers/default/1/qwen_marathi/best-checkpoint-step/tokenizer.json\n/kaggle/input/trained_50k_46/transformers/default/1/qwen_marathi/best-checkpoint-step/vocab.json\n/kaggle/input/trained_50k_46/transformers/default/1/qwen_marathi/best-checkpoint-step/tokenizer_config.json\n/kaggle/input/trained_50k_46/transformers/default/1/qwen_marathi/best-checkpoint-step/chat_template.jinja\n/kaggle/input/trained_50k_46/transformers/default/1/qwen_marathi/best-checkpoint-step/model.safetensors\n/kaggle/input/trained_50k_46/transformers/default/1/qwen_marathi/best-checkpoint-step/special_tokens_map.json\n/kaggle/input/trained_50k_46/transformers/default/1/qwen_marathi/best-checkpoint-step/added_tokens.json\n/kaggle/input/trained_50k_46/transformers/default/1/qwen_marathi/best-checkpoint-step/generation_config.json\n/kaggle/input/trained_50k_46/transformers/default/1/wandb/run-20250713_113217-xstkihlr/run-xstkihlr.wandb\n/kaggle/input/trained_50k_46/transformers/default/1/wandb/run-20250713_113217-xstkihlr/logs/debug.log\n/kaggle/input/trained_50k_46/transformers/default/1/wandb/run-20250713_113217-xstkihlr/logs/debug-internal.log\n/kaggle/input/trained_50k_46/transformers/default/1/wandb/run-20250713_113217-xstkihlr/files/wandb-summary.json\n/kaggle/input/trained_50k_46/transformers/default/1/wandb/run-20250713_113217-xstkihlr/files/config.yaml\n/kaggle/input/trained_50k_46/transformers/default/1/wandb/run-20250713_113217-xstkihlr/files/output.log\n/kaggle/input/trained_50k_46/transformers/default/1/wandb/run-20250713_113217-xstkihlr/files/requirements.txt\n/kaggle/input/trained_50k_46/transformers/default/1/wandb/run-20250713_113217-xstkihlr/files/wandb-metadata.json\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"!pip install -q evaluate","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-13T18:52:48.314301Z","iopub.execute_input":"2025-07-13T18:52:48.314639Z","iopub.status.idle":"2025-07-13T18:52:53.008988Z","shell.execute_reply.started":"2025-07-13T18:52:48.314620Z","shell.execute_reply":"2025-07-13T18:52:53.008299Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.6/193.6 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nbigframes 2.8.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\ncesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\ntorch 2.6.0+cu124 requires nvidia-cublas-cu12==12.4.5.8; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cublas-cu12 12.5.3.2 which is incompatible.\ntorch 2.6.0+cu124 requires nvidia-cuda-cupti-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-cupti-cu12 12.5.82 which is incompatible.\ntorch 2.6.0+cu124 requires nvidia-cuda-nvrtc-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-nvrtc-cu12 12.5.82 which is incompatible.\ntorch 2.6.0+cu124 requires nvidia-cuda-runtime-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-runtime-cu12 12.5.82 which is incompatible.\ntorch 2.6.0+cu124 requires nvidia-cudnn-cu12==9.1.0.70; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cudnn-cu12 9.3.0.75 which is incompatible.\ntorch 2.6.0+cu124 requires nvidia-cufft-cu12==11.2.1.3; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cufft-cu12 11.2.3.61 which is incompatible.\ntorch 2.6.0+cu124 requires nvidia-curand-cu12==10.3.5.147; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-curand-cu12 10.3.6.82 which is incompatible.\ntorch 2.6.0+cu124 requires nvidia-cusolver-cu12==11.6.1.9; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusolver-cu12 11.6.3.83 which is incompatible.\ntorch 2.6.0+cu124 requires nvidia-cusparse-cu12==12.3.1.170; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusparse-cu12 12.5.1.3 which is incompatible.\ntorch 2.6.0+cu124 requires nvidia-nvjitlink-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-nvjitlink-cu12 12.5.82 which is incompatible.\ngcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2025.3.0 which is incompatible.\nbigframes 2.8.0 requires google-cloud-bigquery[bqstorage,pandas]>=3.31.0, but you have google-cloud-bigquery 3.25.0 which is incompatible.\nbigframes 2.8.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"!pip install -q sacrebleu","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-13T18:52:53.009827Z","iopub.execute_input":"2025-07-13T18:52:53.010038Z","iopub.status.idle":"2025-07-13T18:52:56.480929Z","shell.execute_reply.started":"2025-07-13T18:52:53.010016Z","shell.execute_reply":"2025-07-13T18:52:56.480234Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.8/51.8 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.1/104.1 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"!pip install -q wandb","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-13T18:52:56.483016Z","iopub.execute_input":"2025-07-13T18:52:56.483327Z","iopub.status.idle":"2025-07-13T18:52:59.526397Z","shell.execute_reply.started":"2025-07-13T18:52:56.483304Z","shell.execute_reply":"2025-07-13T18:52:59.525406Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"# imports\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, Trainer, TrainingArguments, DataCollatorWithPadding, get_scheduler\nfrom datasets import load_dataset\nimport torch\nimport wandb\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader\nfrom accelerate import Accelerator\nfrom transformers.optimization import AdafactorSchedule\nfrom transformers.optimization import get_cosine_schedule_with_warmup\nfrom accelerate import Accelerator\nfrom tqdm.auto import tqdm\nimport os\nimport math\nfrom transformers.optimization import Adafactor, get_scheduler\nimport evaluate\nimport shutil","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-13T18:52:59.527385Z","iopub.execute_input":"2025-07-13T18:52:59.527612Z","iopub.status.idle":"2025-07-13T18:53:30.465094Z","shell.execute_reply.started":"2025-07-13T18:52:59.527588Z","shell.execute_reply":"2025-07-13T18:53:30.464349Z"}},"outputs":[{"name":"stderr","text":"2025-07-13 18:53:13.661140: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1752432793.855963      36 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1752432793.919766      36 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"# checking GPU availability\nprint(f\"CUDA available: {torch.cuda.is_available()}\")\nprint(f\"Number of GPUs: {torch.cuda.device_count()}\")\nfor i in range(torch.cuda.device_count()):\n    print(f\"GPU {i}: {torch.cuda.get_device_name(i)}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-13T18:53:30.465939Z","iopub.execute_input":"2025-07-13T18:53:30.466136Z","iopub.status.idle":"2025-07-13T18:53:30.470709Z","shell.execute_reply.started":"2025-07-13T18:53:30.466109Z","shell.execute_reply":"2025-07-13T18:53:30.470003Z"}},"outputs":[{"name":"stdout","text":"CUDA available: True\nNumber of GPUs: 1\nGPU 0: Tesla P100-PCIE-16GB\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"# connect wandb library for graphs\nfrom kaggle_secrets import UserSecretsClient\n\nsecret = UserSecretsClient()\nos.environ[\"WANDB_API_KEY\"] = secret.get_secret(\"WANDB_KEY\") \n\nwandb.login()   ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-13T18:53:30.471466Z","iopub.execute_input":"2025-07-13T18:53:30.471817Z","iopub.status.idle":"2025-07-13T18:53:36.862852Z","shell.execute_reply.started":"2025-07-13T18:53:30.471794Z","shell.execute_reply":"2025-07-13T18:53:36.862293Z"}},"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33m22cs3033\u001b[0m (\u001b[33m22cs3033-rgipt-jais\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","output_type":"stream"},{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}],"execution_count":7},{"cell_type":"code","source":"# loading model and tokenizer into the RAM\nmodel_name = \"Qwen/Qwen3-0.6B\"\nmodel = AutoModelForCausalLM.from_pretrained(model_name, low_cpu_mem_usage=True )\nmodel.config.use_cache = False\ntokenizer = AutoTokenizer.from_pretrained(model_name)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-13T18:53:36.863656Z","iopub.execute_input":"2025-07-13T18:53:36.864844Z","iopub.status.idle":"2025-07-13T18:53:46.066595Z","shell.execute_reply.started":"2025-07-13T18:53:36.864814Z","shell.execute_reply":"2025-07-13T18:53:46.065463Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/726 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9344408947bb4686a347e6746aeca019"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/1.50G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"65c19c9ac17646e791ab0b4ae002ce1a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/239 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e968913a0764425a9a73ebbb60915601"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9cf3284bf767458ea85981e07011f6e9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a302a04f3e35494b85a052a00ed2bfd0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a51ed123dec245a19dd2fcc64258853a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/11.4M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0058c90910694a2eab3f9efbe6bacb08"}},"metadata":{}}],"execution_count":8},{"cell_type":"code","source":"# create the response on default pretrained model\nprompt = \"Dengue cases on the rise in city\"\nmessages = [\n    {\"role\": \"system\", \"content\": \"You are a translator. who knows marathai and english language very well. Your task is convert given english sentence in marathi.\"},\n    {\"role\": \"user\", \"content\": prompt}\n]\ntext = tokenizer.apply_chat_template(\n    messages,\n    tokenize=False,\n    add_generation_prompt=True\n)\nmodel_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n\ngenerated_ids = model.generate(\n    **model_inputs,\n    max_new_tokens=512\n)\ngenerated_ids = [\n    output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n]\n\nresponse = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-13T18:53:46.067732Z","iopub.execute_input":"2025-07-13T18:53:46.068733Z","iopub.status.idle":"2025-07-13T18:55:12.598154Z","shell.execute_reply.started":"2025-07-13T18:53:46.068712Z","shell.execute_reply":"2025-07-13T18:55:12.597341Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"response","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-13T18:55:12.600859Z","iopub.execute_input":"2025-07-13T18:55:12.601102Z","iopub.status.idle":"2025-07-13T18:55:12.606213Z","shell.execute_reply.started":"2025-07-13T18:55:12.601076Z","shell.execute_reply":"2025-07-13T18:55:12.605523Z"}},"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"'<think>\\nOkay, the user wants to translate the sentence \"Dengue cases on the rise in city\" into Marathi. Let me start by breaking down the sentence. \\n\\nFirst, \"Dengue cases\" – I know that in Marathi, \"dengue\" is often transliterated as \"देंगु\" or sometimes written as \"देंगु\" with the letter \"इ\". But since it\\'s a specific disease, maybe it\\'s better to keep it as \"देंगु\" for accuracy.\\n\\nNext, \"cases on the rise\" – \"cases\" here refers to the number of cases, so \"मामले\" is the correct term. \"On the rise\" can be translated as \"उपचार बरावे\" or \"उपचार बरावे बरावे\". Since \"on the rise\" is more like increasing, \"बरावे बरावे\" might be better.\\n\\nThen, \"in city\" – \"महानदर्शन\" or \"महानदर्शन\" is a common term for a city in Marathi. So putting it all together: \"देंगु मामले बरावे बरावे महानदर्शन बरावे बरावे बरावे बरावे बरावे बरावे बरावे बरावे बरावे बरावे बरावे बरावे बरावे बरावे बरावे बरावे बरावे बरावे बरावे बरावे बरावे बरावे बरावे बरावे बरावे बरावे बरावे बरावे बरावे बरावे बरावे बरावे बरावे बरावे बरावे बरावे ब'"},"metadata":{}}],"execution_count":10},{"cell_type":"code","source":"# load the english to marathi translation data from huggingface\nds = load_dataset(\"anujsahani01/English-Marathi\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-13T18:55:12.607050Z","iopub.execute_input":"2025-07-13T18:55:12.607295Z","iopub.status.idle":"2025-07-13T18:55:34.213141Z","shell.execute_reply.started":"2025-07-13T18:55:12.607279Z","shell.execute_reply":"2025-07-13T18:55:34.212587Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/206 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2a8ce4cf3a254f9f98ee97b9336477a1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train.csv:   0%|          | 0.00/621M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a2c5e076f3564509b05e2e632b72562d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"test.csv:   0%|          | 0.00/243M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5336833994f1416abf6a3f55e1f659a6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/2637962 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e744d7666c8a49d68e9a9abb308055eb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/879321 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3db9274f146d418dbb06a4262295ffd8"}},"metadata":{}}],"execution_count":11},{"cell_type":"code","source":"ds","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-13T18:55:34.213949Z","iopub.execute_input":"2025-07-13T18:55:34.214221Z","iopub.status.idle":"2025-07-13T18:55:34.219204Z","shell.execute_reply.started":"2025-07-13T18:55:34.214197Z","shell.execute_reply":"2025-07-13T18:55:34.218511Z"}},"outputs":[{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['english', 'marathi'],\n        num_rows: 2637962\n    })\n    test: Dataset({\n        features: ['english', 'marathi'],\n        num_rows: 879321\n    })\n})"},"metadata":{}}],"execution_count":12},{"cell_type":"code","source":"ds['train'][10]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-13T18:55:34.220041Z","iopub.execute_input":"2025-07-13T18:55:34.220277Z","iopub.status.idle":"2025-07-13T18:55:34.235417Z","shell.execute_reply.started":"2025-07-13T18:55:34.220254Z","shell.execute_reply":"2025-07-13T18:55:34.234712Z"}},"outputs":[{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"{'english': 'The funds collected are used for social purposes.',\n 'marathi': 'त्या निधीचा वापर सामाजिक कार्यासाठी करतो.'}"},"metadata":{}}],"execution_count":13},{"cell_type":"code","source":"# Dataset is too big so slice it accroding your computation resources\ndf_train = ds[\"train\"].select(range(10000))\ndf_validation = ds[\"train\"].select(range(10000 , 11000))\ndf_test = ds[\"test\"].select(range(500))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-13T18:55:34.236226Z","iopub.execute_input":"2025-07-13T18:55:34.236497Z","iopub.status.idle":"2025-07-13T18:55:34.252457Z","shell.execute_reply.started":"2025-07-13T18:55:34.236475Z","shell.execute_reply":"2025-07-13T18:55:34.251870Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"# Qwen3-0.6B is Causal Language Models so we have finetuned it Instruction-output format. (instruction‑tuning)\ndef add_instruction(example):\n    example[\"instruction\"] = \"Convert the English text into Marathi language.\"\n    return example\n\n\ndf_train = df_train.map(add_instruction, batched=False)\ndf_validation = df_validation.map(add_instruction, batched=False)\ndf_test = df_test.map(add_instruction, batched=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-13T18:55:34.253145Z","iopub.execute_input":"2025-07-13T18:55:34.253566Z","iopub.status.idle":"2025-07-13T18:55:34.602350Z","shell.execute_reply.started":"2025-07-13T18:55:34.253548Z","shell.execute_reply":"2025-07-13T18:55:34.601550Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/10000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bf67c841474442038f90cf081b33d81f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/1000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f4344491255549cd96210b391219d206"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/500 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ec5ed67637c44a20831956b0f7f01b81"}},"metadata":{}}],"execution_count":15},{"cell_type":"code","source":"ds[\"train\"].column_names","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-13T18:55:34.603141Z","iopub.execute_input":"2025-07-13T18:55:34.603438Z","iopub.status.idle":"2025-07-13T18:55:34.608343Z","shell.execute_reply.started":"2025-07-13T18:55:34.603412Z","shell.execute_reply":"2025-07-13T18:55:34.607775Z"}},"outputs":[{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"['english', 'marathi']"},"metadata":{}}],"execution_count":16},{"cell_type":"code","source":"df_train[0]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-13T18:55:34.608923Z","iopub.execute_input":"2025-07-13T18:55:34.609127Z","iopub.status.idle":"2025-07-13T18:55:34.630508Z","shell.execute_reply.started":"2025-07-13T18:55:34.609113Z","shell.execute_reply":"2025-07-13T18:55:34.629946Z"}},"outputs":[{"execution_count":17,"output_type":"execute_result","data":{"text/plain":"{'english': 'Next few months are really crucial for us.',\n 'marathi': 'पुढील तीन महिने आमच्यासाठी खूप महत्त्वपूर्ण आहेत.',\n 'instruction': 'Convert the English text into Marathi language.'}"},"metadata":{}}],"execution_count":17},{"cell_type":"code","source":"# Know about how tokenizer works\ninputs = tokenizer('Next few months are really crucial for us.','पुढील तीन महिने आमच्यासाठी खूप महत्त्वपूर्ण आहेत.', 'Convert the English text into Marathi language.' )\ninputs","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-13T18:55:34.631172Z","iopub.execute_input":"2025-07-13T18:55:34.631438Z","iopub.status.idle":"2025-07-13T18:55:34.647438Z","shell.execute_reply.started":"2025-07-13T18:55:34.631414Z","shell.execute_reply":"2025-07-13T18:55:34.646835Z"}},"outputs":[{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"{'input_ids': [5847, 2421, 3951, 525, 2167, 16587, 369, 601, 13, 86162, 72653, 149269, 43647, 91811, 14925, 97, 43647, 60096, 91217, 93948, 42311, 101, 34370, 14925, 228, 87244, 146113, 30484, 107, 31411, 116, 31411, 254, 43647, 14925, 244, 12619, 224, 86162, 91217, 93948, 79238, 30484, 97, 30484, 113, 86162, 12619, 224, 44179, 30484, 96, 14925, 228, 93948, 54784, 97, 13], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': [12012, 279, 6364, 1467, 1119, 2876, 66531, 4128, 13]}"},"metadata":{}}],"execution_count":18},{"cell_type":"code","source":"tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-13T18:55:34.648092Z","iopub.execute_input":"2025-07-13T18:55:34.648316Z","iopub.status.idle":"2025-07-13T18:55:34.661531Z","shell.execute_reply.started":"2025-07-13T18:55:34.648295Z","shell.execute_reply":"2025-07-13T18:55:34.660868Z"}},"outputs":[{"execution_count":19,"output_type":"execute_result","data":{"text/plain":"['Next',\n 'Ġfew',\n 'Ġmonths',\n 'Ġare',\n 'Ġreally',\n 'Ġcrucial',\n 'Ġfor',\n 'Ġus',\n '.',\n 'à¤ª',\n 'à¥ģ',\n 'à¤¢',\n 'à¥Ģ',\n 'à¤²',\n 'Ġà¤',\n '¤',\n 'à¥Ģ',\n 'à¤¨',\n 'Ġà¤®',\n 'à¤¹',\n 'à¤¿à¤',\n '¨',\n 'à¥ĩ',\n 'Ġà¤',\n 'Ĩ',\n 'à¤®',\n 'à¤ļ',\n 'à¥įà¤',\n '¯',\n 'à¤¾à¤',\n '¸',\n 'à¤¾à¤',\n 'ł',\n 'à¥Ģ',\n 'Ġà¤',\n 'ĸ',\n 'à¥',\n 'Ĥ',\n 'à¤ª',\n 'Ġà¤®',\n 'à¤¹',\n 'à¤¤',\n 'à¥įà¤',\n '¤',\n 'à¥įà¤',\n 'µ',\n 'à¤ª',\n 'à¥',\n 'Ĥ',\n 'à¤°',\n 'à¥įà¤',\n '£',\n 'Ġà¤',\n 'Ĩ',\n 'à¤¹',\n 'à¥ĩà¤',\n '¤',\n '.']"},"metadata":{}}],"execution_count":19},{"cell_type":"code","source":"instruction = \"Convert the English text into Marathi language.\"\nsrc_text     = \"Next few months are really crucial for us.\"\ntgt_text     = \"पुढील तीन महिने आमच्यासाठी खूप महत्त्वपूर्ण आहेत.\"\n\n# Prompt the model should *see*\nprompt = f\"{instruction} English: {src_text} Marathi:\"\nprint(prompt)\n\ninputs  = tokenizer(prompt, return_tensors=\"pt\") \nprint('Length of tokenize form of prompt:', inputs['input_ids'].size())\n\n\n# Every token sequence in a causal‐LM fine‑tuning setup needs a clear “stop” signal so the model knows when it’s done generating. That’s exactly what tokenizer.eos_token_id is for\n\nlabels  = tokenizer(tgt_text, add_special_tokens=False).input_ids + [tokenizer.eos_token_id] #Here labes are tokenize form our marathi output","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-13T18:55:34.662323Z","iopub.execute_input":"2025-07-13T18:55:34.662940Z","iopub.status.idle":"2025-07-13T18:55:34.677733Z","shell.execute_reply.started":"2025-07-13T18:55:34.662923Z","shell.execute_reply":"2025-07-13T18:55:34.677132Z"}},"outputs":[{"name":"stdout","text":"Convert the English text into Marathi language. English: Next few months are really crucial for us. Marathi:\nLength of tokenize form of prompt: torch.Size([1, 23])\n","output_type":"stream"}],"execution_count":20},{"cell_type":"code","source":"inputs","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-13T18:55:34.678415Z","iopub.execute_input":"2025-07-13T18:55:34.678805Z","iopub.status.idle":"2025-07-13T18:55:34.699396Z","shell.execute_reply.started":"2025-07-13T18:55:34.678787Z","shell.execute_reply":"2025-07-13T18:55:34.698621Z"}},"outputs":[{"execution_count":21,"output_type":"execute_result","data":{"text/plain":"{'input_ids': tensor([[12012,   279,  6364,  1467,  1119,  2876, 66531,  4128,    13,  6364,\n            25,  9295,  2421,  3951,   525,  2167, 16587,   369,   601,    13,\n          2876, 66531,    25]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}"},"metadata":{}}],"execution_count":21},{"cell_type":"code","source":"# The length of tokenize input prompt( instrution + context) is 23","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-13T18:55:34.700030Z","iopub.execute_input":"2025-07-13T18:55:34.700275Z","iopub.status.idle":"2025-07-13T18:55:34.707570Z","shell.execute_reply.started":"2025-07-13T18:55:34.700256Z","shell.execute_reply":"2025-07-13T18:55:34.706882Z"}},"outputs":[],"execution_count":22},{"cell_type":"code","source":"tokenizer.eos_token_id","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-13T18:55:34.708367Z","iopub.execute_input":"2025-07-13T18:55:34.708665Z","iopub.status.idle":"2025-07-13T18:55:34.722552Z","shell.execute_reply.started":"2025-07-13T18:55:34.708649Z","shell.execute_reply":"2025-07-13T18:55:34.722007Z"}},"outputs":[{"execution_count":23,"output_type":"execute_result","data":{"text/plain":"151645"},"metadata":{}}],"execution_count":23},{"cell_type":"code","source":"labels ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-13T18:55:34.723135Z","iopub.execute_input":"2025-07-13T18:55:34.723351Z","iopub.status.idle":"2025-07-13T18:55:34.736326Z","shell.execute_reply.started":"2025-07-13T18:55:34.723335Z","shell.execute_reply":"2025-07-13T18:55:34.735804Z"}},"outputs":[{"execution_count":24,"output_type":"execute_result","data":{"text/plain":"[86162,\n 72653,\n 149269,\n 43647,\n 91811,\n 14925,\n 97,\n 43647,\n 60096,\n 91217,\n 93948,\n 42311,\n 101,\n 34370,\n 14925,\n 228,\n 87244,\n 146113,\n 30484,\n 107,\n 31411,\n 116,\n 31411,\n 254,\n 43647,\n 14925,\n 244,\n 12619,\n 224,\n 86162,\n 91217,\n 93948,\n 79238,\n 30484,\n 97,\n 30484,\n 113,\n 86162,\n 12619,\n 224,\n 44179,\n 30484,\n 96,\n 14925,\n 228,\n 93948,\n 54784,\n 97,\n 13,\n 151645]"},"metadata":{}}],"execution_count":24},{"cell_type":"code","source":"len(labels) # 49 orignal + 1 tokenizer.eos_token_id = 50","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-13T18:55:34.737076Z","iopub.execute_input":"2025-07-13T18:55:34.737342Z","iopub.status.idle":"2025-07-13T18:55:34.750274Z","shell.execute_reply.started":"2025-07-13T18:55:34.737321Z","shell.execute_reply":"2025-07-13T18:55:34.749657Z"}},"outputs":[{"execution_count":25,"output_type":"execute_result","data":{"text/plain":"50"},"metadata":{}}],"execution_count":25},{"cell_type":"code","source":"# For a causal model, you feed it the entire sequence (prompt and target), because it learns by shifting that sequence one token at a time.\nmodel_inputs = tokenizer(prompt + \" \" + tgt_text + tokenizer.eos_token,  return_tensors=\"pt\")\nprint('Length of tokenize form of model_inputs:', model_inputs['input_ids'].size())  \n\n# You need to know how many tokens correspond just to the instruction+source (the “prompt”) so that you can later mask them out in the labels.\nprompt_len = len(tokenizer(prompt, add_special_tokens=False).input_ids)\nprint(\"Tokenized Prompt length:\" , prompt_len)\n\n#  Clone so you don’t overwrite the original inputs\nlabels = model_inputs[\"input_ids\"].clone()\n\n\n# For every position in the prompt, set label to -100  \n# In Hugging Face Trainer, any label token = –100 is ignored by the loss function.\nlabels[:, :prompt_len] = -100        # works for a batch of size 1 or N\n# print('Length that going to be masked' , len(labels))\nmodel_inputs[\"labels\"] = labels","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-13T18:55:34.750962Z","iopub.execute_input":"2025-07-13T18:55:34.751267Z","iopub.status.idle":"2025-07-13T18:55:34.765419Z","shell.execute_reply.started":"2025-07-13T18:55:34.751244Z","shell.execute_reply":"2025-07-13T18:55:34.764620Z"}},"outputs":[{"name":"stdout","text":"Length of tokenize form of model_inputs: torch.Size([1, 73])\nTokenized Prompt length: 23\n","output_type":"stream"}],"execution_count":26},{"cell_type":"code","source":"labels","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-13T18:55:34.766081Z","iopub.execute_input":"2025-07-13T18:55:34.766293Z","iopub.status.idle":"2025-07-13T18:55:34.779691Z","shell.execute_reply.started":"2025-07-13T18:55:34.766278Z","shell.execute_reply":"2025-07-13T18:55:34.778960Z"}},"outputs":[{"execution_count":27,"output_type":"execute_result","data":{"text/plain":"tensor([[  -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n           -100,   -100,   -100,   -100,   -100,  83636,  72653, 149269,  43647,\n          91811,  14925,     97,  43647,  60096,  91217,  93948,  42311,    101,\n          34370,  14925,    228,  87244, 146113,  30484,    107,  31411,    116,\n          31411,    254,  43647,  14925,    244,  12619,    224,  86162,  91217,\n          93948,  79238,  30484,     97,  30484,    113,  86162,  12619,    224,\n          44179,  30484,     96,  14925,    228,  93948,  54784,     97,     13,\n         151645]])"},"metadata":{}}],"execution_count":27},{"cell_type":"code","source":"labels.size()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-13T18:55:34.783582Z","iopub.execute_input":"2025-07-13T18:55:34.783767Z","iopub.status.idle":"2025-07-13T18:55:34.791678Z","shell.execute_reply.started":"2025-07-13T18:55:34.783747Z","shell.execute_reply":"2025-07-13T18:55:34.790932Z"}},"outputs":[{"execution_count":28,"output_type":"execute_result","data":{"text/plain":"torch.Size([1, 73])"},"metadata":{}}],"execution_count":28},{"cell_type":"code","source":"x = labels[0] \nnum_ignored = (x == -100).sum().item()\nnum_not_ignored = (x != -100).sum().item()\nprint(num_ignored)\nprint(num_not_ignored)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-13T18:55:34.792388Z","iopub.execute_input":"2025-07-13T18:55:34.792590Z","iopub.status.idle":"2025-07-13T18:55:34.808379Z","shell.execute_reply.started":"2025-07-13T18:55:34.792574Z","shell.execute_reply":"2025-07-13T18:55:34.807754Z"}},"outputs":[{"name":"stdout","text":"23\n50\n","output_type":"stream"}],"execution_count":29},{"cell_type":"code","source":"model_inputs","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-13T18:55:34.809069Z","iopub.execute_input":"2025-07-13T18:55:34.809325Z","iopub.status.idle":"2025-07-13T18:55:34.822080Z","shell.execute_reply.started":"2025-07-13T18:55:34.809301Z","shell.execute_reply":"2025-07-13T18:55:34.821497Z"}},"outputs":[{"execution_count":30,"output_type":"execute_result","data":{"text/plain":"{'input_ids': tensor([[ 12012,    279,   6364,   1467,   1119,   2876,  66531,   4128,     13,\n           6364,     25,   9295,   2421,   3951,    525,   2167,  16587,    369,\n            601,     13,   2876,  66531,     25,  83636,  72653, 149269,  43647,\n          91811,  14925,     97,  43647,  60096,  91217,  93948,  42311,    101,\n          34370,  14925,    228,  87244, 146113,  30484,    107,  31411,    116,\n          31411,    254,  43647,  14925,    244,  12619,    224,  86162,  91217,\n          93948,  79238,  30484,     97,  30484,    113,  86162,  12619,    224,\n          44179,  30484,     96,  14925,    228,  93948,  54784,     97,     13,\n         151645]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1]]), 'labels': tensor([[  -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n           -100,   -100,   -100,   -100,   -100,  83636,  72653, 149269,  43647,\n          91811,  14925,     97,  43647,  60096,  91217,  93948,  42311,    101,\n          34370,  14925,    228,  87244, 146113,  30484,    107,  31411,    116,\n          31411,    254,  43647,  14925,    244,  12619,    224,  86162,  91217,\n          93948,  79238,  30484,     97,  30484,    113,  86162,  12619,    224,\n          44179,  30484,     96,  14925,    228,  93948,  54784,     97,     13,\n         151645]])}"},"metadata":{}}],"execution_count":30},{"cell_type":"code","source":"model_inputs['input_ids'].size()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-13T18:55:34.822686Z","iopub.execute_input":"2025-07-13T18:55:34.822898Z","iopub.status.idle":"2025-07-13T18:55:34.833558Z","shell.execute_reply.started":"2025-07-13T18:55:34.822877Z","shell.execute_reply":"2025-07-13T18:55:34.832857Z"}},"outputs":[{"execution_count":31,"output_type":"execute_result","data":{"text/plain":"torch.Size([1, 73])"},"metadata":{}}],"execution_count":31},{"cell_type":"code","source":"# this function convert string into vectors\ndef tokenize_function(examples):\n\n    batch_input_ids      = []\n    batch_attention_mask = []\n    batch_labels         = []\n\n    # Iterate over the batch\n    for instruction, english, marathi in zip(\n            examples[\"instruction\"], examples[\"english\"], examples[\"marathi\"]):\n\n        # 1. Build the prompt shown to the model (not trained on)\n        prompt_text = f\"{instruction} English: {english} Marathi:\"\n        prompt_ids  = tokenizer(prompt_text, add_special_tokens=False).input_ids\n\n        # 2. Full text = prompt + target + EOS\n        full_text = f\"{prompt_text} {marathi}{tokenizer.eos_token}\"\n        tok = tokenizer(full_text, add_special_tokens=False)\n\n        # 3. Create labels: mask the prompt tokens with –100\n        label_ids = [-100] * len(prompt_ids) + tok[\"input_ids\"][len(prompt_ids):]\n\n        # 4. Collect results for this example\n        batch_input_ids.append(tok[\"input_ids\"])\n        batch_attention_mask.append(tok[\"attention_mask\"])\n        batch_labels.append(label_ids)\n\n    return {\n        \"input_ids\":      batch_input_ids,\n        \"attention_mask\": batch_attention_mask,\n        \"labels\":         batch_labels,\n    }","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-13T18:55:34.834290Z","iopub.execute_input":"2025-07-13T18:55:34.834535Z","iopub.status.idle":"2025-07-13T18:55:34.846754Z","shell.execute_reply.started":"2025-07-13T18:55:34.834514Z","shell.execute_reply":"2025-07-13T18:55:34.846140Z"}},"outputs":[],"execution_count":32},{"cell_type":"code","source":"train_tokenised_ds = df_train.map(tokenize_function, batched=True , num_proc=4 , remove_columns=df_train.column_names)\ntest_tokenised_ds = df_test.map(tokenize_function, batched=True , num_proc=4 , remove_columns=df_test.column_names)\nvalidation_tokenised_ds = df_validation.map(tokenize_function, batched=True , num_proc=4, remove_columns=df_validation.column_names )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-13T18:55:34.847440Z","iopub.execute_input":"2025-07-13T18:55:34.847648Z","iopub.status.idle":"2025-07-13T18:55:39.228040Z","shell.execute_reply.started":"2025-07-13T18:55:34.847623Z","shell.execute_reply":"2025-07-13T18:55:39.227227Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map (num_proc=4):   0%|          | 0/10000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6a6ea2ae8b8c4858b36d4bce9d2c912a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map (num_proc=4):   0%|          | 0/500 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"47c9dfc441714f4abc28daf40b064e38"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map (num_proc=4):   0%|          | 0/1000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"35a76ccf4ca14d6f854d8c671cf42119"}},"metadata":{}}],"execution_count":33},{"cell_type":"code","source":"test_tokenised_ds","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-13T18:55:39.229261Z","iopub.execute_input":"2025-07-13T18:55:39.229522Z","iopub.status.idle":"2025-07-13T18:55:39.235007Z","shell.execute_reply.started":"2025-07-13T18:55:39.229497Z","shell.execute_reply":"2025-07-13T18:55:39.234234Z"}},"outputs":[{"execution_count":34,"output_type":"execute_result","data":{"text/plain":"Dataset({\n    features: ['input_ids', 'attention_mask', 'labels'],\n    num_rows: 500\n})"},"metadata":{}}],"execution_count":34},{"cell_type":"code","source":"len(test_tokenised_ds[0]['input_ids'])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-13T18:55:39.235733Z","iopub.execute_input":"2025-07-13T18:55:39.235952Z","iopub.status.idle":"2025-07-13T18:55:39.277891Z","shell.execute_reply.started":"2025-07-13T18:55:39.235928Z","shell.execute_reply":"2025-07-13T18:55:39.277030Z"}},"outputs":[{"execution_count":35,"output_type":"execute_result","data":{"text/plain":"90"},"metadata":{}}],"execution_count":35},{"cell_type":"code","source":"if tokenizer.pad_token is None:\n    tokenizer.add_special_tokens({\"pad_token\": tokenizer.eos_token})\n    model.resize_token_embeddings(len(tokenizer))          # update embeddings\nmodel.config.pad_token_id = tokenizer.pad_token_id","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-13T18:55:39.278813Z","iopub.execute_input":"2025-07-13T18:55:39.279237Z","iopub.status.idle":"2025-07-13T18:55:39.300056Z","shell.execute_reply.started":"2025-07-13T18:55:39.279173Z","shell.execute_reply":"2025-07-13T18:55:39.299434Z"}},"outputs":[],"execution_count":36},{"cell_type":"code","source":"for split in (train_tokenised_ds, test_tokenised_ds, validation_tokenised_ds):\n    split.set_format(type=\"torch\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-13T18:55:39.300778Z","iopub.execute_input":"2025-07-13T18:55:39.301259Z","iopub.status.idle":"2025-07-13T18:55:39.319838Z","shell.execute_reply.started":"2025-07-13T18:55:39.301239Z","shell.execute_reply":"2025-07-13T18:55:39.319319Z"}},"outputs":[],"execution_count":37},{"cell_type":"code","source":"from transformers import DataCollatorForSeq2Seq\n\nclass EfficientDataCollatorForSeq2Seq(DataCollatorForSeq2Seq):\n    def __call__(self, features, return_tensors=None):\n        # Let the parent class handle padding, returning NumPy arrays.\n        batch = super().__call__(features, return_tensors=\"np\")\n        # Manually convert each padded NumPy array to a PyTorch tensor.\n        tensor_batch = {}\n        for key, value in batch.items():\n            tensor_batch[key] = torch.from_numpy(value)\n        return tensor_batch\n\ndata_collator = EfficientDataCollatorForSeq2Seq(\n    tokenizer=tokenizer,\n    model=model,\n    padding=True,\n    label_pad_token_id=-100,\n    pad_to_multiple_of=8,\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-13T18:55:39.320978Z","iopub.execute_input":"2025-07-13T18:55:39.321219Z","iopub.status.idle":"2025-07-13T18:55:39.334576Z","shell.execute_reply.started":"2025-07-13T18:55:39.321196Z","shell.execute_reply":"2025-07-13T18:55:39.334023Z"}},"outputs":[],"execution_count":38},{"cell_type":"code","source":"train_tokenised_ds","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-13T18:55:39.335236Z","iopub.execute_input":"2025-07-13T18:55:39.335434Z","iopub.status.idle":"2025-07-13T18:55:39.350728Z","shell.execute_reply.started":"2025-07-13T18:55:39.335413Z","shell.execute_reply":"2025-07-13T18:55:39.350243Z"}},"outputs":[{"execution_count":39,"output_type":"execute_result","data":{"text/plain":"Dataset({\n    features: ['input_ids', 'attention_mask', 'labels'],\n    num_rows: 10000\n})"},"metadata":{}}],"execution_count":39},{"cell_type":"code","source":"# Create data loaders\ntrain_dataloader = DataLoader(\n        train_tokenised_ds,\n        batch_size=2,\n        shuffle=True,\n        collate_fn=data_collator,\n        pin_memory=False\n)\n    \neval_dataloader = DataLoader(\n    validation_tokenised_ds,\n    batch_size=2,\n    shuffle=False,\n    collate_fn=data_collator,\n    pin_memory=False\n)\n\ntest_dataloader = DataLoader(\n    test_tokenised_ds,\n    batch_size=2,\n    shuffle=False,\n    collate_fn=data_collator,\n    pin_memory=False\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-13T18:55:39.351449Z","iopub.execute_input":"2025-07-13T18:55:39.351628Z","iopub.status.idle":"2025-07-13T18:55:39.365119Z","shell.execute_reply.started":"2025-07-13T18:55:39.351615Z","shell.execute_reply":"2025-07-13T18:55:39.364464Z"}},"outputs":[],"execution_count":40},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\">>> Using device: {device}\")\n\nmodel.to(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-13T18:55:39.365918Z","iopub.execute_input":"2025-07-13T18:55:39.366103Z","iopub.status.idle":"2025-07-13T18:55:40.165008Z","shell.execute_reply.started":"2025-07-13T18:55:39.366082Z","shell.execute_reply":"2025-07-13T18:55:40.164311Z"}},"outputs":[{"name":"stdout","text":">>> Using device: cuda\n","output_type":"stream"},{"execution_count":41,"output_type":"execute_result","data":{"text/plain":"Qwen3ForCausalLM(\n  (model): Qwen3Model(\n    (embed_tokens): Embedding(151936, 1024)\n    (layers): ModuleList(\n      (0-27): 28 x Qwen3DecoderLayer(\n        (self_attn): Qwen3Attention(\n          (q_proj): Linear(in_features=1024, out_features=2048, bias=False)\n          (k_proj): Linear(in_features=1024, out_features=1024, bias=False)\n          (v_proj): Linear(in_features=1024, out_features=1024, bias=False)\n          (o_proj): Linear(in_features=2048, out_features=1024, bias=False)\n          (q_norm): Qwen3RMSNorm((128,), eps=1e-06)\n          (k_norm): Qwen3RMSNorm((128,), eps=1e-06)\n        )\n        (mlp): Qwen3MLP(\n          (gate_proj): Linear(in_features=1024, out_features=3072, bias=False)\n          (up_proj): Linear(in_features=1024, out_features=3072, bias=False)\n          (down_proj): Linear(in_features=3072, out_features=1024, bias=False)\n          (act_fn): SiLU()\n        )\n        (input_layernorm): Qwen3RMSNorm((1024,), eps=1e-06)\n        (post_attention_layernorm): Qwen3RMSNorm((1024,), eps=1e-06)\n      )\n    )\n    (norm): Qwen3RMSNorm((1024,), eps=1e-06)\n    (rotary_emb): Qwen3RotaryEmbedding()\n  )\n  (lm_head): Linear(in_features=1024, out_features=151936, bias=False)\n)"},"metadata":{}}],"execution_count":41},{"cell_type":"code","source":"#training paramter and Optimizer , scheduler\nnum_epochs = 1\nper_device_train_batch_size = 6\nper_device_eval_batch_size = 4\ngradient_accumulation_steps = 2\nlearning_rate = 5e-5\nweight_decay = 0.01\nwarmup_steps = 200\nlogging_steps = 500\neval_steps = 500\nsave_total_limit = 2\noutput_dir = \"qwen_marathi\"\nnum_update_steps_per_epoch = math.ceil(len(train_dataloader) / gradient_accumulation_steps)\nmax_train_steps = num_epochs * num_update_steps_per_epoch\noptimizer_Adafactor = Adafactor(\n    model.parameters(),\n    lr=learning_rate,  # Adafactor can compute its own LR, which is often recommended\n    scale_parameter=False,\n    relative_step=False, # Set to True to use internal learning rate\n    warmup_init=False,\n    weight_decay=weight_decay,\n)\noptimizer_AdamW = torch.optim.AdamW(model.parameters(),lr=learning_rate,weight_decay=weight_decay,betas=(0.9, 0.999),eps=1e-8)\nlr_scheduler = get_scheduler(\n    \"linear\",\n    optimizer=optimizer_Adafactor,\n    num_warmup_steps=0,\n    num_training_steps=max_train_steps,\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-13T18:55:40.165856Z","iopub.execute_input":"2025-07-13T18:55:40.166212Z","iopub.status.idle":"2025-07-13T18:55:40.174901Z","shell.execute_reply.started":"2025-07-13T18:55:40.166171Z","shell.execute_reply":"2025-07-13T18:55:40.174173Z"}},"outputs":[],"execution_count":42},{"cell_type":"code","source":"\n# --- 5. Print Training Setup ---\ntotal_batch_size = per_device_train_batch_size * gradient_accumulation_steps\nprint(\"***** Running training *****\")\nprint(f\"  Num examples              = {len(train_tokenised_ds)}\")\nprint(f\"  Num Epochs                = {num_epochs}\")\nprint(f\"  Batch size per device     = {per_device_train_batch_size}\")\nprint(f\"  Total train batch size    = {total_batch_size}\")\nprint(f\"  Gradient Accumulation     = {gradient_accumulation_steps}\")\nprint(f\"  Total optimization steps  = {max_train_steps}\")\nprint(f\"  Number of devices         = 1\")\n\ntorch.cuda.empty_cache()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-13T18:55:40.175744Z","iopub.execute_input":"2025-07-13T18:55:40.175955Z","iopub.status.idle":"2025-07-13T18:55:40.194278Z","shell.execute_reply.started":"2025-07-13T18:55:40.175940Z","shell.execute_reply":"2025-07-13T18:55:40.193570Z"}},"outputs":[{"name":"stdout","text":"***** Running training *****\n  Num examples              = 10000\n  Num Epochs                = 1\n  Batch size per device     = 6\n  Total train batch size    = 12\n  Gradient Accumulation     = 2\n  Total optimization steps  = 2500\n  Number of devices         = 1\n","output_type":"stream"}],"execution_count":43},{"cell_type":"code","source":"# 8. Initialize WandB (main process only)\n\nwandb.init(\n    project=\"qwen-marathi-finetuning\",\n    name=\"qwen-marathi-run2\",\n    config={\n            \"learning_rate\": learning_rate,\n            \"num_epochs\": num_epochs,\n            \"batch_size_per_device\": per_device_train_batch_size,\n            \"gradient_accumulation_steps\": gradient_accumulation_steps,\n            \"weight_decay\": weight_decay,\n            \"warmup_steps\": warmup_steps,\n            \"optimizer\": optimizer_Adafactor ,\n            \"lr_scheduler\": lr_scheduler,\n            \"num_gpus\": torch.cuda.device_count(),\n    }\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-13T18:55:40.194993Z","iopub.execute_input":"2025-07-13T18:55:40.195203Z","iopub.status.idle":"2025-07-13T18:55:47.262978Z","shell.execute_reply.started":"2025-07-13T18:55:40.195167Z","shell.execute_reply":"2025-07-13T18:55:47.262237Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.20.1"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250713_185540-c6kozhp3</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/22cs3033-rgipt-jais/qwen-marathi-finetuning/runs/c6kozhp3' target=\"_blank\">qwen-marathi-run2</a></strong> to <a href='https://wandb.ai/22cs3033-rgipt-jais/qwen-marathi-finetuning' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/22cs3033-rgipt-jais/qwen-marathi-finetuning' target=\"_blank\">https://wandb.ai/22cs3033-rgipt-jais/qwen-marathi-finetuning</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/22cs3033-rgipt-jais/qwen-marathi-finetuning/runs/c6kozhp3' target=\"_blank\">https://wandb.ai/22cs3033-rgipt-jais/qwen-marathi-finetuning/runs/c6kozhp3</a>"},"metadata":{}},{"execution_count":44,"output_type":"execute_result","data":{"text/html":"<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/22cs3033-rgipt-jais/qwen-marathi-finetuning/runs/c6kozhp3?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>","text/plain":"<wandb.sdk.wandb_run.Run at 0x7dec11c60290>"},"metadata":{}}],"execution_count":44},{"cell_type":"code","source":"def save_checkpoint(model, tokenizer, output_dir, checkpoint_name, saved_checkpoints, save_total_limit):\n    \"\"\"Saves model and tokenizer, and manages the total number of checkpoints.\"\"\"\n    checkpoint_dir = os.path.join(output_dir, checkpoint_name)\n    os.makedirs(checkpoint_dir, exist_ok=True)\n\n    # Save model and tokenizer\n    model.save_pretrained(checkpoint_dir)\n    tokenizer.save_pretrained(checkpoint_dir)\n    print(f\"Checkpoint saved to {checkpoint_dir}\")\n    saved_checkpoints.append(checkpoint_dir)\n\n    # Remove old checkpoints if the limit is exceeded\n    if len(saved_checkpoints) > save_total_limit:\n        oldest_checkpoint = saved_checkpoints.pop(0)\n        # Avoid deleting special checkpoints like 'best' or 'final'\n        if \"best\" not in oldest_checkpoint and \"final\" not in oldest_checkpoint:\n            try:\n                shutil.rmtree(oldest_checkpoint)\n                print(f\"Removed old checkpoint: {oldest_checkpoint}\")\n            except OSError as e:\n                print(f\"Error removing old checkpoint {oldest_checkpoint}: {e}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-13T18:55:47.263884Z","iopub.execute_input":"2025-07-13T18:55:47.264160Z","iopub.status.idle":"2025-07-13T18:55:47.270552Z","shell.execute_reply.started":"2025-07-13T18:55:47.264135Z","shell.execute_reply":"2025-07-13T18:55:47.269862Z"}},"outputs":[],"execution_count":45},{"cell_type":"code","source":"def evaluate_model(model, eval_dataloader, device):\n    \"\"\"Evaluates the model and returns the average loss.\"\"\"\n    model.eval()\n    total_eval_loss = 0.0\n    eval_progress = tqdm(eval_dataloader, desc=\"Evaluating\", leave=False)\n\n    with torch.no_grad():\n        for batch in eval_progress:\n            # Move batch to the same device as the model\n            batch = {k: v.to(device) for k, v in batch.items()}\n            outputs = model(**batch)\n            loss = outputs.loss\n            total_eval_loss += loss.item()\n\n    avg_eval_loss = total_eval_loss / len(eval_dataloader)\n    return avg_eval_loss","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-13T18:55:47.271307Z","iopub.execute_input":"2025-07-13T18:55:47.271499Z","iopub.status.idle":"2025-07-13T18:55:47.286686Z","shell.execute_reply.started":"2025-07-13T18:55:47.271485Z","shell.execute_reply":"2025-07-13T18:55:47.286062Z"}},"outputs":[],"execution_count":46},{"cell_type":"code","source":"\nglobal_step = 0\ntotal_loss = 0.0\nbest_eval_loss = float('inf')\nsaved_checkpoints = []\n\nprogress_bar = tqdm(range(max_train_steps) , desc=\"Training\")\nmodel.zero_grad()\n\nfor epoch in range(num_epochs):\n    model.train()\n    epoch_loss = 0.0\n\n    for step, batch in enumerate(train_dataloader):\n        batch = {k: v.to(device) for k, v in batch.items()}\n        \n        outputs = model(**batch)\n        loss = outputs.loss\n\n        loss = loss / gradient_accumulation_steps\n        loss.backward()\n                \n        # optimizer.step()\n        # lr_scheduler.step()\n        # optimizer.zero_grad()\n\n        epoch_loss += loss.detach().float().item()\n        total_loss += loss.detach().float().item()\n\n        if (step + 1) % gradient_accumulation_steps == 0:\n            optimizer_Adafactor.step()\n            lr_scheduler.step()\n            optimizer_Adafactor.zero_grad()\n\n            global_step += 1\n            progress_bar.update(1)\n            progress_bar.set_postfix({\n                'loss': f'{loss.item() * gradient_accumulation_steps:.4f}', # Show actual loss\n                'lr': f'{lr_scheduler.get_last_lr()[0]:.2e}',\n                'step': global_step\n            })\n            \n            # logging\n            if global_step % logging_steps == 0:\n                avg_loss = total_loss / global_step\n                current_lr = lr_scheduler.get_last_lr()[0]\n                \n                print(f\"[Step {global_step}] train_loss={avg_loss:.4f}, lr={current_lr:.2e}\")\n                wandb.log({\n                        \"train/loss\": avg_loss,\n                        \"train/learning_rate\": current_lr,\n                        \"train/epoch\": epoch + (step + 1) / len(train_dataloader),\n                        \"train/global_step\": global_step\n                })\n\n            # evaluation\n            if global_step % eval_steps == 0:\n                eval_loss = evaluate_model(model, eval_dataloader, device )\n                \n                print(f\"[Step {global_step}] eval_loss={eval_loss:.4f}\")\n                wandb.log({\"eval/loss\": eval_loss, \"eval/global_step\": global_step})\n                # save best\n                if eval_loss < best_eval_loss:\n                    best_eval_loss = eval_loss\n                    save_checkpoint(\n                            model, tokenizer, output_dir,\n                            f\"best-checkpoint-step\",\n                            saved_checkpoints, save_total_limit\n                    )\n            model.train()\n\n    \n    # end of epoch\n    avg_epoch_loss = epoch_loss / len(train_dataloader)\n    print(f\"Epoch {epoch+1}/{num_epochs} • avg_train_loss={avg_epoch_loss:.4f}\")\n    wandb.log({\"train/epoch_loss\": avg_epoch_loss, \"train/epoch\": epoch+1})\n\n    # checkpoint at epoch end\n    save_checkpoint(\n            model, tokenizer, output_dir,\n            f\"epoch-{epoch+1}\", saved_checkpoints, save_total_limit\n    )\n\n# Final evaluation & cleanup\nfinal_eval_loss = evaluate_model(model, eval_dataloader , device)\n\nprint(f\"Final evaluation • eval_loss={final_eval_loss:.4f}\")\nwandb.log({\"eval/final_loss\": final_eval_loss})\n\nsave_checkpoint(\n        model, tokenizer,  output_dir,\n        \"final-checkpoint\",\n        saved_checkpoints, save_total_limit\n)\n\nprogress_bar.close()\nwandb.finish()\nprint(\"Training completed!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-13T18:55:47.287355Z","iopub.execute_input":"2025-07-13T18:55:47.287748Z","iopub.status.idle":"2025-07-13T19:22:40.322725Z","shell.execute_reply.started":"2025-07-13T18:55:47.287731Z","shell.execute_reply":"2025-07-13T19:22:40.322038Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/2500 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3b6535ea3d1c49e5b627167ebf11e203"}},"metadata":{}},{"name":"stdout","text":"[Step 500] train_loss=1.6274, lr=4.00e-05\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Evaluating:   0%|          | 0/500 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"[Step 500] eval_loss=1.4148\nCheckpoint saved to qwen_marathi/best-checkpoint-step\n[Step 1000] train_loss=1.4732, lr=3.00e-05\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Evaluating:   0%|          | 0/500 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"[Step 1000] eval_loss=1.2594\nCheckpoint saved to qwen_marathi/best-checkpoint-step\n[Step 1500] train_loss=1.3705, lr=2.00e-05\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Evaluating:   0%|          | 0/500 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"[Step 1500] eval_loss=1.1320\nCheckpoint saved to qwen_marathi/best-checkpoint-step\n[Step 2000] train_loss=1.2991, lr=1.00e-05\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Evaluating:   0%|          | 0/500 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"[Step 2000] eval_loss=1.0440\nCheckpoint saved to qwen_marathi/best-checkpoint-step\n[Step 2500] train_loss=1.2409, lr=0.00e+00\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Evaluating:   0%|          | 0/500 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"[Step 2500] eval_loss=0.9989\nCheckpoint saved to qwen_marathi/best-checkpoint-step\nEpoch 1/1 • avg_train_loss=0.6204\nCheckpoint saved to qwen_marathi/epoch-1\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Evaluating:   0%|          | 0/500 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Final evaluation • eval_loss=0.9989\nCheckpoint saved to qwen_marathi/final-checkpoint\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/final_loss</td><td>▁</td></tr><tr><td>eval/global_step</td><td>▁▃▅▆█</td></tr><tr><td>eval/loss</td><td>█▅▃▂▁</td></tr><tr><td>train/epoch</td><td>▁▃▅▆██</td></tr><tr><td>train/epoch_loss</td><td>▁</td></tr><tr><td>train/global_step</td><td>▁▃▅▆█</td></tr><tr><td>train/learning_rate</td><td>█▆▅▃▁</td></tr><tr><td>train/loss</td><td>█▅▃▂▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/final_loss</td><td>0.99891</td></tr><tr><td>eval/global_step</td><td>2500</td></tr><tr><td>eval/loss</td><td>0.99891</td></tr><tr><td>train/epoch</td><td>1</td></tr><tr><td>train/epoch_loss</td><td>0.62044</td></tr><tr><td>train/global_step</td><td>2500</td></tr><tr><td>train/learning_rate</td><td>0</td></tr><tr><td>train/loss</td><td>1.24088</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">qwen-marathi-run2</strong> at: <a href='https://wandb.ai/22cs3033-rgipt-jais/qwen-marathi-finetuning/runs/c6kozhp3' target=\"_blank\">https://wandb.ai/22cs3033-rgipt-jais/qwen-marathi-finetuning/runs/c6kozhp3</a><br> View project at: <a href='https://wandb.ai/22cs3033-rgipt-jais/qwen-marathi-finetuning' target=\"_blank\">https://wandb.ai/22cs3033-rgipt-jais/qwen-marathi-finetuning</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20250713_185540-c6kozhp3/logs</code>"},"metadata":{}},{"name":"stdout","text":"Training completed!\n","output_type":"stream"}],"execution_count":47},{"cell_type":"code","source":"def translate_to_marathi(english_sentence: str) -> str:\n    instruction = \"Convert the English text into Marathi language.\"\n    prompt = f\"{instruction} English: {english_sentence} Marathi:\"\n    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n\n    # Generate with a reasonable max length\n    outputs = model.generate(\n        **inputs,\n        max_new_tokens=512,\n        do_sample=False,           # greedy\n        eos_token_id=tokenizer.eos_token_id,\n        pad_token_id=tokenizer.pad_token_id,\n    )\n    # Strip off prompt tokens and decode rest\n    generated = outputs[0][inputs.input_ids.size(1):]\n    return tokenizer.decode(generated, skip_special_tokens=True)\n\n# Example\nprint(translate_to_marathi(\"The objective of this article and the following two articles is to deepen our appreciation for Gods qualities that we may think of less often than his principal attributes.\"))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-13T19:22:40.323627Z","iopub.execute_input":"2025-07-13T19:22:40.323856Z","iopub.status.idle":"2025-07-13T19:22:48.205485Z","shell.execute_reply.started":"2025-07-13T19:22:40.323830Z","shell.execute_reply":"2025-07-13T19:22:48.204650Z"}},"outputs":[{"name":"stderr","text":"The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n","output_type":"stream"},{"name":"stdout","text":" या लेखाच्या विषयावरून आपण देवाच्या विशेष वैशिष्ट्यांचा विचार करू शकतो आणि त्याच्या विशेष वैशिष्ट्यांचा विचार करू शकतो आणि आपण त्याच्या विशेष वैशिष्ट्यांचा विचार करू शकतो आणि त्याच्या विशेष वैशिष्ट्यांचा विचार करू शकतो.\n","output_type":"stream"}],"execution_count":48},{"cell_type":"code","source":"import torch\nimport evaluate\nfrom tqdm.auto import tqdm\n\ndef evaluate_model_bleu(model, tokenizer, df_test, batch_size=8):\n    print(\"Evaluating model and calculating SacreBLEU score...\")\n    metric = evaluate.load(\"sacrebleu\")\n    model.eval()  # Set model to inference mode\n\n    # This is a standard practice for decoder-only models.\n    if tokenizer.pad_token is None:\n        tokenizer.pad_token = tokenizer.eos_token\n        model.config.pad_token_id = model.config.eos_token_id\n\n    predictions, refs = [], []\n\n    for i in tqdm(range(0, len(df_test), batch_size), desc=\"Generating Predictions\"):\n        batch = df_test[i : i + batch_size]\n        src_sentences = batch[\"english\"]\n        ref_sentences = batch[\"marathi\"]\n\n        prompts = [\n            f\"Convert the English text into Marathi language. English: {src} Marathi:\"\n            for src in src_sentences\n        ]\n\n        # Tokenize inputs\n        inputs = tokenizer(\n            prompts,\n            return_tensors=\"pt\",\n            padding=True,\n            truncation=True,\n            max_length=512,\n            padding_side='left',  # Critical for decoder-only models\n        ).to(model.device)\n\n        with torch.no_grad():\n            generated = model.generate(\n                **inputs,\n                max_new_tokens=512,\n                do_sample=False,  # Use greedy decoding\n                eos_token_id=tokenizer.eos_token_id,\n            )\n\n        # Efficiently slice and decode the entire batch at once\n        prompt_token_length = inputs.input_ids.shape[1]\n        generated_only_ids = generated[:, prompt_token_length:]  # Corrected line\n\n        # Use batch_decode for faster processing\n        decoded_preds = tokenizer.batch_decode(generated_only_ids, skip_special_tokens=True)\n\n        # Strip any extra whitespace from each prediction\n        predictions.extend([pred.strip() for pred in decoded_preds])\n        refs.extend([[t] for t in ref_sentences])\n\n    # Compute and return the final BLEU score\n    bleu_result = metric.compute(predictions=predictions, references=refs)\n    return bleu_result\n\n# Compile model for faster inference\nprint(\"Compiling model for faster inference...\")\nmodel = torch.compile(model)\n\n# --- How to use the function ---\n# Assuming your model, tokenizer, and df_test are loaded\nbleu_score = evaluate_model_bleu(model, tokenizer, df_test)\nprint(f\"SacreBLEU on test set: {bleu_score['score']:.2f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-13T19:22:48.206536Z","iopub.execute_input":"2025-07-13T19:22:48.207407Z","iopub.status.idle":"2025-07-13T19:36:04.219019Z","shell.execute_reply.started":"2025-07-13T19:22:48.207380Z","shell.execute_reply":"2025-07-13T19:36:04.218424Z"}},"outputs":[{"name":"stdout","text":"Compiling model for faster inference...\nEvaluating model and calculating SacreBLEU score...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading builder script: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"50921d2ddca14b7c82ce5e8ddec0217d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating Predictions:   0%|          | 0/63 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4d0b9f99bddc44588292e733d4821ad8"}},"metadata":{}},{"name":"stderr","text":"The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n","output_type":"stream"},{"name":"stdout","text":"SacreBLEU on test set: 2.44\n","output_type":"stream"}],"execution_count":49},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# resume training function\n\ndef resume_training(\n        resume_from_checkpoint: str,\n        num_additional_epochs: int,\n        resume_from_epoch: int,\n        output_dir: str = \"qwen_marathi\",\n        per_device_train_batch_size: int = 2,\n        gradient_accumulation_steps: int = 4,\n        learning_rate: float = 5e-5,\n        weight_decay: float = 0.01,\n        warmup_steps: int = 200,\n        logging_steps: int = 500,\n        eval_steps: int = 500,\n        save_total_limit: int = 2,\n        use_wandb: bool = True\n    ):\n    \"\"\"\n    Resume fine-tuning from a previous checkpoint.\n    Assumes the original datasets, dataloaders, and helper functions\n    are available in the global scope of the script.\n    \"\"\"\n\n    if not os.path.isdir(resume_from_checkpoint):\n        raise FileNotFoundError(f\"Checkpoint directory not found: {resume_from_checkpoint}\")\n\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    print(f\">>> Resuming training from: {resume_from_checkpoint}\")\n    print(f\">>> Additional epochs to train: {num_additional_epochs}\")\n    print(f\">>> Resuming from epoch (for logging): {resume_from_epoch}\")\n\n\n    model = AutoModelForCausalLM.from_pretrained(resume_from_checkpoint)\n    tokenizer = AutoTokenizer.from_pretrained(resume_from_checkpoint)\n    model.to(device)\n\n    # Ensure pad token is set correctly\n    if tokenizer.pad_token is None:\n        tokenizer.add_special_tokens({\"pad_token\": tokenizer.eos_token})\n        model.resize_token_embeddings(len(tokenizer))\n    model.config.pad_token_id = tokenizer.pad_token_id\n\n    optimizer = Adafactor(\n        model.parameters(),\n        lr=learning_rate,       \n        scale_parameter=False,  \n        relative_step=False,    \n        warmup_init=False,      \n        weight_decay=weight_decay,\n    )\n\n    # Re-compute number of optimization steps\n    num_update_steps_per_epoch = math.ceil(len(train_dataloader) / gradient_accumulation_steps)\n    max_train_steps = num_additional_epochs * num_update_steps_per_epoch\n\n    lr_scheduler = get_scheduler(\n        \"linear\",\n        optimizer=optimizer,\n        num_warmup_steps=warmup_steps,\n        num_training_steps=max_train_steps,\n    )\n\n    if use_wandb:\n        import wandb\n        wandb.init(\n            project=\"qwen-marathi-finetuning\",\n            name=f\"qwen-marathi-resume-epoch{resume_from_epoch}\",\n            config={\n                \"learning_rate\": learning_rate,\n                \"num_additional_epochs\": num_additional_epochs,\n                \"batch_size_per_device\": per_device_train_batch_size,\n                \"gradient_accumulation_steps\": gradient_accumulation_steps,\n                \"weight_decay\": weight_decay,\n                \"warmup_steps\": warmup_steps,\n                \"optimizer\": \"Adafactor\",\n                \"lr_scheduler\": \"linear\",\n                \"num_gpus\": torch.cuda.device_count(),\n                \"resume_from_checkpoint\": resume_from_checkpoint,\n            },\n            resume=\"allow\",\n        )\n\n    global_step = 0\n    total_loss = 0.0\n    best_eval_loss = float('inf')\n    saved_checkpoints = []\n\n    progress_bar = tqdm(range(max_train_steps), desc=\"Resuming Training\")\n\n    model.zero_grad()\n\n    for epoch in range(num_additional_epochs):\n        model.train()\n        epoch_loss = 0.0\n\n        for step, batch in enumerate(train_dataloader):\n            batch = {k: v.to(device) for k, v in batch.items()}\n\n            outputs = model(**batch)\n            loss = outputs.loss\n            loss = loss / gradient_accumulation_steps\n            loss.backward()\n\n            epoch_loss += loss.detach().float().item()\n            total_loss += loss.detach().float().item()\n\n            if (step + 1) % gradient_accumulation_steps == 0:\n                optimizer.step()\n                lr_scheduler.step()\n                optimizer.zero_grad()\n\n                global_step += 1\n                progress_bar.update(1)\n                progress_bar.set_postfix({\n                    'loss': f'{loss.item() * gradient_accumulation_steps:.4f}',\n                    'lr': f'{lr_scheduler.get_last_lr()[0]:.2e}',\n                    'step': global_step,\n                })\n\n                # Logging\n                if global_step % logging_steps == 0 and use_wandb:\n                    avg_loss = total_loss / global_step\n                    wandb.log({\n                        \"train/loss\": avg_loss,\n                        \"train/learning_rate\": lr_scheduler.get_last_lr()[0],\n                        \"train/epoch\": resume_from_epoch + epoch + (step + 1) / len(train_dataloader),\n                        \"train/global_step\": global_step,\n                    })\n\n                # Evaluation\n                if global_step % eval_steps == 0:\n                    eval_loss = evaluate_model(model, eval_dataloader, device)\n                    print(f\"[Resume Step {global_step}] eval_loss={eval_loss:.4f}\")\n                    if use_wandb:\n                        wandb.log({\"eval/loss\": eval_loss, \"eval/global_step\": global_step})\n\n                    if eval_loss < best_eval_loss:\n                        best_eval_loss = eval_loss\n                        save_checkpoint(\n                            model, tokenizer, output_dir,\n                            f\"best-resume-checkpoint-step\",\n                            saved_checkpoints, save_total_limit\n                        )\n                model.train()\n\n        # End-of-epoch checkpoint\n        avg_epoch_loss = epoch_loss / len(train_dataloader)\n        print(f\"Resume Epoch {resume_from_epoch + epoch + 1} completed • avg_train_loss={avg_epoch_loss:.4f}\")\n        if use_wandb:\n            wandb.log({\"train/epoch_loss\": avg_epoch_loss, \"train/epoch\": resume_from_epoch + epoch + 1})\n\n        save_checkpoint(\n            model, tokenizer, output_dir,\n            f\"resume-epoch-{resume_from_epoch + epoch + 1}\",\n            saved_checkpoints, save_total_limit\n        )\n\n    final_eval_loss = evaluate_model(model, eval_dataloader, device)\n    print(f\"Resume Final evaluation • eval_loss={final_eval_loss:.4f}\")\n    if use_wandb:\n        wandb.log({\"eval/final_loss\": final_eval_loss})\n\n    save_checkpoint(\n        model, tokenizer, output_dir,\n        \"resume-final-checkpoint\",\n        saved_checkpoints, save_total_limit\n    )\n\n    progress_bar.close()\n    if use_wandb:\n        wandb.finish()\n    print(\"Resume training completed!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-13T18:09:34.270644Z","iopub.execute_input":"2025-07-13T18:09:34.271229Z","iopub.status.idle":"2025-07-13T18:09:34.291130Z","shell.execute_reply.started":"2025-07-13T18:09:34.271210Z","shell.execute_reply":"2025-07-13T18:09:34.290428Z"}},"outputs":[],"execution_count":50},{"cell_type":"code","source":"resume_training(\n    resume_from_checkpoint=\"/kaggle/input/trained_50k_46/transformers/default/1/qwen_marathi/best-checkpoint-step\",\n    num_additional_epochs=1,\n    resume_from_epoch=1\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-13T18:09:34.291926Z","iopub.execute_input":"2025-07-13T18:09:34.292455Z","iopub.status.idle":"2025-07-13T18:32:23.926809Z","shell.execute_reply.started":"2025-07-13T18:09:34.292416Z","shell.execute_reply":"2025-07-13T18:32:23.926051Z"}},"outputs":[{"name":"stdout","text":">>> Resuming training from: /kaggle/input/trained_50k_46/transformers/default/1/qwen_marathi/best-checkpoint-step\n>>> Additional epochs to train: 1\n>>> Resuming from epoch (for logging): 1\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Finishing previous runs because reinit is set to 'default'."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">qwen-marathi-run2</strong> at: <a href='https://wandb.ai/22cs3033-rgipt-jais/qwen-marathi-finetuning/runs/074053w5' target=\"_blank\">https://wandb.ai/22cs3033-rgipt-jais/qwen-marathi-finetuning/runs/074053w5</a><br> View project at: <a href='https://wandb.ai/22cs3033-rgipt-jais/qwen-marathi-finetuning' target=\"_blank\">https://wandb.ai/22cs3033-rgipt-jais/qwen-marathi-finetuning</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20250713_180927-074053w5/logs</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.20.1"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250713_180953-p5p2onb0</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/22cs3033-rgipt-jais/qwen-marathi-finetuning/runs/p5p2onb0' target=\"_blank\">qwen-marathi-resume-epoch1</a></strong> to <a href='https://wandb.ai/22cs3033-rgipt-jais/qwen-marathi-finetuning' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/22cs3033-rgipt-jais/qwen-marathi-finetuning' target=\"_blank\">https://wandb.ai/22cs3033-rgipt-jais/qwen-marathi-finetuning</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/22cs3033-rgipt-jais/qwen-marathi-finetuning/runs/p5p2onb0' target=\"_blank\">https://wandb.ai/22cs3033-rgipt-jais/qwen-marathi-finetuning/runs/p5p2onb0</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Resuming Training:   0%|          | 0/1250 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"729aae8c2903435789e48a65fc82ece0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Evaluating:   0%|          | 0/500 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"[Resume Step 500] eval_loss=0.8555\nCheckpoint saved to qwen_marathi/best-resume-checkpoint-step\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Evaluating:   0%|          | 0/500 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"[Resume Step 1000] eval_loss=0.7941\nCheckpoint saved to qwen_marathi/best-resume-checkpoint-step\nResume Epoch 2 completed • avg_train_loss=0.2029\nCheckpoint saved to qwen_marathi/resume-epoch-2\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Evaluating:   0%|          | 0/500 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Resume Final evaluation • eval_loss=0.7775\nCheckpoint saved to qwen_marathi/resume-final-checkpoint\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/final_loss</td><td>▁</td></tr><tr><td>eval/global_step</td><td>▁█</td></tr><tr><td>eval/loss</td><td>█▁</td></tr><tr><td>train/epoch</td><td>▁▆█</td></tr><tr><td>train/epoch_loss</td><td>▁</td></tr><tr><td>train/global_step</td><td>▁█</td></tr><tr><td>train/learning_rate</td><td>█▁</td></tr><tr><td>train/loss</td><td>█▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/final_loss</td><td>0.77754</td></tr><tr><td>eval/global_step</td><td>1000</td></tr><tr><td>eval/loss</td><td>0.79407</td></tr><tr><td>train/epoch</td><td>2</td></tr><tr><td>train/epoch_loss</td><td>0.20288</td></tr><tr><td>train/global_step</td><td>1000</td></tr><tr><td>train/learning_rate</td><td>1e-05</td></tr><tr><td>train/loss</td><td>0.8231</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">qwen-marathi-resume-epoch1</strong> at: <a href='https://wandb.ai/22cs3033-rgipt-jais/qwen-marathi-finetuning/runs/p5p2onb0' target=\"_blank\">https://wandb.ai/22cs3033-rgipt-jais/qwen-marathi-finetuning/runs/p5p2onb0</a><br> View project at: <a href='https://wandb.ai/22cs3033-rgipt-jais/qwen-marathi-finetuning' target=\"_blank\">https://wandb.ai/22cs3033-rgipt-jais/qwen-marathi-finetuning</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20250713_180953-p5p2onb0/logs</code>"},"metadata":{}},{"name":"stdout","text":"Resume training completed!\n","output_type":"stream"}],"execution_count":51},{"cell_type":"code","source":"# save final model & tokenizer to disk\noutput_path = \"qwen_marathi/final_model\"\nos.makedirs(output_path, exist_ok=True)\n\nmodel.save_pretrained(output_path)\ntokenizer.save_pretrained(output_path)\n\nprint(f\"Final model and tokenizer saved to: {output_path}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-13T18:35:23.009408Z","iopub.execute_input":"2025-07-13T18:35:23.010138Z","iopub.status.idle":"2025-07-13T18:35:26.921523Z","shell.execute_reply.started":"2025-07-13T18:35:23.010107Z","shell.execute_reply":"2025-07-13T18:35:26.920840Z"}},"outputs":[{"name":"stdout","text":"Final model and tokenizer saved to: qwen_marathi/final_model\n","output_type":"stream"}],"execution_count":52},{"cell_type":"code","source":"import gc\n# Memory cleanup\ntorch.cuda.empty_cache()\ngc.collect()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-13T18:39:16.283362Z","iopub.execute_input":"2025-07-13T18:39:16.283666Z","iopub.status.idle":"2025-07-13T18:39:16.806219Z","shell.execute_reply.started":"2025-07-13T18:39:16.283645Z","shell.execute_reply":"2025-07-13T18:39:16.805204Z"}},"outputs":[{"execution_count":55,"output_type":"execute_result","data":{"text/plain":"3205"},"metadata":{}}],"execution_count":55},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}