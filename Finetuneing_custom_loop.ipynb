# Define helper functions before using them
def evaluate_model(model, eval_dataloader, accelerator):
    """Evaluate the model and return average loss"""
    model.eval()
    total_eval_loss = 0.0
    eval_steps = 0
    
    if accelerator.is_main_process:
        eval_progress = tqdm(eval_dataloader, desc="Evaluating")
    else:
        eval_progress = eval_dataloader
    
    with torch.no_grad():
        for batch in eval_progress:
            outputs = model(**batch)
            loss = outputs.loss
            total_eval_loss += loss.detach().float()
            eval_steps += 1
    
    # Gather losses from all processes
    total_eval_loss = accelerator.gather(total_eval_loss).mean()
    eval_steps = accelerator.gather(torch.tensor(eval_steps, device=accelerator.device)).sum()
    
    avg_eval_loss = total_eval_loss / eval_steps
    return avg_eval_loss.item()

def save_checkpoint(model, tokenizer, accelerator, output_dir, checkpoint_name, saved_checkpoints, save_total_limit):
    checkpoint_dir = os.path.join(output_dir, checkpoint_name)    
    # Save model and tokenizer
    accelerator.wait_for_everyone()
    unwrapped_model = accelerator.unwrap_model(model)
    
    if accelerator.is_main_process:
        # Save model
        unwrapped_model.save_pretrained(
            checkpoint_dir,
            is_main_process=accelerator.is_main_process,
            save_function=accelerator.save,
            safe_serialization=True
        )
        # Save tokenizer
        tokenizer.save_pretrained(checkpoint_dir)
        print(f"Checkpoint saved to {checkpoint_dir}")
        saved_checkpoints.append(checkpoint_dir)        
        # Remove old checkpoints if limit exceeded
        if len(saved_checkpoints) > save_total_limit:
            oldest_checkpoint = saved_checkpoints.pop(0)
            if os.path.exists(oldest_checkpoint) and "best" not in oldest_checkpoint and "final" not in oldest_checkpoint:
                import shutil
                shutil.rmtree(oldest_checkpoint)
                print(f"Removed old checkpoint: {oldest_checkpoint}"){"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":31091,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd \n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-07-09T17:43:56.062903Z","iopub.execute_input":"2025-07-09T17:43:56.063171Z","iopub.status.idle":"2025-07-09T17:43:56.313355Z","shell.execute_reply.started":"2025-07-09T17:43:56.063150Z","shell.execute_reply":"2025-07-09T17:43:56.312801Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"!pip install -q evaluate","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-09T17:43:56.314433Z","iopub.execute_input":"2025-07-09T17:43:56.314803Z","iopub.status.idle":"2025-07-09T17:44:00.848396Z","shell.execute_reply.started":"2025-07-09T17:43:56.314758Z","shell.execute_reply":"2025-07-09T17:44:00.847532Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.6/193.6 kB\u001b[0m \u001b[31m14.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nbigframes 2.8.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\ncesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\ntorch 2.6.0+cu124 requires nvidia-cublas-cu12==12.4.5.8; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cublas-cu12 12.5.3.2 which is incompatible.\ntorch 2.6.0+cu124 requires nvidia-cuda-cupti-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-cupti-cu12 12.5.82 which is incompatible.\ntorch 2.6.0+cu124 requires nvidia-cuda-nvrtc-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-nvrtc-cu12 12.5.82 which is incompatible.\ntorch 2.6.0+cu124 requires nvidia-cuda-runtime-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-runtime-cu12 12.5.82 which is incompatible.\ntorch 2.6.0+cu124 requires nvidia-cudnn-cu12==9.1.0.70; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cudnn-cu12 9.3.0.75 which is incompatible.\ntorch 2.6.0+cu124 requires nvidia-cufft-cu12==11.2.1.3; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cufft-cu12 11.2.3.61 which is incompatible.\ntorch 2.6.0+cu124 requires nvidia-curand-cu12==10.3.5.147; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-curand-cu12 10.3.6.82 which is incompatible.\ntorch 2.6.0+cu124 requires nvidia-cusolver-cu12==11.6.1.9; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusolver-cu12 11.6.3.83 which is incompatible.\ntorch 2.6.0+cu124 requires nvidia-cusparse-cu12==12.3.1.170; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusparse-cu12 12.5.1.3 which is incompatible.\ntorch 2.6.0+cu124 requires nvidia-nvjitlink-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-nvjitlink-cu12 12.5.82 which is incompatible.\ngcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2025.3.0 which is incompatible.\nbigframes 2.8.0 requires google-cloud-bigquery[bqstorage,pandas]>=3.31.0, but you have google-cloud-bigquery 3.25.0 which is incompatible.\nbigframes 2.8.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"!pip install -q sacrebleu","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-09T17:44:00.849528Z","iopub.execute_input":"2025-07-09T17:44:00.849842Z","iopub.status.idle":"2025-07-09T17:44:04.168608Z","shell.execute_reply.started":"2025-07-09T17:44:00.849809Z","shell.execute_reply":"2025-07-09T17:44:04.167917Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.8/51.8 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.1/104.1 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"!pip install -q wandb","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-09T17:44:04.170594Z","iopub.execute_input":"2025-07-09T17:44:04.170841Z","iopub.status.idle":"2025-07-09T17:44:07.210068Z","shell.execute_reply.started":"2025-07-09T17:44:04.170819Z","shell.execute_reply":"2025-07-09T17:44:07.209065Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"# imports\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, Trainer, TrainingArguments, DataCollatorWithPadding, get_scheduler\nfrom datasets import load_dataset\nimport torch\nimport wandb\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader\nfrom accelerate import Accelerator\nfrom transformers.optimization import AdafactorSchedule\nfrom transformers.optimization import get_cosine_schedule_with_warmup\nfrom accelerate import Accelerator\nfrom tqdm.auto import tqdm\nimport os\nimport math","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-09T17:44:07.211201Z","iopub.execute_input":"2025-07-09T17:44:07.211432Z","iopub.status.idle":"2025-07-09T17:44:35.932506Z","shell.execute_reply.started":"2025-07-09T17:44:07.211408Z","shell.execute_reply":"2025-07-09T17:44:35.931911Z"}},"outputs":[{"name":"stderr","text":"2025-07-09 17:44:20.290371: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1752083060.459082      36 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1752083060.508620      36 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"import torch\nprint(f\"CUDA available: {torch.cuda.is_available()}\")\nprint(f\"Number of GPUs: {torch.cuda.device_count()}\")\nfor i in range(torch.cuda.device_count()):\n    print(f\"GPU {i}: {torch.cuda.get_device_name(i)}\")\n\n# Set environment variable for multi-GPU\nimport os\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-09T17:44:35.933246Z","iopub.execute_input":"2025-07-09T17:44:35.933466Z","iopub.status.idle":"2025-07-09T17:44:35.938729Z","shell.execute_reply.started":"2025-07-09T17:44:35.933448Z","shell.execute_reply":"2025-07-09T17:44:35.937920Z"}},"outputs":[{"name":"stdout","text":"CUDA available: True\nNumber of GPUs: 1\nGPU 0: Tesla P100-PCIE-16GB\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"from kaggle_secrets import UserSecretsClient\n\nsecret = UserSecretsClient()\nos.environ[\"WANDB_API_KEY\"] = secret.get_secret(\"WANDB_KEY\") \n\nwandb.login()   ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-09T17:44:35.939907Z","iopub.execute_input":"2025-07-09T17:44:35.940173Z","iopub.status.idle":"2025-07-09T17:44:42.373530Z","shell.execute_reply.started":"2025-07-09T17:44:35.940155Z","shell.execute_reply":"2025-07-09T17:44:42.372815Z"}},"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33m22cs3033\u001b[0m (\u001b[33m22cs3033-rgipt-jais\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","output_type":"stream"},{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}],"execution_count":7},{"cell_type":"code","source":"# loading model and tokenizer into the RAM\nmodel_name = \"Qwen/Qwen3-0.6B\"\nmodel = AutoModelForCausalLM.from_pretrained(model_name, low_cpu_mem_usage=True )\nmodel.config.use_cache = False\ntokenizer = AutoTokenizer.from_pretrained(model_name)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-09T17:44:42.374330Z","iopub.execute_input":"2025-07-09T17:44:42.374940Z","iopub.status.idle":"2025-07-09T17:44:54.790337Z","shell.execute_reply.started":"2025-07-09T17:44:42.374919Z","shell.execute_reply":"2025-07-09T17:44:54.789283Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/726 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ef8dba0075774688b43ccd4bd3232d27"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/1.50G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8377dc487b92433083cfcc5944b6d960"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/239 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"65cd9d17667049568d509868ed1d6149"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c4d7a9339b1b4dafbcd8f00b3b1a5a31"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dc24c37c867d4c20b3d9673eb20b800f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d892bef4c77240c0bbebe07ce5f5f32a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/11.4M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d7b01e6c625a46388dd33b6988965e9c"}},"metadata":{}}],"execution_count":8},{"cell_type":"code","source":"# create the response on default pretrained model\nprompt = \"Dengue cases on the rise in city\"\nmessages = [\n    {\"role\": \"system\", \"content\": \"You are a translator. who knows marathai and english language very well. Your task is convert given english sentence in marathi.\"},\n    {\"role\": \"user\", \"content\": prompt}\n]\ntext = tokenizer.apply_chat_template(\n    messages,\n    tokenize=False,\n    add_generation_prompt=True\n)\nmodel_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n\ngenerated_ids = model.generate(\n    **model_inputs,\n    max_new_tokens=512\n)\ngenerated_ids = [\n    output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n]\n\nresponse = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-09T17:44:54.791350Z","iopub.execute_input":"2025-07-09T17:44:54.791638Z","iopub.status.idle":"2025-07-09T17:46:20.796484Z","shell.execute_reply.started":"2025-07-09T17:44:54.791612Z","shell.execute_reply":"2025-07-09T17:46:20.795632Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"response","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-09T17:46:20.799269Z","iopub.execute_input":"2025-07-09T17:46:20.799493Z","iopub.status.idle":"2025-07-09T17:46:20.804470Z","shell.execute_reply.started":"2025-07-09T17:46:20.799476Z","shell.execute_reply":"2025-07-09T17:46:20.803713Z"}},"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"'<think>\\nOkay, the user wants me to translate the sentence \"Dengue cases on the rise in city\" into Marathi. Let me start by breaking down the sentence. \\n\\nFirst, \"Dengue cases\" – I know \"dengue\" is a common name in Marathi, so I should use the transliteration. The word \"cases\" here refers to the number of cases, so \"cases\" in Marathi is \"प्रतिक्रम\" or \"प्रतिक्रमान\". But \"cases\" in a medical context could be \"प्रतिक्रमान\". \\n\\nNext, \"on the rise\" – the Marathi word for \"on the rise\" is \"अवयव जो जमीन में घट बढ़ रहा है\". Wait, \"rise\" here is positive, so \"बढ़ रहा है\". \\n\\nThen, \"in city\" – \"महासंघ\" or \"महासंघ में\". \"City\" in Marathi is \"महासंघ\". \\n\\nPutting it all together: \"प्रतिक्रमान बढ़ रहा है महासंघ में\". Let me check if \"rise\" is correctly translated. Yes, \"बढ़ रहा है\". \\n\\nI should also ensure that the sentence structure is correct in Marathi. Maybe using \"में\" for \"in the city\" to make it flow well. \\n\\nIs there any other way to phrase this? For example, \"महासंघ में बढ़ रहा है देंगे जो जमीन में\". But the original sentence is \"Dengue cases on the rise in city\", so the order is important. \\n\\nI think the translation I have is accurate. Let me confirm with a quick check. Yes, \"प्रतिक्रमान बढ़ रहा है महासंघ में\" correctly translates the original sentence. \\n\\nSo, the final answer should be \"प्रतिक्रमान बढ़ रहा है म'"},"metadata":{}}],"execution_count":10},{"cell_type":"code","source":"# load the english to marathi translation data from huggingface\nds = load_dataset(\"anujsahani01/English-Marathi\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-09T17:46:20.805206Z","iopub.execute_input":"2025-07-09T17:46:20.805405Z","iopub.status.idle":"2025-07-09T17:46:44.191615Z","shell.execute_reply.started":"2025-07-09T17:46:20.805391Z","shell.execute_reply":"2025-07-09T17:46:44.190912Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/206 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5ef95eb5ab144942891905d28c109f2d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train.csv:   0%|          | 0.00/621M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9d145325077648e1a85fcc93d5569a72"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"test.csv:   0%|          | 0.00/243M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"367b278ac254484d97e98553a61729a8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/2637962 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d68932e35ab84867a63b52c2be95842e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/879321 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8c46ffd4db1d4d59841442dbc15490fb"}},"metadata":{}}],"execution_count":11},{"cell_type":"code","source":"ds","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-09T17:46:44.192370Z","iopub.execute_input":"2025-07-09T17:46:44.192859Z","iopub.status.idle":"2025-07-09T17:46:44.197322Z","shell.execute_reply.started":"2025-07-09T17:46:44.192836Z","shell.execute_reply":"2025-07-09T17:46:44.196662Z"}},"outputs":[{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['english', 'marathi'],\n        num_rows: 2637962\n    })\n    test: Dataset({\n        features: ['english', 'marathi'],\n        num_rows: 879321\n    })\n})"},"metadata":{}}],"execution_count":12},{"cell_type":"code","source":"ds['train'][10]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-09T17:46:44.198073Z","iopub.execute_input":"2025-07-09T17:46:44.198417Z","iopub.status.idle":"2025-07-09T17:46:44.796737Z","shell.execute_reply.started":"2025-07-09T17:46:44.198394Z","shell.execute_reply":"2025-07-09T17:46:44.796087Z"}},"outputs":[{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"{'english': 'The funds collected are used for social purposes.',\n 'marathi': 'त्या निधीचा वापर सामाजिक कार्यासाठी करतो.'}"},"metadata":{}}],"execution_count":13},{"cell_type":"code","source":"# Dataset is too big so slice it for quick experimentation\ndf_train = ds[\"train\"].select(range(1000))\ndf_validation = ds[\"train\"].select(range(1000 , 1100))\ndf_test = ds[\"test\"].select(range(100))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-09T17:46:44.797647Z","iopub.execute_input":"2025-07-09T17:46:44.798237Z","iopub.status.idle":"2025-07-09T17:46:44.812110Z","shell.execute_reply.started":"2025-07-09T17:46:44.798216Z","shell.execute_reply":"2025-07-09T17:46:44.811462Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"# Qwen3-0.6B is Causal Language Models so we have finetuned it Instruction-output format. (instruction‑tuning)\ndef add_instruction(example):\n    example[\"instruction\"] = \"Convert the English text into Marathi language.\"\n    return example\n\n\ndf_train = df_train.map(add_instruction, batched=False)\ndf_validation = df_validation.map(add_instruction, batched=False)\ndf_test = df_test.map(add_instruction, batched=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-09T17:46:44.812885Z","iopub.execute_input":"2025-07-09T17:46:44.813156Z","iopub.status.idle":"2025-07-09T17:46:44.902793Z","shell.execute_reply.started":"2025-07-09T17:46:44.813134Z","shell.execute_reply":"2025-07-09T17:46:44.902128Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/1000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c20ac73f1e1b45b38da0daaa9cf7b84c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/100 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b20a816011fa4e13a4dc4d0341104d7f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/100 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f5dacb0e52ad46399147b2d2f01b3731"}},"metadata":{}}],"execution_count":15},{"cell_type":"code","source":"ds[\"train\"].column_names","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-09T17:46:44.903508Z","iopub.execute_input":"2025-07-09T17:46:44.903755Z","iopub.status.idle":"2025-07-09T17:46:44.908318Z","shell.execute_reply.started":"2025-07-09T17:46:44.903736Z","shell.execute_reply":"2025-07-09T17:46:44.907619Z"}},"outputs":[{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"['english', 'marathi']"},"metadata":{}}],"execution_count":16},{"cell_type":"code","source":"df_train[0]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-09T17:46:44.909174Z","iopub.execute_input":"2025-07-09T17:46:44.909417Z","iopub.status.idle":"2025-07-09T17:46:44.923086Z","shell.execute_reply.started":"2025-07-09T17:46:44.909397Z","shell.execute_reply":"2025-07-09T17:46:44.922415Z"}},"outputs":[{"execution_count":17,"output_type":"execute_result","data":{"text/plain":"{'english': 'Next few months are really crucial for us.',\n 'marathi': 'पुढील तीन महिने आमच्यासाठी खूप महत्त्वपूर्ण आहेत.',\n 'instruction': 'Convert the English text into Marathi language.'}"},"metadata":{}}],"execution_count":17},{"cell_type":"code","source":"# Know about how tokenizer works\ninputs = tokenizer('Next few months are really crucial for us.','पुढील तीन महिने आमच्यासाठी खूप महत्त्वपूर्ण आहेत.', 'Convert the English text into Marathi language.' )\ninputs","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-09T17:46:44.923852Z","iopub.execute_input":"2025-07-09T17:46:44.924110Z","iopub.status.idle":"2025-07-09T17:46:44.935643Z","shell.execute_reply.started":"2025-07-09T17:46:44.924095Z","shell.execute_reply":"2025-07-09T17:46:44.935111Z"}},"outputs":[{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"{'input_ids': [5847, 2421, 3951, 525, 2167, 16587, 369, 601, 13, 86162, 72653, 149269, 43647, 91811, 14925, 97, 43647, 60096, 91217, 93948, 42311, 101, 34370, 14925, 228, 87244, 146113, 30484, 107, 31411, 116, 31411, 254, 43647, 14925, 244, 12619, 224, 86162, 91217, 93948, 79238, 30484, 97, 30484, 113, 86162, 12619, 224, 44179, 30484, 96, 14925, 228, 93948, 54784, 97, 13], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': [12012, 279, 6364, 1467, 1119, 2876, 66531, 4128, 13]}"},"metadata":{}}],"execution_count":18},{"cell_type":"code","source":"tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-09T17:46:44.936360Z","iopub.execute_input":"2025-07-09T17:46:44.936589Z","iopub.status.idle":"2025-07-09T17:46:44.947043Z","shell.execute_reply.started":"2025-07-09T17:46:44.936568Z","shell.execute_reply":"2025-07-09T17:46:44.946468Z"}},"outputs":[{"execution_count":19,"output_type":"execute_result","data":{"text/plain":"['Next',\n 'Ġfew',\n 'Ġmonths',\n 'Ġare',\n 'Ġreally',\n 'Ġcrucial',\n 'Ġfor',\n 'Ġus',\n '.',\n 'à¤ª',\n 'à¥ģ',\n 'à¤¢',\n 'à¥Ģ',\n 'à¤²',\n 'Ġà¤',\n '¤',\n 'à¥Ģ',\n 'à¤¨',\n 'Ġà¤®',\n 'à¤¹',\n 'à¤¿à¤',\n '¨',\n 'à¥ĩ',\n 'Ġà¤',\n 'Ĩ',\n 'à¤®',\n 'à¤ļ',\n 'à¥įà¤',\n '¯',\n 'à¤¾à¤',\n '¸',\n 'à¤¾à¤',\n 'ł',\n 'à¥Ģ',\n 'Ġà¤',\n 'ĸ',\n 'à¥',\n 'Ĥ',\n 'à¤ª',\n 'Ġà¤®',\n 'à¤¹',\n 'à¤¤',\n 'à¥įà¤',\n '¤',\n 'à¥įà¤',\n 'µ',\n 'à¤ª',\n 'à¥',\n 'Ĥ',\n 'à¤°',\n 'à¥įà¤',\n '£',\n 'Ġà¤',\n 'Ĩ',\n 'à¤¹',\n 'à¥ĩà¤',\n '¤',\n '.']"},"metadata":{}}],"execution_count":19},{"cell_type":"code","source":"instruction = \"Convert the English text into Marathi language.\"\nsrc_text     = \"Next few months are really crucial for us.\"\ntgt_text     = \"पुढील तीन महिने आमच्यासाठी खूप महत्त्वपूर्ण आहेत.\"\n\n# Prompt the model should *see*\nprompt = f\"{instruction} English: {src_text} Marathi:\"\nprint(prompt)\n\ninputs  = tokenizer(prompt, return_tensors=\"pt\") \nprint('Length of tokenize form of prompt:', inputs['input_ids'].size())\n\n\n# Every token sequence in a causal‐LM fine‑tuning setup needs a clear “stop” signal so the model knows when it’s done generating. That’s exactly what tokenizer.eos_token_id is for\n\nlabels  = tokenizer(tgt_text, add_special_tokens=False).input_ids + [tokenizer.eos_token_id] #Here labes are tokenize form our marathi output","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-09T17:46:44.947715Z","iopub.execute_input":"2025-07-09T17:46:44.948307Z","iopub.status.idle":"2025-07-09T17:46:44.960596Z","shell.execute_reply.started":"2025-07-09T17:46:44.948290Z","shell.execute_reply":"2025-07-09T17:46:44.959891Z"}},"outputs":[{"name":"stdout","text":"Convert the English text into Marathi language. English: Next few months are really crucial for us. Marathi:\nLength of tokenize form of prompt: torch.Size([1, 23])\n","output_type":"stream"}],"execution_count":20},{"cell_type":"code","source":"inputs","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-09T17:46:44.961429Z","iopub.execute_input":"2025-07-09T17:46:44.962164Z","iopub.status.idle":"2025-07-09T17:46:44.978578Z","shell.execute_reply.started":"2025-07-09T17:46:44.962146Z","shell.execute_reply":"2025-07-09T17:46:44.977890Z"}},"outputs":[{"execution_count":21,"output_type":"execute_result","data":{"text/plain":"{'input_ids': tensor([[12012,   279,  6364,  1467,  1119,  2876, 66531,  4128,    13,  6364,\n            25,  9295,  2421,  3951,   525,  2167, 16587,   369,   601,    13,\n          2876, 66531,    25]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}"},"metadata":{}}],"execution_count":21},{"cell_type":"code","source":"# The length of tokenize input prompt( instrution + context) is 23","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-09T17:46:44.979433Z","iopub.execute_input":"2025-07-09T17:46:44.979903Z","iopub.status.idle":"2025-07-09T17:46:44.987026Z","shell.execute_reply.started":"2025-07-09T17:46:44.979886Z","shell.execute_reply":"2025-07-09T17:46:44.986423Z"}},"outputs":[],"execution_count":22},{"cell_type":"code","source":"tokenizer.eos_token_id","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-09T17:46:44.987689Z","iopub.execute_input":"2025-07-09T17:46:44.987930Z","iopub.status.idle":"2025-07-09T17:46:44.999440Z","shell.execute_reply.started":"2025-07-09T17:46:44.987906Z","shell.execute_reply":"2025-07-09T17:46:44.998901Z"}},"outputs":[{"execution_count":23,"output_type":"execute_result","data":{"text/plain":"151645"},"metadata":{}}],"execution_count":23},{"cell_type":"code","source":"labels ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-09T17:46:45.000163Z","iopub.execute_input":"2025-07-09T17:46:45.000817Z","iopub.status.idle":"2025-07-09T17:46:45.010065Z","shell.execute_reply.started":"2025-07-09T17:46:45.000793Z","shell.execute_reply":"2025-07-09T17:46:45.009502Z"}},"outputs":[{"execution_count":24,"output_type":"execute_result","data":{"text/plain":"[86162,\n 72653,\n 149269,\n 43647,\n 91811,\n 14925,\n 97,\n 43647,\n 60096,\n 91217,\n 93948,\n 42311,\n 101,\n 34370,\n 14925,\n 228,\n 87244,\n 146113,\n 30484,\n 107,\n 31411,\n 116,\n 31411,\n 254,\n 43647,\n 14925,\n 244,\n 12619,\n 224,\n 86162,\n 91217,\n 93948,\n 79238,\n 30484,\n 97,\n 30484,\n 113,\n 86162,\n 12619,\n 224,\n 44179,\n 30484,\n 96,\n 14925,\n 228,\n 93948,\n 54784,\n 97,\n 13,\n 151645]"},"metadata":{}}],"execution_count":24},{"cell_type":"code","source":"len(labels) # 49 orignal + 1 tokenizer.eos_token_id = 50","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-09T17:46:45.010721Z","iopub.execute_input":"2025-07-09T17:46:45.011297Z","iopub.status.idle":"2025-07-09T17:46:45.021341Z","shell.execute_reply.started":"2025-07-09T17:46:45.011280Z","shell.execute_reply":"2025-07-09T17:46:45.020800Z"}},"outputs":[{"execution_count":25,"output_type":"execute_result","data":{"text/plain":"50"},"metadata":{}}],"execution_count":25},{"cell_type":"code","source":"# For a causal model, you feed it the entire sequence (prompt and target), because it learns by shifting that sequence one token at a time.\nmodel_inputs = tokenizer(prompt + \" \" + tgt_text + tokenizer.eos_token,  return_tensors=\"pt\")\nprint('Length of tokenize form of model_inputs:', model_inputs['input_ids'].size())  \n\n# You need to know how many tokens correspond just to the instruction+source (the “prompt”) so that you can later mask them out in the labels.\nprompt_len = len(tokenizer(prompt, add_special_tokens=False).input_ids)\nprint(\"Tokenized Prompt length:\" , prompt_len)\n\n#  Clone so you don’t overwrite the original inputs\nlabels = model_inputs[\"input_ids\"].clone()\n\n\n# For every position in the prompt, set label to -100  \n# In Hugging Face Trainer, any label token = –100 is ignored by the loss function.\nlabels[:, :prompt_len] = -100        # works for a batch of size 1 or N\n# print('Length that going to be masked' , len(labels))\nmodel_inputs[\"labels\"] = labels","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-09T17:46:45.022084Z","iopub.execute_input":"2025-07-09T17:46:45.022356Z","iopub.status.idle":"2025-07-09T17:46:45.032359Z","shell.execute_reply.started":"2025-07-09T17:46:45.022333Z","shell.execute_reply":"2025-07-09T17:46:45.031665Z"}},"outputs":[{"name":"stdout","text":"Length of tokenize form of model_inputs: torch.Size([1, 73])\nTokenized Prompt length: 23\n","output_type":"stream"}],"execution_count":26},{"cell_type":"code","source":"labels","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-09T17:46:45.033061Z","iopub.execute_input":"2025-07-09T17:46:45.033300Z","iopub.status.idle":"2025-07-09T17:46:45.041293Z","shell.execute_reply.started":"2025-07-09T17:46:45.033276Z","shell.execute_reply":"2025-07-09T17:46:45.040678Z"}},"outputs":[{"execution_count":27,"output_type":"execute_result","data":{"text/plain":"tensor([[  -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n           -100,   -100,   -100,   -100,   -100,  83636,  72653, 149269,  43647,\n          91811,  14925,     97,  43647,  60096,  91217,  93948,  42311,    101,\n          34370,  14925,    228,  87244, 146113,  30484,    107,  31411,    116,\n          31411,    254,  43647,  14925,    244,  12619,    224,  86162,  91217,\n          93948,  79238,  30484,     97,  30484,    113,  86162,  12619,    224,\n          44179,  30484,     96,  14925,    228,  93948,  54784,     97,     13,\n         151645]])"},"metadata":{}}],"execution_count":27},{"cell_type":"code","source":"labels.size()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-09T17:46:45.044763Z","iopub.execute_input":"2025-07-09T17:46:45.045031Z","iopub.status.idle":"2025-07-09T17:46:45.051655Z","shell.execute_reply.started":"2025-07-09T17:46:45.045010Z","shell.execute_reply":"2025-07-09T17:46:45.051120Z"}},"outputs":[{"execution_count":28,"output_type":"execute_result","data":{"text/plain":"torch.Size([1, 73])"},"metadata":{}}],"execution_count":28},{"cell_type":"code","source":"x = labels[0] \nnum_ignored = (x == -100).sum().item()\nnum_not_ignored = (x != -100).sum().item()\nprint(num_ignored)\nprint(num_not_ignored)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-09T17:46:45.052349Z","iopub.execute_input":"2025-07-09T17:46:45.052595Z","iopub.status.idle":"2025-07-09T17:46:45.065574Z","shell.execute_reply.started":"2025-07-09T17:46:45.052574Z","shell.execute_reply":"2025-07-09T17:46:45.064975Z"}},"outputs":[{"name":"stdout","text":"23\n50\n","output_type":"stream"}],"execution_count":29},{"cell_type":"code","source":"model_inputs","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-09T17:46:45.066341Z","iopub.execute_input":"2025-07-09T17:46:45.066756Z","iopub.status.idle":"2025-07-09T17:46:45.077096Z","shell.execute_reply.started":"2025-07-09T17:46:45.066739Z","shell.execute_reply":"2025-07-09T17:46:45.076446Z"}},"outputs":[{"execution_count":30,"output_type":"execute_result","data":{"text/plain":"{'input_ids': tensor([[ 12012,    279,   6364,   1467,   1119,   2876,  66531,   4128,     13,\n           6364,     25,   9295,   2421,   3951,    525,   2167,  16587,    369,\n            601,     13,   2876,  66531,     25,  83636,  72653, 149269,  43647,\n          91811,  14925,     97,  43647,  60096,  91217,  93948,  42311,    101,\n          34370,  14925,    228,  87244, 146113,  30484,    107,  31411,    116,\n          31411,    254,  43647,  14925,    244,  12619,    224,  86162,  91217,\n          93948,  79238,  30484,     97,  30484,    113,  86162,  12619,    224,\n          44179,  30484,     96,  14925,    228,  93948,  54784,     97,     13,\n         151645]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1]]), 'labels': tensor([[  -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n           -100,   -100,   -100,   -100,   -100,  83636,  72653, 149269,  43647,\n          91811,  14925,     97,  43647,  60096,  91217,  93948,  42311,    101,\n          34370,  14925,    228,  87244, 146113,  30484,    107,  31411,    116,\n          31411,    254,  43647,  14925,    244,  12619,    224,  86162,  91217,\n          93948,  79238,  30484,     97,  30484,    113,  86162,  12619,    224,\n          44179,  30484,     96,  14925,    228,  93948,  54784,     97,     13,\n         151645]])}"},"metadata":{}}],"execution_count":30},{"cell_type":"code","source":"model_inputs['input_ids'].size()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-09T17:46:45.077953Z","iopub.execute_input":"2025-07-09T17:46:45.078179Z","iopub.status.idle":"2025-07-09T17:46:45.086607Z","shell.execute_reply.started":"2025-07-09T17:46:45.078164Z","shell.execute_reply":"2025-07-09T17:46:45.086095Z"}},"outputs":[{"execution_count":31,"output_type":"execute_result","data":{"text/plain":"torch.Size([1, 73])"},"metadata":{}}],"execution_count":31},{"cell_type":"code","source":"def tokenize_function(examples):\n\n    batch_input_ids      = []\n    batch_attention_mask = []\n    batch_labels         = []\n\n    # Iterate over the batch\n    for instruction, english, marathi in zip(\n            examples[\"instruction\"], examples[\"english\"], examples[\"marathi\"]):\n\n        # 1. Build the prompt shown to the model (not trained on)\n        prompt_text = f\"{instruction} English: {english} Marathi:\"\n        prompt_ids  = tokenizer(prompt_text, add_special_tokens=False).input_ids\n\n        # 2. Full text = prompt + target + EOS\n        full_text = f\"{prompt_text} {marathi}{tokenizer.eos_token}\"\n        tok = tokenizer(full_text, add_special_tokens=False)\n\n        # 3. Create labels: mask the prompt tokens with –100\n        label_ids = [-100] * len(prompt_ids) + tok[\"input_ids\"][len(prompt_ids):]\n\n        # 4. Collect results for this example\n        batch_input_ids.append(tok[\"input_ids\"])\n        batch_attention_mask.append(tok[\"attention_mask\"])\n        batch_labels.append(label_ids)\n\n    return {\n        \"input_ids\":      batch_input_ids,\n        \"attention_mask\": batch_attention_mask,\n        \"labels\":         batch_labels,\n    }","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-09T17:46:45.087223Z","iopub.execute_input":"2025-07-09T17:46:45.087433Z","iopub.status.idle":"2025-07-09T17:46:45.096857Z","shell.execute_reply.started":"2025-07-09T17:46:45.087418Z","shell.execute_reply":"2025-07-09T17:46:45.096186Z"}},"outputs":[],"execution_count":32},{"cell_type":"code","source":"train_tokenised_ds = df_train.map(tokenize_function, batched=True , num_proc=4 , remove_columns=df_train.column_names)\ntest_tokenised_ds = df_test.map(tokenize_function, batched=True , num_proc=4 , remove_columns=df_test.column_names)\nvalidation_tokenised_ds = df_validation.map(tokenize_function, batched=True , num_proc=4, remove_columns=df_validation.column_names )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-09T17:46:45.097569Z","iopub.execute_input":"2025-07-09T17:46:45.098330Z","iopub.status.idle":"2025-07-09T17:46:47.377934Z","shell.execute_reply.started":"2025-07-09T17:46:45.098306Z","shell.execute_reply":"2025-07-09T17:46:47.377040Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map (num_proc=4):   0%|          | 0/1000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"85cbfbe5eb96481ea3fd310fae929fdf"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map (num_proc=4):   0%|          | 0/100 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c44fc980d2c045178f9087da446a67de"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map (num_proc=4):   0%|          | 0/100 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d9fd1c67d91f4563be4ede29385c4288"}},"metadata":{}}],"execution_count":33},{"cell_type":"code","source":"test_tokenised_ds","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-09T17:46:47.379168Z","iopub.execute_input":"2025-07-09T17:46:47.379435Z","iopub.status.idle":"2025-07-09T17:46:47.385496Z","shell.execute_reply.started":"2025-07-09T17:46:47.379409Z","shell.execute_reply":"2025-07-09T17:46:47.384623Z"}},"outputs":[{"execution_count":34,"output_type":"execute_result","data":{"text/plain":"Dataset({\n    features: ['input_ids', 'attention_mask', 'labels'],\n    num_rows: 100\n})"},"metadata":{}}],"execution_count":34},{"cell_type":"code","source":"len(test_tokenised_ds[0]['input_ids'])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-09T17:46:47.386203Z","iopub.execute_input":"2025-07-09T17:46:47.386457Z","iopub.status.idle":"2025-07-09T17:46:47.404599Z","shell.execute_reply.started":"2025-07-09T17:46:47.386432Z","shell.execute_reply":"2025-07-09T17:46:47.403476Z"}},"outputs":[{"execution_count":35,"output_type":"execute_result","data":{"text/plain":"90"},"metadata":{}}],"execution_count":35},{"cell_type":"code","source":"if tokenizer.pad_token is None:\n    tokenizer.add_special_tokens({\"pad_token\": tokenizer.eos_token})\n    model.resize_token_embeddings(len(tokenizer))          # update embeddings\nmodel.config.pad_token_id = tokenizer.pad_token_id","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-09T17:46:47.405635Z","iopub.execute_input":"2025-07-09T17:46:47.405972Z","iopub.status.idle":"2025-07-09T17:46:47.416927Z","shell.execute_reply.started":"2025-07-09T17:46:47.405938Z","shell.execute_reply":"2025-07-09T17:46:47.416244Z"}},"outputs":[],"execution_count":36},{"cell_type":"code","source":"for split in (train_tokenised_ds, test_tokenised_ds, validation_tokenised_ds):\n    split.set_format(type=\"torch\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-09T17:46:47.417754Z","iopub.execute_input":"2025-07-09T17:46:47.418095Z","iopub.status.idle":"2025-07-09T17:46:47.433738Z","shell.execute_reply.started":"2025-07-09T17:46:47.418068Z","shell.execute_reply":"2025-07-09T17:46:47.432908Z"}},"outputs":[],"execution_count":37},{"cell_type":"code","source":"\nfrom transformers import DataCollatorForSeq2Seq\n\n\ndata_collator = DataCollatorForSeq2Seq(\n    tokenizer=tokenizer,\n    model=model,\n    padding=True,\n    label_pad_token_id=-100,\n    pad_to_multiple_of=8,\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-09T17:46:47.434742Z","iopub.execute_input":"2025-07-09T17:46:47.435376Z","iopub.status.idle":"2025-07-09T17:46:47.447347Z","shell.execute_reply.started":"2025-07-09T17:46:47.435355Z","shell.execute_reply":"2025-07-09T17:46:47.446465Z"}},"outputs":[],"execution_count":38},{"cell_type":"code","source":"train_tokenised_ds","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-09T17:46:47.448269Z","iopub.execute_input":"2025-07-09T17:46:47.448511Z","iopub.status.idle":"2025-07-09T17:46:47.461995Z","shell.execute_reply.started":"2025-07-09T17:46:47.448487Z","shell.execute_reply":"2025-07-09T17:46:47.461162Z"}},"outputs":[{"execution_count":39,"output_type":"execute_result","data":{"text/plain":"Dataset({\n    features: ['input_ids', 'attention_mask', 'labels'],\n    num_rows: 1000\n})"},"metadata":{}}],"execution_count":39},{"cell_type":"code","source":"# Create data loaders\ntrain_dataloader = DataLoader(\n        train_tokenised_ds,\n        batch_size=8,\n        shuffle=True,\n        collate_fn=data_collator,\n        pin_memory=False\n)\n    \neval_dataloader = DataLoader(\n    validation_tokenised_ds,\n    batch_size=8,\n    shuffle=False,\n    collate_fn=data_collator,\n    pin_memory=False\n)\n\ntest_dataloader = DataLoader(\n    test_tokenised_ds,\n    batch_size=8,\n    shuffle=False,\n    collate_fn=data_collator,\n    pin_memory=False\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-09T17:46:47.462969Z","iopub.execute_input":"2025-07-09T17:46:47.463316Z","iopub.status.idle":"2025-07-09T17:46:47.476060Z","shell.execute_reply.started":"2025-07-09T17:46:47.463290Z","shell.execute_reply":"2025-07-09T17:46:47.475353Z"}},"outputs":[],"execution_count":40},{"cell_type":"code","source":"import os\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1\"\naccelerator = Accelerator(\n    gradient_accumulation_steps=2,\n    device_placement=True,\n    mixed_precision=\"fp16\",  \n    log_with=\"wandb\",\n    project_dir=\"qwen_marathi\"\n)\n\n# 4. Wrap model for in-notebook multi‑GPU with DataParallel\nif torch.cuda.device_count() > 1:\n    print(f\">>> Wrapping model in DataParallel on {torch.cuda.device_count()} GPUs\")\n    model = torch.nn.DataParallel(model)\n\n# 5. Move model to the Accelerator device\nmodel.to(accelerator.device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-09T17:46:47.476928Z","iopub.execute_input":"2025-07-09T17:46:47.477151Z","iopub.status.idle":"2025-07-09T17:46:48.325190Z","shell.execute_reply.started":"2025-07-09T17:46:47.477131Z","shell.execute_reply":"2025-07-09T17:46:48.324598Z"}},"outputs":[{"execution_count":41,"output_type":"execute_result","data":{"text/plain":"Qwen3ForCausalLM(\n  (model): Qwen3Model(\n    (embed_tokens): Embedding(151936, 1024)\n    (layers): ModuleList(\n      (0-27): 28 x Qwen3DecoderLayer(\n        (self_attn): Qwen3Attention(\n          (q_proj): Linear(in_features=1024, out_features=2048, bias=False)\n          (k_proj): Linear(in_features=1024, out_features=1024, bias=False)\n          (v_proj): Linear(in_features=1024, out_features=1024, bias=False)\n          (o_proj): Linear(in_features=2048, out_features=1024, bias=False)\n          (q_norm): Qwen3RMSNorm((128,), eps=1e-06)\n          (k_norm): Qwen3RMSNorm((128,), eps=1e-06)\n        )\n        (mlp): Qwen3MLP(\n          (gate_proj): Linear(in_features=1024, out_features=3072, bias=False)\n          (up_proj): Linear(in_features=1024, out_features=3072, bias=False)\n          (down_proj): Linear(in_features=3072, out_features=1024, bias=False)\n          (act_fn): SiLU()\n        )\n        (input_layernorm): Qwen3RMSNorm((1024,), eps=1e-06)\n        (post_attention_layernorm): Qwen3RMSNorm((1024,), eps=1e-06)\n      )\n    )\n    (norm): Qwen3RMSNorm((1024,), eps=1e-06)\n    (rotary_emb): Qwen3RotaryEmbedding()\n  )\n  (lm_head): Linear(in_features=1024, out_features=151936, bias=False)\n)"},"metadata":{}}],"execution_count":41},{"cell_type":"code","source":"# Optimizer & scheduler\nnum_epochs = 1\nper_device_train_batch_size = 2\nper_device_eval_batch_size = 6\ngradient_accumulation_steps = 2\nlearning_rate = 5e-5\nweight_decay = 0.01\nwarmup_steps = 200\nlogging_steps = 500\neval_steps = 500\nsave_total_limit = 2\noutput_dir = \"qwen_marathi\"\nnum_update_steps_per_epoch = math.ceil(len(train_dataloader) / gradient_accumulation_steps)\nmax_train_steps = num_epochs * num_update_steps_per_epoch\noptimizer = torch.optim.AdamW(model.parameters(),lr=learning_rate,weight_decay=weight_decay,betas=(0.9, 0.999),eps=1e-8)\nlr_scheduler = get_scheduler(\n    \"linear\",\n    optimizer=optimizer,\n    num_warmup_steps=0,\n    num_training_steps=max_train_steps,\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-09T17:46:48.326043Z","iopub.execute_input":"2025-07-09T17:46:48.326834Z","iopub.status.idle":"2025-07-09T17:46:48.333286Z","shell.execute_reply.started":"2025-07-09T17:46:48.326804Z","shell.execute_reply":"2025-07-09T17:46:48.332541Z"}},"outputs":[],"execution_count":42},{"cell_type":"code","source":"\n\n# 6. Prepare everything with Accelerate (include test_dataloader if you plan to run test eval here)\nmodel, optimizer, train_dataloader, eval_dataloader, test_dataloader, lr_scheduler = accelerator.prepare(\n    model, optimizer, train_dataloader, eval_dataloader, test_dataloader, lr_scheduler\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-09T17:46:48.334071Z","iopub.execute_input":"2025-07-09T17:46:48.334339Z","iopub.status.idle":"2025-07-09T17:46:48.381277Z","shell.execute_reply.started":"2025-07-09T17:46:48.334313Z","shell.execute_reply":"2025-07-09T17:46:48.380781Z"}},"outputs":[],"execution_count":43},{"cell_type":"code","source":"# 7. Print training setup (main process only)\nif accelerator.is_main_process:\n    total_batch_size = per_device_train_batch_size * accelerator.num_processes * gradient_accumulation_steps\n    print(f\"***** Running training *****\")\n    print(f\"  Num examples              = {len(train_tokenised_ds)}\")\n    print(f\"  Num Epochs                = {num_epochs}\")\n    print(f\"  Instantaneous batch size  = {per_device_train_batch_size}\")\n    print(f\"  Total train batch size    = {total_batch_size}\")\n    print(f\"  Gradient Accumulation     = {gradient_accumulation_steps}\")\n    print(f\"  Total optimization steps  = {max_train_steps}\")\n    print(f\"  Number of devices         = {accelerator.num_processes}\")\n\ntorch.cuda.empty_cache()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-09T17:46:48.382049Z","iopub.execute_input":"2025-07-09T17:46:48.382294Z","iopub.status.idle":"2025-07-09T17:46:48.387368Z","shell.execute_reply.started":"2025-07-09T17:46:48.382276Z","shell.execute_reply":"2025-07-09T17:46:48.386553Z"}},"outputs":[{"name":"stdout","text":"***** Running training *****\n  Num examples              = 1000\n  Num Epochs                = 1\n  Instantaneous batch size  = 2\n  Total train batch size    = 4\n  Gradient Accumulation     = 2\n  Total optimization steps  = 63\n  Number of devices         = 1\n","output_type":"stream"}],"execution_count":44},{"cell_type":"code","source":"# 8. Initialize WandB (main process only)\nif accelerator.is_main_process:\n    wandb.init(\n        project=\"qwen-marathi-finetuning\",\n        name=\"qwen-marathi-run2\",\n        config={\n            \"learning_rate\": learning_rate,\n            \"num_epochs\": num_epochs,\n            \"batch_size_per_device\": per_device_train_batch_size,\n            \"gradient_accumulation_steps\": gradient_accumulation_steps,\n            \"weight_decay\": weight_decay,\n            \"warmup_steps\": warmup_steps,\n            \"optimizer\": \"AdamW\",\n            \"lr_scheduler\": \"linear\",\n            \"num_gpus\": 2,\n        }\n    )\n\n# 9. Set up training loop variables & progress bar\nglobal_step = 0\ntotal_loss = 0.0\nbest_eval_loss = float('inf')\nsaved_checkpoints = []\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-09T17:46:48.388228Z","iopub.execute_input":"2025-07-09T17:46:48.388502Z","iopub.status.idle":"2025-07-09T17:46:56.577607Z","shell.execute_reply.started":"2025-07-09T17:46:48.388472Z","shell.execute_reply":"2025-07-09T17:46:56.577056Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.20.1"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250709_174648-5d2jwam5</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/22cs3033-rgipt-jais/qwen-marathi-finetuning/runs/5d2jwam5' target=\"_blank\">qwen-marathi-run2</a></strong> to <a href='https://wandb.ai/22cs3033-rgipt-jais/qwen-marathi-finetuning' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/22cs3033-rgipt-jais/qwen-marathi-finetuning' target=\"_blank\">https://wandb.ai/22cs3033-rgipt-jais/qwen-marathi-finetuning</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/22cs3033-rgipt-jais/qwen-marathi-finetuning/runs/5d2jwam5' target=\"_blank\">https://wandb.ai/22cs3033-rgipt-jais/qwen-marathi-finetuning/runs/5d2jwam5</a>"},"metadata":{}}],"execution_count":45},{"cell_type":"code","source":"for epoch in range(num_epochs):\n    model.train()\n    epoch_loss = 0.0\n\n    for step, batch in enumerate(train_dataloader):\n        with accelerator.accumulate(model):\n            outputs = model(**batch)\n            loss = outputs.loss\n            accelerator.backward(loss)\n            optimizer.step()\n            lr_scheduler.step()\n            optimizer.zero_grad()\n\n            epoch_loss += loss.detach().float().item()\n            total_loss += loss.detach().float().item()\n\n        if accelerator.sync_gradients:\n            global_step += 1\n\n            # update tqdm\n            if accelerator.is_main_process:\n                progress_bar.update(1)\n                progress_bar.set_postfix({\n                    'loss': f'{loss.item():.4f}',\n                    'lr': f'{lr_scheduler.get_last_lr()[0]:.2e}',\n                    'step': global_step\n                })\n\n            # logging\n            if global_step % logging_steps == 0:\n                avg_loss = total_loss / global_step\n                current_lr = lr_scheduler.get_last_lr()[0]\n                if accelerator.is_main_process:\n                    print(f\"[Step {global_step}] train_loss={avg_loss:.4f}, lr={current_lr:.2e}\")\n                    wandb.log({\n                        \"train/loss\": avg_loss,\n                        \"train/learning_rate\": current_lr,\n                        \"train/epoch\": epoch + (step + 1) / len(train_dataloader),\n                        \"train/global_step\": global_step\n                    })\n\n            # evaluation\n            if global_step % eval_steps == 0:\n                eval_loss = evaluate_model(model, eval_dataloader, accelerator)\n                if accelerator.is_main_process:\n                    print(f\"[Step {global_step}] eval_loss={eval_loss:.4f}\")\n                    wandb.log({\"eval/loss\": eval_loss, \"eval/global_step\": global_step})\n                    # save best\n                    if eval_loss < best_eval_loss:\n                        best_eval_loss = eval_loss\n                        save_checkpoint(\n                            model, tokenizer, accelerator, output_dir,\n                            f\"best-checkpoint-step{global_step}\",\n                            saved_checkpoints, save_total_limit\n                        )\n                model.train()\n\n    # end of epoch\n    if accelerator.is_main_process:\n        avg_epoch_loss = epoch_loss / len(train_dataloader)\n        print(f\"Epoch {epoch+1}/{num_epochs} • avg_train_loss={avg_epoch_loss:.4f}\")\n        wandb.log({\"train/epoch_loss\": avg_epoch_loss, \"train/epoch\": epoch+1})\n\n        # checkpoint at epoch end\n        save_checkpoint(\n            model, tokenizer, accelerator, output_dir,\n            f\"epoch-{epoch+1}\",\n            saved_checkpoints, save_total_limit\n        )\n\n# Final evaluation & cleanup\nfinal_eval_loss = evaluate_model(model, eval_dataloader, accelerator)\nif accelerator.is_main_process:\n    print(f\"Final evaluation • eval_loss={final_eval_loss:.4f}\")\n    wandb.log({\"eval/final_loss\": final_eval_loss})\n\n    save_checkpoint(\n        model, tokenizer, accelerator, output_dir,\n        \"final-checkpoint\",\n        saved_checkpoints, save_total_limit\n    )\n\n    progress_bar.close()\n    wandb.finish()\n    print(\"Training completed!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-09T17:46:56.578404Z","iopub.execute_input":"2025-07-09T17:46:56.578663Z","iopub.status.idle":"2025-07-09T17:46:59.163510Z","shell.execute_reply.started":"2025-07-09T17:46:56.578640Z","shell.execute_reply":"2025-07-09T17:46:59.162441Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/transformers/data/data_collator.py:741: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:254.)\n  batch[\"labels\"] = torch.tensor(batch[\"labels\"], dtype=torch.int64)\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_36/1950327319.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m             \u001b[0;31m# update tqdm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0maccelerator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_main_process\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m                 \u001b[0mprogress_bar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m                 progress_bar.set_postfix({\n\u001b[1;32m     24\u001b[0m                     \u001b[0;34m'loss'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34mf'{loss.item():.4f}'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'progress_bar' is not defined"],"ename":"NameError","evalue":"name 'progress_bar' is not defined","output_type":"error"}],"execution_count":46},{"cell_type":"code","source":"def evaluate_model(model, eval_dataloader, accelerator):\n    \"\"\"Evaluate the model and return average loss\"\"\"\n    model.eval()\n    total_eval_loss = 0.0\n    eval_steps = 0\n    \n    if accelerator.is_main_process:\n        eval_progress = tqdm(eval_dataloader, desc=\"Evaluating\")\n    else:\n        eval_progress = eval_dataloader\n    \n    with torch.no_grad():\n        for batch in eval_progress:\n            outputs = model(**batch)\n            loss = outputs.loss\n            total_eval_loss += loss.detach().float()\n            eval_steps += 1\n    \n    # Gather losses from all processes\n    total_eval_loss = accelerator.gather(total_eval_loss).mean()\n    eval_steps = accelerator.gather(torch.tensor(eval_steps, device=accelerator.device)).sum()\n    \n    avg_eval_loss = total_eval_loss / eval_steps\n    return avg_eval_loss.item()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-09T17:46:59.164181Z","iopub.status.idle":"2025-07-09T17:46:59.164498Z","shell.execute_reply.started":"2025-07-09T17:46:59.164361Z","shell.execute_reply":"2025-07-09T17:46:59.164380Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def save_checkpoint(model, tokenizer, accelerator, output_dir, checkpoint_name, saved_checkpoints, save_total_limit):\n    checkpoint_dir = os.path.join(output_dir, checkpoint_name)    \n    # Save model and tokenizer\n    accelerator.wait_for_everyone()\n    unwrapped_model = accelerator.unwrap_model(model)\n    \n    if accelerator.is_main_process:\n        # Save model\n        unwrapped_model.save_pretrained(\n            checkpoint_dir,\n            is_main_process=accelerator.is_main_process,\n            save_function=accelerator.save,\n            safe_serialization=True\n        )\n        # Save tokenizer\n        tokenizer.save_pretrained(checkpoint_dir)\n        print(f\"Checkpoint saved to {checkpoint_dir}\")\n        saved_checkpoints.append(checkpoint_dir)        \n        # Remove old checkpoints if limit exceeded\n        if len(saved_checkpoints) > save_total_limit:\n            oldest_checkpoint = saved_checkpoints.pop(0)\n            if os.path.exists(oldest_checkpoint) and \"best\" not in oldest_checkpoint and \"final\" not in oldest_checkpoint:\n                import shutil\n                shutil.rmtree(oldest_checkpoint)\n                print(f\"Removed old checkpoint: {oldest_checkpoint}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-09T17:46:59.165755Z","iopub.status.idle":"2025-07-09T17:46:59.166006Z","shell.execute_reply.started":"2025-07-09T17:46:59.165901Z","shell.execute_reply":"2025-07-09T17:46:59.165911Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}