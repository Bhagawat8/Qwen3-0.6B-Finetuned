{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.status.busy":"2025-07-13T19:20:10.310499Z","iopub.execute_input":"2025-07-13T19:20:10.311071Z","iopub.status.idle":"2025-07-13T19:20:10.686031Z","shell.execute_reply.started":"2025-07-13T19:20:10.311039Z","shell.execute_reply":"2025-07-13T19:20:10.685175Z"},"trusted":true},"outputs":[],"execution_count":1},{"cell_type":"code","source":"!pip install -q evaluate","metadata":{"execution":{"iopub.status.busy":"2025-07-13T19:20:11.174924Z","iopub.execute_input":"2025-07-13T19:20:11.175309Z","iopub.status.idle":"2025-07-13T19:20:16.131037Z","shell.execute_reply.started":"2025-07-13T19:20:11.175288Z","shell.execute_reply":"2025-07-13T19:20:16.130123Z"},"trusted":true},"outputs":[{"name":"stdout","text":"\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.6/193.6 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nbigframes 2.8.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\ncesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\ntorch 2.6.0+cu124 requires nvidia-cublas-cu12==12.4.5.8; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cublas-cu12 12.5.3.2 which is incompatible.\ntorch 2.6.0+cu124 requires nvidia-cuda-cupti-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-cupti-cu12 12.5.82 which is incompatible.\ntorch 2.6.0+cu124 requires nvidia-cuda-nvrtc-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-nvrtc-cu12 12.5.82 which is incompatible.\ntorch 2.6.0+cu124 requires nvidia-cuda-runtime-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-runtime-cu12 12.5.82 which is incompatible.\ntorch 2.6.0+cu124 requires nvidia-cudnn-cu12==9.1.0.70; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cudnn-cu12 9.3.0.75 which is incompatible.\ntorch 2.6.0+cu124 requires nvidia-cufft-cu12==11.2.1.3; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cufft-cu12 11.2.3.61 which is incompatible.\ntorch 2.6.0+cu124 requires nvidia-curand-cu12==10.3.5.147; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-curand-cu12 10.3.6.82 which is incompatible.\ntorch 2.6.0+cu124 requires nvidia-cusolver-cu12==11.6.1.9; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusolver-cu12 11.6.3.83 which is incompatible.\ntorch 2.6.0+cu124 requires nvidia-cusparse-cu12==12.3.1.170; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusparse-cu12 12.5.1.3 which is incompatible.\ntorch 2.6.0+cu124 requires nvidia-nvjitlink-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-nvjitlink-cu12 12.5.82 which is incompatible.\ngcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2025.3.0 which is incompatible.\nbigframes 2.8.0 requires google-cloud-bigquery[bqstorage,pandas]>=3.31.0, but you have google-cloud-bigquery 3.25.0 which is incompatible.\nbigframes 2.8.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"!pip install -q sacrebleu","metadata":{"execution":{"iopub.status.busy":"2025-07-13T19:20:49.710738Z","iopub.execute_input":"2025-07-13T19:20:49.710974Z","iopub.status.idle":"2025-07-13T19:20:52.819784Z","shell.execute_reply.started":"2025-07-13T19:20:49.710956Z","shell.execute_reply":"2025-07-13T19:20:52.818738Z"},"trusted":true},"outputs":[],"execution_count":5},{"cell_type":"code","source":"# imports\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, Trainer, TrainingArguments, DataCollatorWithPadding\nfrom datasets import load_dataset\nimport torch\nimport wandb\nfrom kaggle_secrets import UserSecretsClient\nimport os, wandb","metadata":{"execution":{"iopub.status.busy":"2025-07-13T19:20:19.650849Z","iopub.execute_input":"2025-07-13T19:20:19.651257Z","iopub.status.idle":"2025-07-13T19:20:49.709418Z","shell.execute_reply.started":"2025-07-13T19:20:19.651218Z","shell.execute_reply":"2025-07-13T19:20:49.708669Z"},"trusted":true},"outputs":[{"name":"stderr","text":"2025-07-13 19:20:33.145748: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1752434433.331319      36 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1752434433.380933      36 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"# connect wandb library for graphs\nsecret = UserSecretsClient()\nos.environ[\"WANDB_API_KEY\"] = secret.get_secret(\"WANDB_KEY\") \n\nwandb.login()   ","metadata":{"execution":{"iopub.status.busy":"2025-07-13T19:21:09.091186Z","iopub.execute_input":"2025-07-13T19:21:09.091490Z","iopub.status.idle":"2025-07-13T19:21:15.126270Z","shell.execute_reply.started":"2025-07-13T19:21:09.091469Z","shell.execute_reply":"2025-07-13T19:21:15.125489Z"},"trusted":true},"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33m22cs3033\u001b[0m (\u001b[33m22cs3033-rgipt-jais\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","output_type":"stream"},{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}],"execution_count":7},{"cell_type":"code","source":"# loading model and tokenizer into the RAM\nmodel_name = \"Qwen/Qwen3-0.6B\"\nmodel = AutoModelForCausalLM.from_pretrained(model_name, low_cpu_mem_usage=True )\nmodel.config.use_cache = False\ntokenizer = AutoTokenizer.from_pretrained(model_name)","metadata":{"execution":{"iopub.status.busy":"2025-07-13T19:21:30.468845Z","iopub.execute_input":"2025-07-13T19:21:30.469811Z","iopub.status.idle":"2025-07-13T19:21:41.131144Z","shell.execute_reply.started":"2025-07-13T19:21:30.469776Z","shell.execute_reply":"2025-07-13T19:21:41.130289Z"},"trusted":true},"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/726 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bb56344bb1874e2b8916197ea1b14bfd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/1.50G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dfe8bfea4f1f484aa0b93162baa5a1c4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/239 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"647fc3f8b3b44a0dbdb3af9b027cf7ca"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a043b54bc1ca4d648a101f6bab45f46b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2a64bf4f79354419850374ddc1444948"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6df779b2fb2747a58045ef2871e62503"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/11.4M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"591fb9a3d5fd4704ad99d59236351314"}},"metadata":{}}],"execution_count":8},{"cell_type":"code","source":"# load the model into GPU\ndevice = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\nmodel.to(device)","metadata":{"execution":{"iopub.status.busy":"2025-07-13T19:21:46.675434Z","iopub.execute_input":"2025-07-13T19:21:46.675735Z","iopub.status.idle":"2025-07-13T19:21:47.537650Z","shell.execute_reply.started":"2025-07-13T19:21:46.675714Z","shell.execute_reply":"2025-07-13T19:21:47.536684Z"},"trusted":true},"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"Qwen3ForCausalLM(\n  (model): Qwen3Model(\n    (embed_tokens): Embedding(151936, 1024)\n    (layers): ModuleList(\n      (0-27): 28 x Qwen3DecoderLayer(\n        (self_attn): Qwen3Attention(\n          (q_proj): Linear(in_features=1024, out_features=2048, bias=False)\n          (k_proj): Linear(in_features=1024, out_features=1024, bias=False)\n          (v_proj): Linear(in_features=1024, out_features=1024, bias=False)\n          (o_proj): Linear(in_features=2048, out_features=1024, bias=False)\n          (q_norm): Qwen3RMSNorm((128,), eps=1e-06)\n          (k_norm): Qwen3RMSNorm((128,), eps=1e-06)\n        )\n        (mlp): Qwen3MLP(\n          (gate_proj): Linear(in_features=1024, out_features=3072, bias=False)\n          (up_proj): Linear(in_features=1024, out_features=3072, bias=False)\n          (down_proj): Linear(in_features=3072, out_features=1024, bias=False)\n          (act_fn): SiLU()\n        )\n        (input_layernorm): Qwen3RMSNorm((1024,), eps=1e-06)\n        (post_attention_layernorm): Qwen3RMSNorm((1024,), eps=1e-06)\n      )\n    )\n    (norm): Qwen3RMSNorm((1024,), eps=1e-06)\n    (rotary_emb): Qwen3RotaryEmbedding()\n  )\n  (lm_head): Linear(in_features=1024, out_features=151936, bias=False)\n)"},"metadata":{}}],"execution_count":9},{"cell_type":"code","source":"# create the response on default pretrained model\nprompt = \"Dengue cases on the rise in city\"\nmessages = [\n    {\"role\": \"system\", \"content\": \"You are a translator. who knows marathai and english language very well. Your task is convert given english sentence in marathi.\"},\n    {\"role\": \"user\", \"content\": prompt}\n]\ntext = tokenizer.apply_chat_template(\n    messages,\n    tokenize=False,\n    add_generation_prompt=True\n)\nmodel_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n\ngenerated_ids = model.generate(\n    **model_inputs,\n    max_new_tokens=512\n)\ngenerated_ids = [\n    output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n]\n\nresponse = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n","metadata":{"execution":{"iopub.status.busy":"2025-07-13T19:21:52.163509Z","iopub.execute_input":"2025-07-13T19:21:52.164359Z","iopub.status.idle":"2025-07-13T19:22:05.037669Z","shell.execute_reply.started":"2025-07-13T19:21:52.164325Z","shell.execute_reply":"2025-07-13T19:22:05.036859Z"},"trusted":true},"outputs":[],"execution_count":10},{"cell_type":"code","source":"response","metadata":{"execution":{"iopub.status.busy":"2025-07-13T19:22:05.038850Z","iopub.execute_input":"2025-07-13T19:22:05.039061Z","iopub.status.idle":"2025-07-13T19:22:05.044004Z","shell.execute_reply.started":"2025-07-13T19:22:05.039045Z","shell.execute_reply":"2025-07-13T19:22:05.043132Z"},"trusted":true},"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"'<think>\\nOkay, the user wants me to translate the sentence \"Dengue cases on the rise in city\" into Marathi. Let me start by breaking down the sentence. \"Dengue cases\" is straightforward. \"On the rise\" would be \"अव्यापन असते\" or \"अव्यापन वर्तते\". \"In city\" translates to \"तेल असते\" or \"तेल असते\". \\n\\nNow, putting it all together: \"Dengue cases on the rise in city\" becomes \"तेल असते अव्यापन वर्तते तेल\". Wait, is that correct? Let me check. The structure should be \"Dengue cases are increasing in the city\". So maybe \"तेल असते अव्यापन वर्तते तेल\" makes sense. Alternatively, \"Dengue cases are rising in the city\" could be \"तेल असते अव्यापन वर्तते तेल\". Yes, that seems right. I should make sure there are no grammatical errors. The sentence is clear and the translation is accurate. Alright, that should be the answer.\\n</think>\\n\\nतेल असते अव्यापन वर्तते तेल'"},"metadata":{}}],"execution_count":11},{"cell_type":"code","source":"# load the english to marathi translation data from huggingface\nds = load_dataset(\"anujsahani01/English-Marathi\")","metadata":{"execution":{"iopub.status.busy":"2025-07-13T19:22:05.044830Z","iopub.execute_input":"2025-07-13T19:22:05.045090Z","iopub.status.idle":"2025-07-13T19:22:27.570624Z","shell.execute_reply.started":"2025-07-13T19:22:05.045067Z","shell.execute_reply":"2025-07-13T19:22:27.570057Z"},"trusted":true},"outputs":[{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/206 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4e066105c0014be590b3aa2227c94a21"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train.csv:   0%|          | 0.00/621M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e85164b5ca4e46ff8b2a4c1bdb6408c6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"test.csv:   0%|          | 0.00/243M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ebb9f2e11b7d4cde943c87c1e15e8053"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/2637962 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5ee04a684cca49b28bec88d8125266bb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/879321 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"84b78c123f4444d0a66408892a1bd036"}},"metadata":{}}],"execution_count":12},{"cell_type":"code","source":"ds","metadata":{"execution":{"iopub.status.busy":"2025-07-13T19:22:27.572008Z","iopub.execute_input":"2025-07-13T19:22:27.572223Z","iopub.status.idle":"2025-07-13T19:22:27.576734Z","shell.execute_reply.started":"2025-07-13T19:22:27.572206Z","shell.execute_reply":"2025-07-13T19:22:27.576079Z"},"trusted":true},"outputs":[{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['english', 'marathi'],\n        num_rows: 2637962\n    })\n    test: Dataset({\n        features: ['english', 'marathi'],\n        num_rows: 879321\n    })\n})"},"metadata":{}}],"execution_count":13},{"cell_type":"code","source":"ds['train'][10]","metadata":{"execution":{"iopub.status.busy":"2025-07-13T19:22:27.577334Z","iopub.execute_input":"2025-07-13T19:22:27.577510Z","iopub.status.idle":"2025-07-13T19:22:29.834908Z","shell.execute_reply.started":"2025-07-13T19:22:27.577496Z","shell.execute_reply":"2025-07-13T19:22:29.834212Z"},"trusted":true},"outputs":[{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"{'english': 'The funds collected are used for social purposes.',\n 'marathi': 'त्या निधीचा वापर सामाजिक कार्यासाठी करतो.'}"},"metadata":{}}],"execution_count":14},{"cell_type":"code","source":"# Dataset is too big so slice it for quick experimentation\ndf_train = ds[\"train\"].select(range(10000))\ndf_validation = ds[\"train\"].select(range(10000 , 11000))\ndf_test = ds[\"test\"].select(range(200))","metadata":{"execution":{"iopub.status.busy":"2025-07-13T19:22:29.835630Z","iopub.execute_input":"2025-07-13T19:22:29.835916Z","iopub.status.idle":"2025-07-13T19:22:29.853442Z","shell.execute_reply.started":"2025-07-13T19:22:29.835886Z","shell.execute_reply":"2025-07-13T19:22:29.852835Z"},"trusted":true},"outputs":[],"execution_count":15},{"cell_type":"code","source":"# Qwen3-0.6B is Causal Language Models so we have finetuned it Instruction-output format. (instruction‑tuning)\ndef add_instruction(example):\n    example[\"instruction\"] = \"Convert the English text into Marathi language.\"\n    return example\n\n\ndf_train = df_train.map(add_instruction, batched=False)\ndf_validation = df_validation.map(add_instruction, batched=False)\ndf_test = df_test.map(add_instruction, batched=False)","metadata":{"execution":{"iopub.status.busy":"2025-07-13T19:22:29.854213Z","iopub.execute_input":"2025-07-13T19:22:29.854467Z","iopub.status.idle":"2025-07-13T19:22:30.171578Z","shell.execute_reply.started":"2025-07-13T19:22:29.854443Z","shell.execute_reply":"2025-07-13T19:22:30.170877Z"},"trusted":true},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/10000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0690daf62e1d496e94ba9e3b9d5e4d24"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/1000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2390b4d8b1784152b3996c259af54562"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/200 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"080f0b83dffb452b8c11f049c1332ced"}},"metadata":{}}],"execution_count":16},{"cell_type":"code","source":"ds[\"train\"].column_names","metadata":{"execution":{"iopub.status.busy":"2025-07-13T19:22:30.172201Z","iopub.execute_input":"2025-07-13T19:22:30.172366Z","iopub.status.idle":"2025-07-13T19:22:30.177589Z","shell.execute_reply.started":"2025-07-13T19:22:30.172352Z","shell.execute_reply":"2025-07-13T19:22:30.176909Z"},"trusted":true},"outputs":[{"execution_count":17,"output_type":"execute_result","data":{"text/plain":"['english', 'marathi']"},"metadata":{}}],"execution_count":17},{"cell_type":"code","source":"df_train[0]","metadata":{"execution":{"iopub.status.busy":"2025-07-13T19:22:30.178407Z","iopub.execute_input":"2025-07-13T19:22:30.178919Z","iopub.status.idle":"2025-07-13T19:22:30.198224Z","shell.execute_reply.started":"2025-07-13T19:22:30.178871Z","shell.execute_reply":"2025-07-13T19:22:30.197534Z"},"trusted":true},"outputs":[{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"{'english': 'Next few months are really crucial for us.',\n 'marathi': 'पुढील तीन महिने आमच्यासाठी खूप महत्त्वपूर्ण आहेत.',\n 'instruction': 'Convert the English text into Marathi language.'}"},"metadata":{}}],"execution_count":18},{"cell_type":"code","source":"# Know about how tokenizer works\ninputs = tokenizer('Next few months are really crucial for us.','पुढील तीन महिने आमच्यासाठी खूप महत्त्वपूर्ण आहेत.', 'Convert the English text into Marathi language.' )\ninputs","metadata":{"execution":{"iopub.status.busy":"2025-07-13T19:22:37.364696Z","iopub.execute_input":"2025-07-13T19:22:37.364996Z","iopub.status.idle":"2025-07-13T19:22:37.370400Z","shell.execute_reply.started":"2025-07-13T19:22:37.364975Z","shell.execute_reply":"2025-07-13T19:22:37.369630Z"},"trusted":true},"outputs":[{"execution_count":21,"output_type":"execute_result","data":{"text/plain":"{'input_ids': [5847, 2421, 3951, 525, 2167, 16587, 369, 601, 13, 86162, 72653, 149269, 43647, 91811, 14925, 97, 43647, 60096, 91217, 93948, 42311, 101, 34370, 14925, 228, 87244, 146113, 30484, 107, 31411, 116, 31411, 254, 43647, 14925, 244, 12619, 224, 86162, 91217, 93948, 79238, 30484, 97, 30484, 113, 86162, 12619, 224, 44179, 30484, 96, 14925, 228, 93948, 54784, 97, 13], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': [12012, 279, 6364, 1467, 1119, 2876, 66531, 4128, 13]}"},"metadata":{}}],"execution_count":21},{"cell_type":"code","source":"tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"])","metadata":{"execution":{"iopub.status.busy":"2025-07-13T19:22:37.620565Z","iopub.execute_input":"2025-07-13T19:22:37.621324Z","iopub.status.idle":"2025-07-13T19:22:37.626744Z","shell.execute_reply.started":"2025-07-13T19:22:37.621294Z","shell.execute_reply":"2025-07-13T19:22:37.625999Z"},"trusted":true},"outputs":[{"execution_count":22,"output_type":"execute_result","data":{"text/plain":"['Next',\n 'Ġfew',\n 'Ġmonths',\n 'Ġare',\n 'Ġreally',\n 'Ġcrucial',\n 'Ġfor',\n 'Ġus',\n '.',\n 'à¤ª',\n 'à¥ģ',\n 'à¤¢',\n 'à¥Ģ',\n 'à¤²',\n 'Ġà¤',\n '¤',\n 'à¥Ģ',\n 'à¤¨',\n 'Ġà¤®',\n 'à¤¹',\n 'à¤¿à¤',\n '¨',\n 'à¥ĩ',\n 'Ġà¤',\n 'Ĩ',\n 'à¤®',\n 'à¤ļ',\n 'à¥įà¤',\n '¯',\n 'à¤¾à¤',\n '¸',\n 'à¤¾à¤',\n 'ł',\n 'à¥Ģ',\n 'Ġà¤',\n 'ĸ',\n 'à¥',\n 'Ĥ',\n 'à¤ª',\n 'Ġà¤®',\n 'à¤¹',\n 'à¤¤',\n 'à¥įà¤',\n '¤',\n 'à¥įà¤',\n 'µ',\n 'à¤ª',\n 'à¥',\n 'Ĥ',\n 'à¤°',\n 'à¥įà¤',\n '£',\n 'Ġà¤',\n 'Ĩ',\n 'à¤¹',\n 'à¥ĩà¤',\n '¤',\n '.']"},"metadata":{}}],"execution_count":22},{"cell_type":"code","source":"instruction = \"Convert the English text into Marathi language.\"\nsrc_text     = \"Next few months are really crucial for us.\"\ntgt_text     = \"पुढील तीन महिने आमच्यासाठी खूप महत्त्वपूर्ण आहेत.\"\n\n# Prompt the model should *see*\nprompt = f\"{instruction} English: {src_text} Marathi:\"\nprint(prompt)\n\ninputs  = tokenizer(prompt, return_tensors=\"pt\") \nprint('Length of tokenize form of prompt:', inputs['input_ids'].size())\n\n\n# Every token sequence in a causal‐LM fine‑tuning setup needs a clear “stop” signal so the model knows when it’s done generating. That’s exactly what tokenizer.eos_token_id is for\n\nlabels  = tokenizer(tgt_text, add_special_tokens=False).input_ids + [tokenizer.eos_token_id] #Here labes are tokenize form our marathi output","metadata":{"execution":{"iopub.status.busy":"2025-07-13T19:22:37.834051Z","iopub.execute_input":"2025-07-13T19:22:37.834852Z","iopub.status.idle":"2025-07-13T19:22:37.841891Z","shell.execute_reply.started":"2025-07-13T19:22:37.834818Z","shell.execute_reply":"2025-07-13T19:22:37.841130Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Convert the English text into Marathi language. English: Next few months are really crucial for us. Marathi:\nLength of tokenize form of prompt: torch.Size([1, 23])\n","output_type":"stream"}],"execution_count":23},{"cell_type":"code","source":"inputs","metadata":{"execution":{"iopub.status.busy":"2025-07-13T19:22:38.031451Z","iopub.execute_input":"2025-07-13T19:22:38.032268Z","iopub.status.idle":"2025-07-13T19:22:38.044028Z","shell.execute_reply.started":"2025-07-13T19:22:38.032233Z","shell.execute_reply":"2025-07-13T19:22:38.043362Z"},"trusted":true},"outputs":[{"execution_count":24,"output_type":"execute_result","data":{"text/plain":"{'input_ids': tensor([[12012,   279,  6364,  1467,  1119,  2876, 66531,  4128,    13,  6364,\n            25,  9295,  2421,  3951,   525,  2167, 16587,   369,   601,    13,\n          2876, 66531,    25]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}"},"metadata":{}}],"execution_count":24},{"cell_type":"code","source":"# The length of tokenize input prompt( instrution + context) is 23","metadata":{"execution":{"iopub.status.busy":"2025-07-13T19:22:38.214267Z","iopub.execute_input":"2025-07-13T19:22:38.215035Z","iopub.status.idle":"2025-07-13T19:22:38.218646Z","shell.execute_reply.started":"2025-07-13T19:22:38.215003Z","shell.execute_reply":"2025-07-13T19:22:38.217838Z"},"trusted":true},"outputs":[],"execution_count":25},{"cell_type":"code","source":"tokenizer.eos_token_id","metadata":{"execution":{"iopub.status.busy":"2025-07-13T19:22:42.167742Z","iopub.execute_input":"2025-07-13T19:22:42.168529Z","iopub.status.idle":"2025-07-13T19:22:42.173434Z","shell.execute_reply.started":"2025-07-13T19:22:42.168493Z","shell.execute_reply":"2025-07-13T19:22:42.172716Z"},"trusted":true},"outputs":[{"execution_count":26,"output_type":"execute_result","data":{"text/plain":"151645"},"metadata":{}}],"execution_count":26},{"cell_type":"code","source":"labels ","metadata":{"execution":{"iopub.status.busy":"2025-07-13T19:22:42.405780Z","iopub.execute_input":"2025-07-13T19:22:42.406015Z","iopub.status.idle":"2025-07-13T19:22:42.411354Z","shell.execute_reply.started":"2025-07-13T19:22:42.405995Z","shell.execute_reply":"2025-07-13T19:22:42.410685Z"},"trusted":true},"outputs":[{"execution_count":27,"output_type":"execute_result","data":{"text/plain":"[86162,\n 72653,\n 149269,\n 43647,\n 91811,\n 14925,\n 97,\n 43647,\n 60096,\n 91217,\n 93948,\n 42311,\n 101,\n 34370,\n 14925,\n 228,\n 87244,\n 146113,\n 30484,\n 107,\n 31411,\n 116,\n 31411,\n 254,\n 43647,\n 14925,\n 244,\n 12619,\n 224,\n 86162,\n 91217,\n 93948,\n 79238,\n 30484,\n 97,\n 30484,\n 113,\n 86162,\n 12619,\n 224,\n 44179,\n 30484,\n 96,\n 14925,\n 228,\n 93948,\n 54784,\n 97,\n 13,\n 151645]"},"metadata":{}}],"execution_count":27},{"cell_type":"code","source":"len(labels) # 49 orignal + 1 tokenizer.eos_token_id = 50","metadata":{"execution":{"iopub.status.busy":"2025-07-13T19:22:42.659412Z","iopub.execute_input":"2025-07-13T19:22:42.659633Z","iopub.status.idle":"2025-07-13T19:22:42.664258Z","shell.execute_reply.started":"2025-07-13T19:22:42.659616Z","shell.execute_reply":"2025-07-13T19:22:42.663554Z"},"trusted":true},"outputs":[{"execution_count":28,"output_type":"execute_result","data":{"text/plain":"50"},"metadata":{}}],"execution_count":28},{"cell_type":"code","source":"# For a causal model, you feed it the entire sequence (prompt and target), because it learns by shifting that sequence one token at a time.\nmodel_inputs = tokenizer(prompt + \" \" + tgt_text + tokenizer.eos_token,  return_tensors=\"pt\")\nprint('Length of tokenize form of model_inputs:', model_inputs['input_ids'].size())  \n\n# You need to know how many tokens correspond just to the instruction+source (the “prompt”) so that you can later mask them out in the labels.\nprompt_len = len(tokenizer(prompt, add_special_tokens=False).input_ids)\nprint(\"Tokenized Prompt length:\" , prompt_len)\n\n#  Clone so you don’t overwrite the original inputs\nlabels = model_inputs[\"input_ids\"].clone()\n\n\n# For every position in the prompt, set label to -100  \n# In Hugging Face Trainer, any label token = –100 is ignored by the loss function.\nlabels[:, :prompt_len] = -100        # works for a batch of size 1 or N\n# print('Length that going to be masked' , len(labels))\nmodel_inputs[\"labels\"] = labels","metadata":{"execution":{"iopub.status.busy":"2025-07-13T19:22:45.923390Z","iopub.execute_input":"2025-07-13T19:22:45.923689Z","iopub.status.idle":"2025-07-13T19:22:45.930895Z","shell.execute_reply.started":"2025-07-13T19:22:45.923642Z","shell.execute_reply":"2025-07-13T19:22:45.930167Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Length of tokenize form of model_inputs: torch.Size([1, 73])\nTokenized Prompt length: 23\n","output_type":"stream"}],"execution_count":29},{"cell_type":"code","source":"labels","metadata":{"execution":{"iopub.status.busy":"2025-07-13T19:22:48.570289Z","iopub.execute_input":"2025-07-13T19:22:48.570646Z","iopub.status.idle":"2025-07-13T19:22:48.578285Z","shell.execute_reply.started":"2025-07-13T19:22:48.570620Z","shell.execute_reply":"2025-07-13T19:22:48.577415Z"},"trusted":true},"outputs":[{"execution_count":30,"output_type":"execute_result","data":{"text/plain":"tensor([[  -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n           -100,   -100,   -100,   -100,   -100,  83636,  72653, 149269,  43647,\n          91811,  14925,     97,  43647,  60096,  91217,  93948,  42311,    101,\n          34370,  14925,    228,  87244, 146113,  30484,    107,  31411,    116,\n          31411,    254,  43647,  14925,    244,  12619,    224,  86162,  91217,\n          93948,  79238,  30484,     97,  30484,    113,  86162,  12619,    224,\n          44179,  30484,     96,  14925,    228,  93948,  54784,     97,     13,\n         151645]])"},"metadata":{}}],"execution_count":30},{"cell_type":"code","source":"labels.size()","metadata":{"execution":{"iopub.status.busy":"2025-07-13T19:22:48.831295Z","iopub.execute_input":"2025-07-13T19:22:48.832236Z","iopub.status.idle":"2025-07-13T19:22:48.837062Z","shell.execute_reply.started":"2025-07-13T19:22:48.832203Z","shell.execute_reply":"2025-07-13T19:22:48.836288Z"},"trusted":true},"outputs":[{"execution_count":31,"output_type":"execute_result","data":{"text/plain":"torch.Size([1, 73])"},"metadata":{}}],"execution_count":31},{"cell_type":"code","source":"x = labels[0] \nnum_ignored = (x == -100).sum().item()\nnum_not_ignored = (x != -100).sum().item()\nprint(num_ignored)\nprint(num_not_ignored)","metadata":{"execution":{"iopub.status.busy":"2025-07-13T19:22:49.043866Z","iopub.execute_input":"2025-07-13T19:22:49.044402Z","iopub.status.idle":"2025-07-13T19:22:49.052177Z","shell.execute_reply.started":"2025-07-13T19:22:49.044370Z","shell.execute_reply":"2025-07-13T19:22:49.051347Z"},"trusted":true},"outputs":[{"name":"stdout","text":"23\n50\n","output_type":"stream"}],"execution_count":32},{"cell_type":"code","source":"model_inputs","metadata":{"execution":{"iopub.status.busy":"2025-07-13T19:22:51.635297Z","iopub.execute_input":"2025-07-13T19:22:51.636105Z","iopub.status.idle":"2025-07-13T19:22:51.643451Z","shell.execute_reply.started":"2025-07-13T19:22:51.636065Z","shell.execute_reply":"2025-07-13T19:22:51.642673Z"},"trusted":true},"outputs":[{"execution_count":33,"output_type":"execute_result","data":{"text/plain":"{'input_ids': tensor([[ 12012,    279,   6364,   1467,   1119,   2876,  66531,   4128,     13,\n           6364,     25,   9295,   2421,   3951,    525,   2167,  16587,    369,\n            601,     13,   2876,  66531,     25,  83636,  72653, 149269,  43647,\n          91811,  14925,     97,  43647,  60096,  91217,  93948,  42311,    101,\n          34370,  14925,    228,  87244, 146113,  30484,    107,  31411,    116,\n          31411,    254,  43647,  14925,    244,  12619,    224,  86162,  91217,\n          93948,  79238,  30484,     97,  30484,    113,  86162,  12619,    224,\n          44179,  30484,     96,  14925,    228,  93948,  54784,     97,     13,\n         151645]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1]]), 'labels': tensor([[  -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n           -100,   -100,   -100,   -100,   -100,  83636,  72653, 149269,  43647,\n          91811,  14925,     97,  43647,  60096,  91217,  93948,  42311,    101,\n          34370,  14925,    228,  87244, 146113,  30484,    107,  31411,    116,\n          31411,    254,  43647,  14925,    244,  12619,    224,  86162,  91217,\n          93948,  79238,  30484,     97,  30484,    113,  86162,  12619,    224,\n          44179,  30484,     96,  14925,    228,  93948,  54784,     97,     13,\n         151645]])}"},"metadata":{}}],"execution_count":33},{"cell_type":"code","source":"model_inputs['input_ids'].size()","metadata":{"execution":{"iopub.status.busy":"2025-07-13T19:22:51.894357Z","iopub.execute_input":"2025-07-13T19:22:51.894632Z","iopub.status.idle":"2025-07-13T19:22:51.899681Z","shell.execute_reply.started":"2025-07-13T19:22:51.894611Z","shell.execute_reply":"2025-07-13T19:22:51.899064Z"},"trusted":true},"outputs":[{"execution_count":34,"output_type":"execute_result","data":{"text/plain":"torch.Size([1, 73])"},"metadata":{}}],"execution_count":34},{"cell_type":"code","source":"# this function convert string into vectors\ndef tokenize_function(examples):\n\n    batch_input_ids      = []\n    batch_attention_mask = []\n    batch_labels         = []\n\n    # Iterate over the batch\n    for instruction, english, marathi in zip(\n            examples[\"instruction\"], examples[\"english\"], examples[\"marathi\"]):\n\n        # 1. Build the prompt shown to the model (not trained on)\n        prompt_text = f\"{instruction} English: {english} Marathi:\"\n        prompt_ids  = tokenizer(prompt_text, add_special_tokens=False).input_ids\n\n        # 2. Full text = prompt + target + EOS\n        full_text = f\"{prompt_text} {marathi}{tokenizer.eos_token}\"\n        tok = tokenizer(full_text, add_special_tokens=False)\n\n        # 3. Create labels: mask the prompt tokens with –100\n        label_ids = [-100] * len(prompt_ids) + tok[\"input_ids\"][len(prompt_ids):]\n\n        # 4. Collect results for this example\n        batch_input_ids.append(tok[\"input_ids\"])\n        batch_attention_mask.append(tok[\"attention_mask\"])\n        batch_labels.append(label_ids)\n\n    return {\n        \"input_ids\":      batch_input_ids,\n        \"attention_mask\": batch_attention_mask,\n        \"labels\":         batch_labels,\n    }","metadata":{"execution":{"iopub.status.busy":"2025-07-13T19:22:52.117529Z","iopub.execute_input":"2025-07-13T19:22:52.118358Z","iopub.status.idle":"2025-07-13T19:22:52.125038Z","shell.execute_reply.started":"2025-07-13T19:22:52.118324Z","shell.execute_reply":"2025-07-13T19:22:52.124189Z"},"trusted":true},"outputs":[],"execution_count":35},{"cell_type":"code","source":"train_tokenised_ds = df_train.map(tokenize_function, batched=True , num_proc=4 , remove_columns=df_train.column_names)\ntest_tokenised_ds = df_test.map(tokenize_function, batched=True , num_proc=4 , remove_columns=df_test.column_names)\nvalidation_tokenised_ds = df_validation.map(tokenize_function, batched=True , num_proc=4, remove_columns=df_validation.column_names )","metadata":{"execution":{"iopub.status.busy":"2025-07-13T19:22:54.769102Z","iopub.execute_input":"2025-07-13T19:22:54.769387Z","iopub.status.idle":"2025-07-13T19:22:59.382698Z","shell.execute_reply.started":"2025-07-13T19:22:54.769367Z","shell.execute_reply":"2025-07-13T19:22:59.381796Z"},"trusted":true},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map (num_proc=4):   0%|          | 0/10000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"85078c5e5d0a4b1ab2384a98e4882b6e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map (num_proc=4):   0%|          | 0/200 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1f725e1fe7e948b6ba1e8093c32edfde"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map (num_proc=4):   0%|          | 0/1000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"94bc81a150014cae9cc05857893dd3c9"}},"metadata":{}}],"execution_count":36},{"cell_type":"code","source":"test_tokenised_ds","metadata":{"execution":{"iopub.status.busy":"2025-07-13T19:22:59.384489Z","iopub.execute_input":"2025-07-13T19:22:59.385277Z","iopub.status.idle":"2025-07-13T19:22:59.390892Z","shell.execute_reply.started":"2025-07-13T19:22:59.385240Z","shell.execute_reply":"2025-07-13T19:22:59.390142Z"},"trusted":true},"outputs":[{"execution_count":37,"output_type":"execute_result","data":{"text/plain":"Dataset({\n    features: ['input_ids', 'attention_mask', 'labels'],\n    num_rows: 200\n})"},"metadata":{}}],"execution_count":37},{"cell_type":"code","source":"len(test_tokenised_ds[0]['input_ids'])","metadata":{"execution":{"iopub.status.busy":"2025-07-13T19:23:01.778390Z","iopub.execute_input":"2025-07-13T19:23:01.778762Z","iopub.status.idle":"2025-07-13T19:23:01.785817Z","shell.execute_reply.started":"2025-07-13T19:23:01.778726Z","shell.execute_reply":"2025-07-13T19:23:01.785058Z"},"trusted":true},"outputs":[{"execution_count":38,"output_type":"execute_result","data":{"text/plain":"90"},"metadata":{}}],"execution_count":38},{"cell_type":"code","source":"if tokenizer.pad_token is None:\n    tokenizer.add_special_tokens({\"pad_token\": tokenizer.eos_token})\n    model.resize_token_embeddings(len(tokenizer))          # update embeddings\nmodel.config.pad_token_id = tokenizer.pad_token_id","metadata":{"execution":{"iopub.status.busy":"2025-07-13T19:23:02.073735Z","iopub.execute_input":"2025-07-13T19:23:02.074013Z","iopub.status.idle":"2025-07-13T19:23:02.078788Z","shell.execute_reply.started":"2025-07-13T19:23:02.073990Z","shell.execute_reply":"2025-07-13T19:23:02.078095Z"},"trusted":true},"outputs":[],"execution_count":39},{"cell_type":"code","source":"for split in (train_tokenised_ds, test_tokenised_ds, validation_tokenised_ds):\n    split.set_format(type=\"torch\")","metadata":{"execution":{"iopub.status.busy":"2025-07-13T19:23:04.401271Z","iopub.execute_input":"2025-07-13T19:23:04.401576Z","iopub.status.idle":"2025-07-13T19:23:04.410113Z","shell.execute_reply.started":"2025-07-13T19:23:04.401552Z","shell.execute_reply":"2025-07-13T19:23:04.409338Z"},"trusted":true},"outputs":[],"execution_count":40},{"cell_type":"code","source":"from transformers import DataCollatorForSeq2Seq\ndata_collator = DataCollatorForSeq2Seq(\n    tokenizer=tokenizer,\n    model=model,\n    label_pad_token_id=-100,        # keep –100 so they’re ignored in the loss\n    pad_to_multiple_of=8,           # speed-up on tensor-core GPUs; omit if CPU\n)\n","metadata":{"execution":{"iopub.status.busy":"2025-07-13T19:23:05.545827Z","iopub.execute_input":"2025-07-13T19:23:05.546124Z","iopub.status.idle":"2025-07-13T19:23:05.550343Z","shell.execute_reply.started":"2025-07-13T19:23:05.546099Z","shell.execute_reply":"2025-07-13T19:23:05.549533Z"},"trusted":true},"outputs":[],"execution_count":41},{"cell_type":"code","source":"train_tokenised_ds","metadata":{"execution":{"iopub.status.busy":"2025-07-13T19:23:06.396644Z","iopub.execute_input":"2025-07-13T19:23:06.397230Z","iopub.status.idle":"2025-07-13T19:23:06.402101Z","shell.execute_reply.started":"2025-07-13T19:23:06.397206Z","shell.execute_reply":"2025-07-13T19:23:06.401290Z"},"trusted":true},"outputs":[{"execution_count":42,"output_type":"execute_result","data":{"text/plain":"Dataset({\n    features: ['input_ids', 'attention_mask', 'labels'],\n    num_rows: 10000\n})"},"metadata":{}}],"execution_count":42},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(\"Running on\", device)","metadata":{"execution":{"iopub.status.busy":"2025-07-13T19:23:08.801342Z","iopub.execute_input":"2025-07-13T19:23:08.801622Z","iopub.status.idle":"2025-07-13T19:23:08.806326Z","shell.execute_reply.started":"2025-07-13T19:23:08.801599Z","shell.execute_reply":"2025-07-13T19:23:08.805680Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Running on cuda\n","output_type":"stream"}],"execution_count":43},{"cell_type":"code","source":"os.environ[\"WANDB_PROJECT\"] = \"qwen_marathi\"","metadata":{"execution":{"iopub.status.busy":"2025-07-13T19:23:09.210918Z","iopub.execute_input":"2025-07-13T19:23:09.211600Z","iopub.status.idle":"2025-07-13T19:23:09.215525Z","shell.execute_reply.started":"2025-07-13T19:23:09.211563Z","shell.execute_reply":"2025-07-13T19:23:09.214771Z"},"trusted":true},"outputs":[],"execution_count":44},{"cell_type":"code","source":"from transformers import TrainingArguments\n\ntraining_args = TrainingArguments(\n    output_dir=\"qwen_marathi\",\n    overwrite_output_dir=True,\n    \n    num_train_epochs=1,\n    per_device_train_batch_size=4,\n    per_device_eval_batch_size=4,\n    gradient_accumulation_steps=5,  \n    eval_steps=100,\n    logging_steps=100,\n    save_total_limit=2,\n    learning_rate=5e-5,\n    weight_decay=0.01,\n    warmup_steps=200,\n    # metric_for_best_model=\"eval_loss\",\n    lr_scheduler_type=\"cosine\", # linear or cosine or cosine_with_restarts\n    fp16= True,\n\n    #new\n    optim=\"adafactor\",  # or adamw_torch\n    report_to=\"wandb\",\n    run_name=\"qwen-marathi-run2\",\n    save_only_model=True,        # HF ≥ 4.38\n    save_safetensors=True, \n    #new\n    gradient_checkpointing=True,\n    # bf16=True,                # TPU v3/v4 natively run in bfloat16\n    # tpu_num_cores=8, \n)\n","metadata":{"execution":{"iopub.status.busy":"2025-07-13T19:23:12.664155Z","iopub.execute_input":"2025-07-13T19:23:12.664850Z","iopub.status.idle":"2025-07-13T19:23:12.696165Z","shell.execute_reply.started":"2025-07-13T19:23:12.664816Z","shell.execute_reply":"2025-07-13T19:23:12.695444Z"},"trusted":true},"outputs":[],"execution_count":45},{"cell_type":"code","source":"from transformers import Trainer\nmodel.to(device) \ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_tokenised_ds,\n    eval_dataset=validation_tokenised_ds,\n    processing_class=tokenizer,        # <-- pass tokenizer here\n    data_collator=data_collator,\n)\n\ntrainer.train()","metadata":{"execution":{"iopub.status.busy":"2025-07-13T19:23:13.130114Z","iopub.execute_input":"2025-07-13T19:23:13.130747Z","iopub.status.idle":"2025-07-13T19:49:26.930955Z","shell.execute_reply.started":"2025-07-13T19:23:13.130719Z","shell.execute_reply":"2025-07-13T19:49:26.930223Z"},"trusted":true},"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Changes to your `wandb` environment variables will be ignored because your `wandb` session has already started. For more information on how to modify your settings with `wandb.init()` arguments, please refer to <a href='https://wandb.me/wandb-init' target=\"_blank\">the W&B docs</a>."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.20.1"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250713_192313-k10varml</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/22cs3033-rgipt-jais/qwen_marathi/runs/k10varml' target=\"_blank\">qwen-marathi-run2</a></strong> to <a href='https://wandb.ai/22cs3033-rgipt-jais/qwen_marathi' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/22cs3033-rgipt-jais/qwen_marathi' target=\"_blank\">https://wandb.ai/22cs3033-rgipt-jais/qwen_marathi</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/22cs3033-rgipt-jais/qwen_marathi/runs/k10varml' target=\"_blank\">https://wandb.ai/22cs3033-rgipt-jais/qwen_marathi/runs/k10varml</a>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/transformers/data/data_collator.py:741: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:254.)\n  batch[\"labels\"] = torch.tensor(batch[\"labels\"], dtype=torch.int64)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='500' max='500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [500/500 26:03, Epoch 1/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>100</td>\n      <td>1.484300</td>\n    </tr>\n    <tr>\n      <td>200</td>\n      <td>1.378200</td>\n    </tr>\n    <tr>\n      <td>300</td>\n      <td>1.279700</td>\n    </tr>\n    <tr>\n      <td>400</td>\n      <td>1.124600</td>\n    </tr>\n    <tr>\n      <td>500</td>\n      <td>1.011300</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"execution_count":46,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=500, training_loss=1.2556070861816406, metrics={'train_runtime': 1573.1979, 'train_samples_per_second': 6.356, 'train_steps_per_second': 0.318, 'total_flos': 4067213061390336.0, 'train_loss': 1.2556070861816406, 'epoch': 1.0})"},"metadata":{}}],"execution_count":46},{"cell_type":"code","source":"trainer.save_model(\"qwen_marathi/best_model\")\n","metadata":{"execution":{"iopub.status.busy":"2025-07-13T19:49:26.932183Z","iopub.execute_input":"2025-07-13T19:49:26.932386Z","iopub.status.idle":"2025-07-13T19:49:33.128525Z","shell.execute_reply.started":"2025-07-13T19:49:26.932371Z","shell.execute_reply":"2025-07-13T19:49:33.127641Z"},"trusted":true},"outputs":[],"execution_count":47},{"cell_type":"code","source":"df_test[0]","metadata":{"execution":{"iopub.status.busy":"2025-07-13T19:49:33.129439Z","iopub.execute_input":"2025-07-13T19:49:33.129724Z","iopub.status.idle":"2025-07-13T19:49:33.137549Z","shell.execute_reply.started":"2025-07-13T19:49:33.129700Z","shell.execute_reply":"2025-07-13T19:49:33.136886Z"},"trusted":true},"outputs":[{"execution_count":48,"output_type":"execute_result","data":{"text/plain":"{'english': 'This name is derived from two Sanskrit words Knti and Pur.',\n 'marathi': 'हे नाव दोन संस्कृत शब्दांकडून प्राप्त झाले आहे - कांती आणि पुरा.',\n 'instruction': 'Convert the English text into Marathi language.'}"},"metadata":{}}],"execution_count":48},{"cell_type":"code","source":"# ------------------------------------------------------------\n#  Evaluate the fine-tuned model on the held-out test split\n#  and report SacreBLEU.  (pip install evaluate tqdm if needed)\n# ------------------------------------------------------------\nimport torch, evaluate\nfrom tqdm.auto import tqdm\n\nmetric = evaluate.load(\"sacrebleu\")        # BLEU implementation recommended by HF\n\nmodel.eval()                               # inference mode – disables dropout, etc.\nbatch_size         = 8                     # tweak to fit your GPU / CPU\npredictions, refs  = [], []\n\nfor i in tqdm(range(0, len(df_test), batch_size)):\n    batch            = df_test[i : i + batch_size]\n    src_sentences    = batch[\"english\"]\n    ref_sentences    = batch[\"marathi\"]\n\n    # Build the same prompt format the model saw during training\n    prompts = [\n        f\"Convert the English text into Marathi language. English: {src} Marathi:\"\n        for src in src_sentences\n    ]\n\n    # Tokenise & send to device\n    inputs = tokenizer(\n        prompts,\n        return_tensors=\"pt\",\n        padding=True,\n        truncation=True,\n        max_length=512,          # truncate over-long inputs, if any\n    ).to(model.device)\n\n    with torch.no_grad():\n        generated = model.generate(\n            **inputs,\n            max_new_tokens=512,  # generation budget\n            do_sample=False,     # greedy decoding\n            eos_token_id=tokenizer.eos_token_id,\n            pad_token_id=tokenizer.pad_token_id,\n        )\n\n    # Strip the prompt part from each generated sequence\n    input_lens = (inputs.input_ids != tokenizer.pad_token_id).sum(1)\n    for gen_ids, in_len in zip(generated, input_lens):\n        gen_only_ids = gen_ids[in_len:]                   # remove prompt tokens\n        pred_text     = tokenizer.decode(\n            gen_only_ids, skip_special_tokens=True\n        ).strip()\n        predictions.append(pred_text)\n\n    # Each reference must itself be a list (to support multi-ref BLEU)\n    refs.extend([[t] for t in ref_sentences])\n\n# ----  Compute BLEU  -------------------------------------------------\nbleu_result = metric.compute(predictions=predictions, references=refs)\n\nprint(f\"SacreBLEU on test set: {bleu_result['score']:.2f}\")","metadata":{"execution":{"iopub.status.busy":"2025-07-13T19:50:27.618056Z","iopub.execute_input":"2025-07-13T19:50:27.618330Z","iopub.status.idle":"2025-07-13T19:57:03.300939Z","shell.execute_reply.started":"2025-07-13T19:50:27.618308Z","shell.execute_reply":"2025-07-13T19:57:03.300027Z"},"trusted":true},"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading builder script: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"83895e9f689d4e04806112ac23692c41"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/25 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fead0fbad153466d84b3d41c14b9c552"}},"metadata":{}},{"name":"stderr","text":"The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nA decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nA decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nA decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nA decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nA decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nA decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nA decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nA decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nA decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nA decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nA decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nA decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nA decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nA decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nA decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nA decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nA decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nA decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nA decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nA decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nA decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nA decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nA decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nA decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\nThe following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nA decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n","output_type":"stream"},{"name":"stdout","text":"SacreBLEU on test set: 1.89\n","output_type":"stream"}],"execution_count":50},{"cell_type":"code","source":"# 5️⃣ Evaluate on validation and test sets\nval_metrics  = trainer.evaluate(eval_dataset=validation_tokenised_ds)\ntest_metrics = trainer.evaluate(eval_dataset=test_tokenised_ds)\n\nprint(\"Validation metrics:\", val_metrics)\nprint(\"Test metrics:\", test_metrics)\n\n# 6️⃣ Save final tokenizer + config (for inference)\ntokenizer.save_pretrained(\"qwen_marathi/final_tokenizer\")\nmodel.save_pretrained(\"qwen_marathi/final_model\")","metadata":{"execution":{"iopub.status.busy":"2025-07-13T19:57:03.302387Z","iopub.execute_input":"2025-07-13T19:57:03.302721Z","iopub.status.idle":"2025-07-13T19:57:52.685553Z","shell.execute_reply.started":"2025-07-13T19:57:03.302701Z","shell.execute_reply":"2025-07-13T19:57:52.684998Z"},"trusted":true},"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='300' max='250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [250/250 00:45]\n    </div>\n    "},"metadata":{}},{"name":"stdout","text":"Validation metrics: {'eval_loss': 1.0184826850891113, 'eval_runtime': 38.7886, 'eval_samples_per_second': 25.781, 'eval_steps_per_second': 6.445, 'epoch': 1.0}\nTest metrics: {'eval_loss': 1.0316262245178223, 'eval_runtime': 7.1915, 'eval_samples_per_second': 27.811, 'eval_steps_per_second': 6.953, 'epoch': 1.0}\n","output_type":"stream"}],"execution_count":51},{"cell_type":"code","source":"def translate_to_marathi(english_sentence: str) -> str:\n    instruction = \"Convert the English text into Marathi language.\"\n    prompt = f\"{instruction} English: {english_sentence} Marathi:\"\n    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n\n    # Generate with a reasonable max length\n    outputs = model.generate(\n        **inputs,\n        max_new_tokens=512,\n        do_sample=False,           # greedy\n        eos_token_id=tokenizer.eos_token_id,\n        pad_token_id=tokenizer.pad_token_id,\n    )\n    # Strip off prompt tokens and decode rest\n    generated = outputs[0][inputs.input_ids.size(1):]\n    return tokenizer.decode(generated, skip_special_tokens=True)\n\n# Example\nprint(translate_to_marathi(\"The objective of this article and the following two articles is to deepen our appreciation for Gods qualities that we may think of less often than his principal attributes.\"))","metadata":{"execution":{"iopub.status.busy":"2025-07-13T19:57:52.686290Z","iopub.execute_input":"2025-07-13T19:57:52.686503Z","iopub.status.idle":"2025-07-13T19:57:58.248092Z","shell.execute_reply.started":"2025-07-13T19:57:52.686486Z","shell.execute_reply":"2025-07-13T19:57:58.247239Z"},"trusted":true},"outputs":[{"name":"stderr","text":"The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n","output_type":"stream"},{"name":"stdout","text":" या आणि नंतर दोन लेखांच्या विशिष्ट विषयांमध्ये आपल्या देवाच्या विषयांची आपल्या विशिष्ट विषयांची विशिष्ट विषय वाढवण्याची आहे.\n","output_type":"stream"}],"execution_count":52},{"cell_type":"code","source":"df_test[15]","metadata":{"execution":{"iopub.status.busy":"2025-07-13T19:57:58.249493Z","iopub.execute_input":"2025-07-13T19:57:58.249830Z","iopub.status.idle":"2025-07-13T19:57:58.256093Z","shell.execute_reply.started":"2025-07-13T19:57:58.249810Z","shell.execute_reply":"2025-07-13T19:57:58.255322Z"},"trusted":true},"outputs":[{"execution_count":53,"output_type":"execute_result","data":{"text/plain":"{'english': 'The objective of this article and the following two articles is to deepen our appreciation for Gods qualities that we may think of less often than his principal attributes.',\n 'marathi': 'त्याच प्रकारे, आपण जेव्हा यहोवाच्या एखाद्या गुणाची ओळख करून घेतो, त्या गुणावर मनन करतो आणि आपल्या स्वतःच्या जीवनात तो गुण दाखवतो तेव्हा यहोवाच्या त्या गुणाबद्दलची आपली कदर आणखी वाढते.',\n 'instruction': 'Convert the English text into Marathi language.'}"},"metadata":{}}],"execution_count":53},{"cell_type":"code","source":"","metadata":{},"outputs":[],"execution_count":null}]}